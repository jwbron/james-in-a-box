# ADR: Log Access Lockdown via Gateway Sidecar

**Driver:** James Wiesebron
**Approver:** TBD
**Contributors:** James Wiesebron
**Informed:** Engineering teams
**Proposed:** January 2026
**Status:** Proposed

## Table of Contents

- [Industry Standards Reference](#industry-standards-reference)
- [Context](#context)
- [Problem Statement](#problem-statement)
- [Decision](#decision)
- [High-Level Design](#high-level-design)
- [Security Analysis](#security-analysis)
- [Consequences](#consequences)
- [Alternatives Considered](#alternatives-considered)
- [Implementation Plan](#implementation-plan)

## Industry Standards Reference

This ADR extends the security principles established in [ADR-Internet-Tool-Access-Lockdown](./ADR-Internet-Tool-Access-Lockdown.md) and [ADR-Git-Isolation-Architecture](../implemented/ADR-Git-Isolation-Architecture.md), applying the same OWASP Agentic Application Security framework to log access:

| OWASP Risk | Description | Mitigation in This ADR |
|------------|-------------|------------------------|
| **ASI01** - Agentic Excessive Authority | Agents granted overly broad permissions | Agents can only read their own logs via gateway API |
| **ASI02** - Tool Misuse & Exploitation | Agents misusing available tools | Log search scoped by container/task ID; no cross-agent access |
| **ASI03** - Identity & Privilege Abuse | Credential theft or misuse | Logs not mounted in container; gateway validates all access |
| **ASI06** - Memory/Context Poisoning | Corruption of agent memory/config | Logs are read-only from container perspective |
| **ASI10** - Rogue Agents | Agent operating outside intended behavior | Infrastructure controls prevent unauthorized log access |

**Reference:** [OWASP Top 10 for Agentic Applications](https://genai.owasp.org/)

## Context

### Background

The james-in-a-box system stores logs in several locations accessible from the host filesystem:

| Path | Content | Source | Current Access |
|------|---------|--------|----------------|
| `~/.jib-sharing/container-logs/` | Container stdout/stderr (persistent) | `container_logging.py` | Direct filesystem read |
| `~/.jib-sharing/logs/` | Claude output streams (real-time) | `jib_logging` library | Direct filesystem read |
| `/var/log/jib/model_output/` | Full Claude responses (large files) | `model_capture.py` | Direct filesystem read |

**Note:** The `/var/log/jib/model_output/` path is defined in `shared/jib_logging/model_capture.py:28` and stores full model responses for debugging and cost tracking. This directory is created on-demand when model capture is enabled.

Currently, log access has **no gateway enforcement**:
- The `jib-logs` utility reads logs directly without authentication
- Container log persistence (`container_logging.py`) writes to shared directories
- Any process with filesystem access can read any agent's logs

This creates security and privacy concerns:
1. **Cross-agent information leakage**: One agent could read another agent's logs
2. **Sensitive data exposure**: Logs may contain API responses, internal data, or error details
3. **No audit trail**: Cannot determine who accessed which logs and when
4. **Inconsistent with git isolation model**: Git operations go through gateway, but logs don't

### Current Logging Architecture

The existing [ADR-Standardized-Logging-Interface](./ADR-Standardized-Logging-Interface.md) defines the **format** and **generation** of logs, but not **access control**. Logs are:

1. Generated by various components using `jib_logging` library
2. Written to files in `~/.jib-sharing/` directories
3. Indexed by `container_logging.py` for correlation (task_id → container_id)
4. Read via `jib-logs` CLI utility or direct file access

**Gap:** Access control and auditing are missing from the log lifecycle.

### Scope

**In Scope:**
- Log access control via gateway sidecar
- Read-only log APIs for containers
- Log search/retrieval policies
- Audit logging for log access
- Container filesystem isolation for logs

**Out of Scope:**
- Log generation changes (already covered by ADR-Standardized-Logging-Interface)
- Log format changes (already standardized)
- Log retention policies (separate concern)
- Real-time log streaming (future enhancement)

## Problem Statement

**Logs should follow the same security model as git operations: containers cannot access logs directly, and all access goes through the gateway with policy enforcement and audit logging.**

Specific requirements:

1. **Isolation**: Containers cannot mount or read log directories directly
2. **Scoped access**: Agents can only read their own logs (by container_id or task_id)
3. **Audit trail**: All log access is recorded with timestamp, accessor, and scope
4. **Gateway enforcement**: Policy violations result in HTTP 403, not silent failures
5. **Consistency**: Same authentication/authorization pattern as git operations

## Decision

**Extend the gateway sidecar architecture to provide log access control, mirroring the git isolation model.**

### Core Principles

1. **Filesystem isolation**: `~/.jib-sharing/` is not mounted in the jib container
2. **Gateway as single access point**: All log reads go through `/api/v1/logs/*` endpoints
3. **Container-scoped access**: Default policy allows reading only own logs
4. **Audit everything**: Every log access is recorded in gateway audit logs
5. **Fail closed**: Missing authentication or policy violation returns 403

## High-Level Design

### Architecture Overview

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                     Container Filesystem View                                 │
│                                                                               │
│  /home/jib/repos/my-repo/    ← Worktree (existing, via git isolation)        │
│  /home/jib/context-sync/     ← Read-only context                             │
│  /home/jib/beads/            ← Task tracking                                 │
│                                                                               │
│  ~/.jib-sharing/             ← NOT MOUNTED (no logs visible)                 │
│  /var/log/jib/               ← NOT MOUNTED (no logs visible)                 │
│                                                                               │
│  Container CANNOT directly read any logs                                      │
└─────────────────────────────────────────────────────────────────────────────┘
                                    │
                                    │ HTTP API calls
                                    │ (authenticated with Bearer token)
                                    ▼
┌─────────────────────────────────────────────────────────────────────────────┐
│                        Gateway Sidecar (Trusted)                              │
│                                                                               │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                    Log Access Endpoints                               │    │
│  │                                                                       │    │
│  │  GET  /api/v1/logs/list           - List recent logs                 │    │
│  │  GET  /api/v1/logs/task/<id>      - Get logs for task                │    │
│  │  GET  /api/v1/logs/container/<id> - Get logs by container ID         │    │
│  │  GET  /api/v1/logs/search         - Search with pattern (scoped)     │    │
│  │  GET  /api/v1/logs/model/<id>     - Get model output for task        │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                    │                                          │
│                                    ▼                                          │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                    Policy Enforcement                                 │    │
│  │                                                                       │    │
│  │  - Validate authentication (Bearer token)                            │    │
│  │  - Extract requester's container_id/task_id from context             │    │
│  │  - Check: requested logs belong to requester OR admin role           │    │
│  │  - Return 403 if policy violated                                     │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                    │                                          │
│                                    ▼                                          │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                    Log Storage (Host Filesystem)                      │    │
│  │                                                                       │    │
│  │  ~/.jib-sharing/container-logs/  - Container stdout/stderr           │    │
│  │  ~/.jib-sharing/logs/            - Real-time Claude output           │    │
│  │  /var/log/jib/model_output/      - Full model responses              │    │
│  │                                                                       │    │
│  │  Gateway has read access; containers do NOT                          │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                               │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Component Summary

| Component | Purpose | Implementation |
|-----------|---------|----------------|
| Log API endpoints | Controlled interface for log access | Flask routes in gateway.py |
| Log policy engine | Validate access against ownership | Extension of policy.py |
| Log client | Convenience methods for API calls | Python module for containers |
| Audit logger | Record all log access | Existing gateway audit system |
| Container mount changes | Remove log paths from container | Docker Compose changes |

### Key Security Properties

1. **Container cannot read logs** - `~/.jib-sharing/` is not mounted
2. **Scoped access by default** - Agents can only read their own logs
3. **All access is auditable** - Gateway logs every log access request
4. **Authentication required** - Same Bearer token as git operations
5. **Policy violations logged** - Denied access attempts are recorded

### Container Identity Verification

**How the gateway knows which container is making the request:**

The gateway uses the existing authentication model established in [ADR-Git-Isolation-Architecture](../implemented/ADR-Git-Isolation-Architecture.md). Container identity is established at worktree creation time:

1. **Worktree creation** (`jib --exec` or `jib --task`): The orchestrator creates a unique `container_id` and `task_id`, passing them to the gateway as part of the worktree registration.

2. **Gateway session state**: The gateway maintains a mapping of authenticated sessions to their associated `container_id` and `task_id`. This is stored in memory and is authoritative.

3. **Request validation**: When a container calls `/api/v1/logs/*`, the gateway extracts the session from the Bearer token and looks up the registered `container_id`/`task_id` for that session.

4. **Header verification (defense in depth)**: The `X-Container-ID` and `X-Task-ID` headers are validated against the session's registered values. Mismatches result in 403 responses and audit log entries.

**Why spoofing is not a meaningful attack vector:**
- The Bearer token is cryptographically tied to the session
- The session's identity claims are set by the orchestrator, not the container
- Network isolation means only containers in the jib-network can reach the gateway
- Containers cannot obtain other containers' Bearer tokens

This model trusts the authenticated caller's session, with header validation as a defense-in-depth measure to detect bugs or misconfigurations rather than active attacks.

### Gateway Log API

#### Endpoints

| Endpoint | Method | Parameters | Response | Policy |
|----------|--------|------------|----------|--------|
| `/api/v1/logs/list` | GET | `?limit=N&offset=M` | List of log entries | Own logs only |
| `/api/v1/logs/task/<task_id>` | GET | - | Log content for task | Task must belong to requester |
| `/api/v1/logs/container/<container_id>` | GET | `?lines=N` | Log content | Container must be self |
| `/api/v1/logs/search` | GET | `?pattern=<regex>&scope=self` | Matching log entries | Search limited to own logs |
| `/api/v1/logs/model/<task_id>` | GET | - | Full model output | Task must belong to requester |

#### Request/Response Format

**Request (authenticated):**
```http
GET /api/v1/logs/task/task-20260128-123456 HTTP/1.1
Authorization: Bearer <gateway-secret>
X-Container-ID: jib-abc123
X-Task-ID: task-20260128-123456
```

**Response (success):**
```json
{
  "task_id": "task-20260128-123456",
  "container_id": "jib-abc123",
  "log_file": "/path/to/log",
  "content": "... log lines ...",
  "lines": 500,
  "truncated": false
}
```

**Response (policy violation):**
```json
{
  "error": "access_denied",
  "message": "Cannot access logs for task task-other-123",
  "reason": "Task does not belong to requesting container"
}
```

### Log Index Integrity

The log index (`~/.jib-sharing/container-logs/log-index.json`) is critical for mapping `task_id` → `container_id`. This section defines integrity requirements:

**Index Location and Ownership:**
- The index is stored on the **host filesystem**, not in any container
- Only the orchestrator (`jib` CLI on host) and gateway sidecar have write access
- Containers cannot write to the index (filesystem not mounted)

**Integrity Guarantees:**

| Property | Mechanism |
|----------|-----------|
| Atomicity | File locking (`fcntl.LOCK_EX`) prevents concurrent modifications |
| Consistency | Index updates happen only at container start/end (orchestrator) |
| Availability | Gateway reads are fast (in-memory cache with file watch) |
| Durability | JSON file persists across gateway restarts |

**Failure Modes and Mitigations:**

| Failure | Detection | Mitigation |
|---------|-----------|------------|
| Missing task in index | Gateway returns 404 for unknown tasks | Fail-closed: cannot access unindexed logs |
| Corrupted index JSON | Parse error on gateway startup | Gateway falls back to rebuild from log file headers |
| Index out of sync | Task exists in logs but not index | Gateway can optionally scan log directories periodically |
| Index tampering (host compromise) | Outside threat model | If host is compromised, all security is compromised |

**Implementation Note:** The current `container_logging.py:129-196` provides the atomic update mechanism. The gateway should load the index into memory at startup and use `inotify` (Linux) or polling to detect updates, avoiding file I/O on every request.

### Log Policy Engine

Extend the existing policy engine with log-specific rules:

```python
@dataclass
class LogAccessPolicy:
    """Policy for log access control."""

    def check_task_access(
        self,
        requester_container_id: str,
        requester_task_id: str | None,
        target_task_id: str,
    ) -> PolicyResult:
        """Check if requester can access logs for target task."""
        # Look up task's container from index
        owner_container = self.log_index.get_container_for_task(target_task_id)

        if owner_container is None:
            return PolicyResult(
                allowed=False,
                reason="Task not found in log index",
            )

        if owner_container == requester_container_id:
            return PolicyResult(allowed=True, reason="Owner access")

        if requester_task_id == target_task_id:
            return PolicyResult(allowed=True, reason="Task identity match")

        return PolicyResult(
            allowed=False,
            reason="Cross-container log access denied",
            details={
                "requester": requester_container_id,
                "owner": owner_container,
                "target_task": target_task_id,
            },
        )
```

### Audit Log Format

Every log access produces an audit entry:

```json
{
  "timestamp": "2026-01-28T14:32:01.234Z",
  "event_type": "log_access",
  "operation": "read_task_logs",
  "source_ip": "172.18.0.2",
  "source_container": "jib-abc123",
  "auth_valid": true,
  "request": {
    "target_task_id": "task-20260128-123456",
    "endpoint": "/api/v1/logs/task/task-20260128-123456"
  },
  "policy_result": {
    "allowed": true,
    "reason": "Owner access"
  },
  "response": {
    "status": 200,
    "lines_returned": 500,
    "duration_ms": 45
  }
}
```

### Container Log Client

Provide a Python client for containers to access logs via the gateway:

```python
# shared/jib_logging/log_client.py

from jib_logging.log_client import LogClient

client = LogClient()  # Uses environment for gateway URL and secret

# Get own logs
logs = client.get_task_logs("task-20260128-123456")
print(logs.content)

# Search own logs
results = client.search("error", limit=10)
for entry in results:
    print(f"{entry.timestamp}: {entry.message}")

# Get model output
model_output = client.get_model_output("task-20260128-123456")
print(f"Tokens used: {model_output.input_tokens + model_output.output_tokens}")
```

### jib-logs CLI Migration

The existing `jib-logs` utility should route through the gateway when run from within a container:

```bash
# Current behavior (direct filesystem access - INSECURE):
jib-logs --search "error"
# Reads from ~/.jib-sharing/ directly

# New behavior (gateway-mediated):
jib-logs --search "error"
# 1. Calls is_in_container() (see Phase 3 for detection logic)
# 2. If in container: calls GET /api/v1/logs/search?pattern=error
# 3. Returns only logs for current container's tasks
```

**Container Detection:** The CLI uses the `is_in_container()` function documented in Phase 3. Primary detection is via `JIB_CONTAINER=1` environment variable set by the orchestrator. Secondary detection checks for absence of `~/.jib-sharing/` (not mounted in container) or presence of `/.dockerenv`.

For host-side usage (admin/debugging), the utility can continue with direct access, but should emit an audit log.

## Security Analysis

### Threat Model

| Threat | Mitigation | Residual Risk |
|--------|------------|---------------|
| Agent reads other agent's logs | Filesystem isolation + policy enforcement | VERY LOW - No mount = no access |
| Agent discovers task IDs to probe | Policy checks ownership, not just task existence | LOW - Probe attempts logged |
| Sensitive data in logs | Logs are already written; this ADR controls read access | MEDIUM - Data at rest unchanged |
| Log injection attacks | Logs are read-only via gateway | VERY LOW - No write operations |
| Bypass via docker exec | Gateway still controls container mounts | LOW - Requires host access |
| Audit log tampering | Audit logs in gateway, not container | LOW - Container can't modify |

### What This Does NOT Address

1. **Sensitive data in logs**: This ADR controls access, not content. Logs may still contain sensitive data.
2. **Log encryption at rest**: Logs are stored unencrypted on host. Encryption is a separate concern.
3. **Cross-agent data in logs**: If logs contain information about other agents, that's a logging practice issue.
4. **Real-time log streaming**: This ADR covers pull-based access, not WebSocket streaming.

### Defense in Depth Summary

```
Layer 1: Behavioral (CLAUDE.md instructions)
    ↓ Agent told not to access other logs
Layer 2: Filesystem Isolation (this ADR)
    ↓ ~/.jib-sharing/ not mounted = cannot access even if instructed
Layer 3: Gateway Policy Enforcement
    ↓ API validates ownership before returning logs
Layer 4: Audit Logging
    ↓ All access attempts recorded
Layer 5: Network Isolation
    ↓ Container can only reach gateway
```

## Consequences

### Positive

- **Consistent security model**: Logs follow same pattern as git (gateway-mediated)
- **Audit trail**: All log access is recorded with context
- **Cross-agent isolation**: Agents cannot read each other's logs
- **Policy flexibility**: Can add new policies (role-based, time-limited) later
- **No code changes in logging**: Log generation unchanged; only access controlled

### Negative

- **Latency**: Log reads go through HTTP API instead of filesystem
- **Complexity**: Additional gateway endpoints to maintain
- **Breaking change**: Tools that read logs directly will need updates
- **Debugging friction**: Developers may need admin access for cross-container debugging

### Trade-offs

| Aspect | Direct Filesystem | Gateway-Mediated |
|--------|-------------------|------------------|
| Access speed | Fast (filesystem) | HTTP overhead (~10-50ms) |
| Security | None | Policy-enforced |
| Auditability | None | Full audit trail |
| Cross-agent access | Unrestricted | Denied by default |
| Debugging | Easy | Requires gateway access |

### Performance Considerations

Gateway-mediated log access introduces latency that may affect debugging workflows:

**Expected Overhead:**

| Operation | Direct FS | Gateway | Impact |
|-----------|-----------|---------|--------|
| Single log read | ~1ms | ~10-50ms | Minimal for single reads |
| Log search (small) | ~50ms | ~100-200ms | Noticeable but acceptable |
| Log search (large, 100MB+) | ~500ms | ~1-5s | May require pagination |
| Streaming tail | N/A (inotify) | Deferred to Phase 4 | Significant workflow impact |

**Mitigation Strategies:**

1. **Gateway caching**: Cache recently-accessed log content in memory (TTL ~30s)
2. **Pagination for search**: Return max 1000 results per request, with cursor-based pagination
3. **Result limits**: Default `?limit=100` on list/search endpoints
4. **Async search**: For large searches, return a search job ID and poll for results
5. **Index pre-loading**: Gateway loads log index at startup, watches for updates via `inotify`

**Real-time streaming (Phase 4 consideration):** The lack of `--tail` functionality will impact debugging. While deferred, this should be prioritized if user feedback indicates significant friction. WebSocket streaming through the gateway is technically feasible using the same authentication model.

## Alternatives Considered

### Alternative 1: Unix File Permissions

**Approach:** Set file permissions so each container's user can only read own logs

**Pros:**
- Simple, uses existing Unix security model
- No gateway changes needed

**Cons:**
- Requires unique UID per container (complex orchestration)
- No audit trail
- Permissions can be complex to manage across volumes
- Doesn't work well with Docker shared volumes

**Rejected:** Too complex to implement reliably in containerized environment

### Alternative 2: Encrypted Logs Per-Container

**Approach:** Encrypt each container's logs with a unique key

**Pros:**
- Strong isolation via cryptography
- Works even with shared filesystem

**Cons:**
- Key management complexity
- Performance overhead for encryption/decryption
- Doesn't solve cross-agent probing
- Adds significant implementation complexity

**Rejected:** Overkill for the threat model; gateway isolation is simpler

### Alternative 3: Separate Log Volumes Per Container

**Approach:** Mount a unique log volume for each container

**Pros:**
- Natural filesystem isolation
- No gateway changes needed

**Cons:**
- Volume proliferation (one per container)
- No central log search/aggregation
- Storage management complexity
- Breaks log correlation features

**Rejected:** Fragments the logging architecture; loses correlation benefits

## Implementation Plan

This section provides detailed implementation guidance for each phase, including specific files to modify, code structures, testing strategies, and acceptance criteria.

---

### Phase 1: Gateway Log Endpoints

**Objective:** Add API endpoints to the gateway sidecar for controlled log access with policy enforcement and audit logging.

**Duration estimate:** N/A (no time estimates)

#### Task 1.1: Create Log Index Reader

**File:** `gateway-sidecar/log_index.py` (new file)

**Purpose:** Read and cache the log index maintained by `container_logging.py` for fast task→container lookups.

```python
# gateway-sidecar/log_index.py
"""Log index reader for gateway-mediated log access."""

import json
import os
from dataclasses import dataclass
from pathlib import Path
from threading import Lock
from typing import Any

from jib_logging import get_logger

logger = get_logger("gateway-sidecar.log_index")

# Default paths (configurable via environment)
DEFAULT_LOG_INDEX_PATH = Path.home() / ".jib-sharing" / "container-logs" / "log-index.json"
DEFAULT_CONTAINER_LOGS_DIR = Path.home() / ".jib-sharing" / "container-logs"
DEFAULT_CLAUDE_LOGS_DIR = Path.home() / ".jib-sharing" / "logs"
DEFAULT_MODEL_OUTPUT_DIR = Path("/var/log/jib/model_output")


@dataclass
class LogEntry:
    """A single log entry from the index."""
    container_id: str
    task_id: str | None
    thread_ts: str | None
    log_file: str | None
    timestamp: str


class LogIndex:
    """Thread-safe log index reader with caching."""

    def __init__(self, index_path: Path | None = None):
        self._index_path = index_path or DEFAULT_LOG_INDEX_PATH
        self._cache: dict[str, Any] | None = None
        self._cache_mtime: float = 0
        self._lock = Lock()

    def _load_index(self) -> dict[str, Any]:
        """Load index from file, using cache if file unchanged."""
        if not self._index_path.exists():
            return {"task_to_container": {}, "thread_to_task": {}, "entries": []}

        try:
            mtime = self._index_path.stat().st_mtime
            with self._lock:
                if self._cache is not None and mtime == self._cache_mtime:
                    return self._cache

            with open(self._index_path) as f:
                data = json.load(f)

            with self._lock:
                self._cache = data
                self._cache_mtime = mtime

            return data
        except Exception as e:
            logger.warning(f"Failed to load log index: {e}")
            return {"task_to_container": {}, "thread_to_task": {}, "entries": []}

    def get_container_for_task(self, task_id: str) -> str | None:
        """Look up the container ID that owns a task."""
        index = self._load_index()
        return index.get("task_to_container", {}).get(task_id)

    def get_task_for_thread(self, thread_ts: str) -> str | None:
        """Look up the task ID for a Slack thread."""
        index = self._load_index()
        return index.get("thread_to_task", {}).get(thread_ts)

    def list_entries(
        self,
        container_id: str | None = None,
        limit: int = 50,
        offset: int = 0,
    ) -> list[LogEntry]:
        """List log entries, optionally filtered by container."""
        index = self._load_index()
        entries = index.get("entries", [])

        if container_id:
            entries = [e for e in entries if e.get("container_id") == container_id]

        # Return newest first
        entries = list(reversed(entries))
        return [
            LogEntry(
                container_id=e.get("container_id", ""),
                task_id=e.get("task_id"),
                thread_ts=e.get("thread_ts"),
                log_file=e.get("log_file"),
                timestamp=e.get("timestamp", ""),
            )
            for e in entries[offset : offset + limit]
        ]


# Singleton instance
_log_index: LogIndex | None = None


def get_log_index() -> LogIndex:
    """Get the singleton LogIndex instance."""
    global _log_index
    if _log_index is None:
        _log_index = LogIndex()
    return _log_index
```

**Acceptance criteria:**
- [ ] Index loads correctly from `~/.jib-sharing/container-logs/log-index.json`
- [ ] Caching works (file only re-read when mtime changes)
- [ ] Thread-safe for concurrent access
- [ ] Returns empty data gracefully when index doesn't exist

---

#### Task 1.2: Create Log Policy Engine

**File:** `gateway-sidecar/log_policy.py` (new file)

**Purpose:** Enforce access control rules for log requests.

```python
# gateway-sidecar/log_policy.py
"""Log access policy enforcement."""

from dataclasses import dataclass
from typing import Any

from jib_logging import get_logger

# Import from local modules
try:
    from .log_index import get_log_index
except ImportError:
    from log_index import get_log_index

logger = get_logger("gateway-sidecar.log_policy")


@dataclass
class LogPolicyResult:
    """Result of a log access policy check."""
    allowed: bool
    reason: str
    details: dict[str, Any] | None = None

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for API response."""
        result = {"allowed": self.allowed, "reason": self.reason}
        if self.details:
            result["details"] = self.details
        return result


class LogPolicy:
    """Log access policy enforcement."""

    def __init__(self):
        self._log_index = get_log_index()

    def check_task_access(
        self,
        requester_container_id: str,
        requester_task_id: str | None,
        target_task_id: str,
    ) -> LogPolicyResult:
        """Check if requester can access logs for target task.

        Access is allowed if:
        1. The requester's container owns the target task
        2. The requester's task_id matches the target task_id
        """
        # Look up task's owner container
        owner_container = self._log_index.get_container_for_task(target_task_id)

        if owner_container is None:
            return LogPolicyResult(
                allowed=False,
                reason="Task not found in log index",
                details={"target_task_id": target_task_id},
            )

        # Check ownership by container
        if owner_container == requester_container_id:
            return LogPolicyResult(
                allowed=True,
                reason="Owner access (container match)",
            )

        # Check ownership by task identity
        if requester_task_id and requester_task_id == target_task_id:
            return LogPolicyResult(
                allowed=True,
                reason="Owner access (task identity match)",
            )

        return LogPolicyResult(
            allowed=False,
            reason="Cross-container log access denied",
            details={
                "requester_container": requester_container_id,
                "owner_container": owner_container,
                "target_task_id": target_task_id,
            },
        )

    def check_container_access(
        self,
        requester_container_id: str,
        target_container_id: str,
    ) -> LogPolicyResult:
        """Check if requester can access logs for target container.

        Access is allowed only if requester == target (self-access).
        """
        if requester_container_id == target_container_id:
            return LogPolicyResult(
                allowed=True,
                reason="Self-access (container identity match)",
            )

        return LogPolicyResult(
            allowed=False,
            reason="Cross-container log access denied",
            details={
                "requester_container": requester_container_id,
                "target_container": target_container_id,
            },
        )

    def check_search_access(
        self,
        requester_container_id: str,
        scope: str,
    ) -> LogPolicyResult:
        """Check if requester can perform a log search.

        Searches are always scoped to the requester's own logs.
        The 'scope' parameter is validated but currently only 'self' is allowed.
        """
        if scope != "self":
            return LogPolicyResult(
                allowed=False,
                reason="Invalid search scope",
                details={"scope": scope, "allowed_scopes": ["self"]},
            )

        return LogPolicyResult(
            allowed=True,
            reason="Search allowed (self scope)",
        )


# Singleton instance
_log_policy: LogPolicy | None = None


def get_log_policy() -> LogPolicy:
    """Get the singleton LogPolicy instance."""
    global _log_policy
    if _log_policy is None:
        _log_policy = LogPolicy()
    return _log_policy
```

**Acceptance criteria:**
- [ ] Task access check correctly validates ownership
- [ ] Container access check allows self-access only
- [ ] Search access enforces 'self' scope
- [ ] Policy results include meaningful reasons and details

---

#### Task 1.3: Create Log Reader Module

**File:** `gateway-sidecar/log_reader.py` (new file)

**Purpose:** Read log file content from the filesystem with size limits and pagination.

```python
# gateway-sidecar/log_reader.py
"""Log file reading utilities for gateway."""

import os
import re
from dataclasses import dataclass
from pathlib import Path
from typing import Iterator

from jib_logging import get_logger

try:
    from .log_index import (
        DEFAULT_CONTAINER_LOGS_DIR,
        DEFAULT_CLAUDE_LOGS_DIR,
        DEFAULT_MODEL_OUTPUT_DIR,
        get_log_index,
    )
except ImportError:
    from log_index import (
        DEFAULT_CONTAINER_LOGS_DIR,
        DEFAULT_CLAUDE_LOGS_DIR,
        DEFAULT_MODEL_OUTPUT_DIR,
        get_log_index,
    )

logger = get_logger("gateway-sidecar.log_reader")

# Limits
MAX_LOG_LINES = 10000  # Maximum lines to return
MAX_LOG_SIZE_BYTES = 50 * 1024 * 1024  # 50MB max file size to read
MAX_SEARCH_RESULTS = 1000  # Maximum search results


@dataclass
class LogContent:
    """Log content with metadata."""
    task_id: str | None
    container_id: str | None
    log_file: str | None
    content: str
    lines: int
    truncated: bool
    size_bytes: int


@dataclass
class SearchResult:
    """A single search result."""
    log_file: str
    line_number: int
    content: str
    task_id: str | None
    container_id: str | None


def read_task_logs(task_id: str, max_lines: int = 1000) -> LogContent | None:
    """Read logs for a specific task.

    Looks for:
    1. Symlink at {task_id}.log -> {container_id}.log
    2. Container log with task_id content
    3. Claude output log at ~/.jib-sharing/logs/{task_id}.log
    """
    # Try symlink first
    symlink_path = DEFAULT_CONTAINER_LOGS_DIR / f"{task_id}.log"
    if symlink_path.exists():
        return _read_log_file(symlink_path, task_id=task_id, max_lines=max_lines)

    # Look up container from index
    log_index = get_log_index()
    container_id = log_index.get_container_for_task(task_id)
    if container_id:
        container_log = DEFAULT_CONTAINER_LOGS_DIR / f"{container_id}.log"
        if container_log.exists():
            return _read_log_file(
                container_log, task_id=task_id, container_id=container_id, max_lines=max_lines
            )

    # Try Claude output log
    claude_log = DEFAULT_CLAUDE_LOGS_DIR / f"{task_id}.log"
    if claude_log.exists():
        return _read_log_file(claude_log, task_id=task_id, max_lines=max_lines)

    return None


def read_container_logs(container_id: str, max_lines: int = 1000) -> LogContent | None:
    """Read logs for a specific container."""
    container_log = DEFAULT_CONTAINER_LOGS_DIR / f"{container_id}.log"
    if container_log.exists():
        return _read_log_file(container_log, container_id=container_id, max_lines=max_lines)
    return None


def read_model_output(task_id: str) -> LogContent | None:
    """Read model output for a specific task."""
    # Model output files are named by task_id
    model_output_path = DEFAULT_MODEL_OUTPUT_DIR / f"{task_id}.json"
    if model_output_path.exists():
        return _read_log_file(model_output_path, task_id=task_id, max_lines=MAX_LOG_LINES)

    # Also check for .log extension
    model_output_path = DEFAULT_MODEL_OUTPUT_DIR / f"{task_id}.log"
    if model_output_path.exists():
        return _read_log_file(model_output_path, task_id=task_id, max_lines=MAX_LOG_LINES)

    return None


def search_logs(
    pattern: str,
    container_id: str,
    max_results: int = 100,
) -> list[SearchResult]:
    """Search logs for a pattern, scoped to a specific container.

    Only searches logs belonging to the specified container.
    """
    results: list[SearchResult] = []
    regex = re.compile(pattern, re.IGNORECASE)

    # Get all tasks for this container from the index
    log_index = get_log_index()
    container_entries = log_index.list_entries(container_id=container_id, limit=1000)

    searched_files: set[str] = set()

    for entry in container_entries:
        if len(results) >= max_results:
            break

        if not entry.log_file or entry.log_file in searched_files:
            continue

        searched_files.add(entry.log_file)
        log_path = Path(entry.log_file)

        if not log_path.exists():
            continue

        try:
            file_size = log_path.stat().st_size
            if file_size > MAX_LOG_SIZE_BYTES:
                logger.warning(f"Skipping large log file: {log_path} ({file_size} bytes)")
                continue

            with open(log_path) as f:
                for line_num, line in enumerate(f, 1):
                    if regex.search(line):
                        results.append(
                            SearchResult(
                                log_file=str(log_path),
                                line_number=line_num,
                                content=line.rstrip()[:500],  # Truncate long lines
                                task_id=entry.task_id,
                                container_id=entry.container_id,
                            )
                        )
                        if len(results) >= max_results:
                            break
        except Exception as e:
            logger.warning(f"Error searching log file {log_path}: {e}")

    return results


def _read_log_file(
    path: Path,
    task_id: str | None = None,
    container_id: str | None = None,
    max_lines: int = 1000,
) -> LogContent:
    """Read a log file with size limits."""
    # Resolve symlinks
    actual_path = path.resolve() if path.is_symlink() else path

    file_size = actual_path.stat().st_size
    truncated = False

    if file_size > MAX_LOG_SIZE_BYTES:
        logger.warning(f"Log file exceeds size limit: {actual_path} ({file_size} bytes)")
        truncated = True

    lines: list[str] = []
    try:
        with open(actual_path) as f:
            for i, line in enumerate(f):
                if i >= max_lines:
                    truncated = True
                    break
                lines.append(line)
    except Exception as e:
        logger.error(f"Error reading log file {actual_path}: {e}")
        return LogContent(
            task_id=task_id,
            container_id=container_id,
            log_file=str(actual_path),
            content=f"Error reading log file: {e}",
            lines=0,
            truncated=False,
            size_bytes=0,
        )

    return LogContent(
        task_id=task_id,
        container_id=container_id,
        log_file=str(actual_path),
        content="".join(lines),
        lines=len(lines),
        truncated=truncated,
        size_bytes=file_size,
    )
```

**Acceptance criteria:**
- [ ] Task logs read correctly (symlink and direct file)
- [ ] Container logs read correctly
- [ ] Model output read correctly
- [ ] Search works with regex patterns
- [ ] Size limits enforced
- [ ] Truncation handled gracefully

---

#### Task 1.4: Add Log Endpoints to Gateway

**File:** `gateway-sidecar/gateway.py` (modify existing)

**Changes:** Add log access endpoints following the existing pattern.

**Add imports at top of file:**
```python
# Add to imports section
try:
    from .log_index import get_log_index
    from .log_policy import get_log_policy
    from .log_reader import (
        read_task_logs,
        read_container_logs,
        read_model_output,
        search_logs,
    )
except ImportError:
    from log_index import get_log_index
    from log_policy import get_log_policy
    from log_reader import (
        read_task_logs,
        read_container_logs,
        read_model_output,
        search_logs,
    )
```

**Add endpoints (after existing git/gh endpoints):**
```python
# ============================================================================
# LOG ACCESS ENDPOINTS
# ============================================================================


def _extract_requester_identity(
    headers: dict[str, str]
) -> tuple[str | None, str | None]:
    """Extract container_id and task_id from request headers.

    In the full implementation, these are validated against session state.
    For now, we trust the headers (defense-in-depth).
    """
    container_id = headers.get("X-Container-ID")
    task_id = headers.get("X-Task-ID")
    return container_id, task_id


@app.route("/api/v1/logs/list", methods=["GET"])
@require_auth
def logs_list():
    """List recent log entries for the requester's container."""
    container_id, task_id = _extract_requester_identity(dict(request.headers))

    if not container_id:
        audit_log("log_access", "list", success=False, details={"error": "missing_container_id"})
        return make_error("Missing X-Container-ID header", 400)

    limit = min(int(request.args.get("limit", 50)), 100)
    offset = int(request.args.get("offset", 0))

    log_index = get_log_index()
    entries = log_index.list_entries(container_id=container_id, limit=limit, offset=offset)

    audit_log(
        "log_access",
        "list",
        success=True,
        details={"container_id": container_id, "count": len(entries)},
    )

    return make_success(
        message=f"Found {len(entries)} log entries",
        data={
            "entries": [
                {
                    "container_id": e.container_id,
                    "task_id": e.task_id,
                    "thread_ts": e.thread_ts,
                    "log_file": e.log_file,
                    "timestamp": e.timestamp,
                }
                for e in entries
            ],
            "limit": limit,
            "offset": offset,
        },
    )


@app.route("/api/v1/logs/task/<task_id>", methods=["GET"])
@require_auth
def logs_task(task_id: str):
    """Get logs for a specific task."""
    container_id, requester_task_id = _extract_requester_identity(dict(request.headers))

    if not container_id:
        audit_log(
            "log_access",
            "read_task",
            success=False,
            details={"error": "missing_container_id", "target_task_id": task_id},
        )
        return make_error("Missing X-Container-ID header", 400)

    # Policy check
    log_policy = get_log_policy()
    policy_result = log_policy.check_task_access(
        requester_container_id=container_id,
        requester_task_id=requester_task_id,
        target_task_id=task_id,
    )

    if not policy_result.allowed:
        audit_log(
            "log_access",
            "read_task",
            success=False,
            details={
                "container_id": container_id,
                "target_task_id": task_id,
                "policy_reason": policy_result.reason,
            },
        )
        return make_error(policy_result.reason, 403, policy_result.details)

    # Read logs
    max_lines = min(int(request.args.get("lines", 1000)), 10000)
    log_content = read_task_logs(task_id, max_lines=max_lines)

    if log_content is None:
        audit_log(
            "log_access",
            "read_task",
            success=False,
            details={"container_id": container_id, "target_task_id": task_id, "error": "not_found"},
        )
        return make_error(f"No logs found for task {task_id}", 404)

    audit_log(
        "log_access",
        "read_task",
        success=True,
        details={
            "container_id": container_id,
            "target_task_id": task_id,
            "lines": log_content.lines,
            "truncated": log_content.truncated,
        },
    )

    return make_success(
        message=f"Retrieved logs for task {task_id}",
        data={
            "task_id": log_content.task_id,
            "container_id": log_content.container_id,
            "log_file": log_content.log_file,
            "content": log_content.content,
            "lines": log_content.lines,
            "truncated": log_content.truncated,
        },
    )


@app.route("/api/v1/logs/container/<target_container_id>", methods=["GET"])
@require_auth
def logs_container(target_container_id: str):
    """Get logs for a specific container (self-access only)."""
    container_id, task_id = _extract_requester_identity(dict(request.headers))

    if not container_id:
        audit_log(
            "log_access",
            "read_container",
            success=False,
            details={"error": "missing_container_id", "target_container": target_container_id},
        )
        return make_error("Missing X-Container-ID header", 400)

    # Policy check
    log_policy = get_log_policy()
    policy_result = log_policy.check_container_access(
        requester_container_id=container_id,
        target_container_id=target_container_id,
    )

    if not policy_result.allowed:
        audit_log(
            "log_access",
            "read_container",
            success=False,
            details={
                "container_id": container_id,
                "target_container": target_container_id,
                "policy_reason": policy_result.reason,
            },
        )
        return make_error(policy_result.reason, 403, policy_result.details)

    # Read logs
    max_lines = min(int(request.args.get("lines", 1000)), 10000)
    log_content = read_container_logs(target_container_id, max_lines=max_lines)

    if log_content is None:
        audit_log(
            "log_access",
            "read_container",
            success=False,
            details={"container_id": container_id, "error": "not_found"},
        )
        return make_error(f"No logs found for container {target_container_id}", 404)

    audit_log(
        "log_access",
        "read_container",
        success=True,
        details={"container_id": container_id, "lines": log_content.lines},
    )

    return make_success(
        message=f"Retrieved logs for container {target_container_id}",
        data={
            "container_id": log_content.container_id,
            "log_file": log_content.log_file,
            "content": log_content.content,
            "lines": log_content.lines,
            "truncated": log_content.truncated,
        },
    )


@app.route("/api/v1/logs/search", methods=["GET"])
@require_auth
def logs_search():
    """Search logs with a pattern (scoped to requester's logs)."""
    container_id, task_id = _extract_requester_identity(dict(request.headers))

    if not container_id:
        audit_log(
            "log_access",
            "search",
            success=False,
            details={"error": "missing_container_id"},
        )
        return make_error("Missing X-Container-ID header", 400)

    pattern = request.args.get("pattern")
    if not pattern:
        return make_error("Missing 'pattern' query parameter", 400)

    scope = request.args.get("scope", "self")
    max_results = min(int(request.args.get("limit", 100)), 1000)

    # Policy check
    log_policy = get_log_policy()
    policy_result = log_policy.check_search_access(
        requester_container_id=container_id,
        scope=scope,
    )

    if not policy_result.allowed:
        audit_log(
            "log_access",
            "search",
            success=False,
            details={
                "container_id": container_id,
                "pattern": pattern,
                "policy_reason": policy_result.reason,
            },
        )
        return make_error(policy_result.reason, 403, policy_result.details)

    # Perform search
    try:
        results = search_logs(
            pattern=pattern,
            container_id=container_id,
            max_results=max_results,
        )
    except Exception as e:
        logger.error(f"Search error: {e}")
        return make_error(f"Search failed: {e}", 500)

    audit_log(
        "log_access",
        "search",
        success=True,
        details={
            "container_id": container_id,
            "pattern": pattern,
            "results_count": len(results),
        },
    )

    return make_success(
        message=f"Found {len(results)} matches",
        data={
            "pattern": pattern,
            "scope": scope,
            "results": [
                {
                    "log_file": r.log_file,
                    "line_number": r.line_number,
                    "content": r.content,
                    "task_id": r.task_id,
                    "container_id": r.container_id,
                }
                for r in results
            ],
            "limit": max_results,
            "truncated": len(results) == max_results,
        },
    )


@app.route("/api/v1/logs/model/<task_id>", methods=["GET"])
@require_auth
def logs_model(task_id: str):
    """Get model output for a specific task."""
    container_id, requester_task_id = _extract_requester_identity(dict(request.headers))

    if not container_id:
        audit_log(
            "log_access",
            "read_model",
            success=False,
            details={"error": "missing_container_id", "target_task_id": task_id},
        )
        return make_error("Missing X-Container-ID header", 400)

    # Policy check (same as task access)
    log_policy = get_log_policy()
    policy_result = log_policy.check_task_access(
        requester_container_id=container_id,
        requester_task_id=requester_task_id,
        target_task_id=task_id,
    )

    if not policy_result.allowed:
        audit_log(
            "log_access",
            "read_model",
            success=False,
            details={
                "container_id": container_id,
                "target_task_id": task_id,
                "policy_reason": policy_result.reason,
            },
        )
        return make_error(policy_result.reason, 403, policy_result.details)

    # Read model output
    log_content = read_model_output(task_id)

    if log_content is None:
        audit_log(
            "log_access",
            "read_model",
            success=False,
            details={"container_id": container_id, "target_task_id": task_id, "error": "not_found"},
        )
        return make_error(f"No model output found for task {task_id}", 404)

    audit_log(
        "log_access",
        "read_model",
        success=True,
        details={
            "container_id": container_id,
            "target_task_id": task_id,
            "size_bytes": log_content.size_bytes,
        },
    )

    return make_success(
        message=f"Retrieved model output for task {task_id}",
        data={
            "task_id": task_id,
            "log_file": log_content.log_file,
            "content": log_content.content,
            "size_bytes": log_content.size_bytes,
            "truncated": log_content.truncated,
        },
    )
```

**Acceptance criteria:**
- [ ] All 5 endpoints implemented: `/list`, `/task/<id>`, `/container/<id>`, `/search`, `/model/<id>`
- [ ] All endpoints require authentication
- [ ] Policy enforcement returns 403 with meaningful error messages
- [ ] Audit logging for all operations (success and failure)
- [ ] Query parameters validated with sensible limits

---

#### Task 1.5: Unit Tests for Log Access

**File:** `gateway-sidecar/tests/test_log_endpoints.py` (new file)

**Test coverage requirements:**
- [ ] Test log index loading and caching
- [ ] Test policy enforcement (allow owner, deny cross-container)
- [ ] Test each endpoint with valid and invalid requests
- [ ] Test authentication requirement
- [ ] Test rate limits and size limits
- [ ] Test edge cases (missing files, corrupted index)

**Sample test structure:**
```python
# gateway-sidecar/tests/test_log_endpoints.py
"""Tests for log access endpoints."""

import json
import pytest
from pathlib import Path
from unittest.mock import patch, MagicMock

# Import test fixtures from existing tests
from .conftest import client, auth_headers


class TestLogPolicy:
    """Tests for log access policy enforcement."""

    def test_task_access_allowed_for_owner(self):
        """Owner can access their own task's logs."""
        ...

    def test_task_access_denied_for_non_owner(self):
        """Non-owner cannot access other's task logs."""
        ...

    def test_container_access_self_only(self):
        """Container can only access its own logs."""
        ...

    def test_search_scope_enforced(self):
        """Search is scoped to requester's logs only."""
        ...


class TestLogEndpoints:
    """Tests for log API endpoints."""

    def test_logs_list_requires_auth(self, client):
        """List endpoint requires authentication."""
        ...

    def test_logs_list_returns_own_logs(self, client, auth_headers):
        """List returns only requester's logs."""
        ...

    def test_logs_task_policy_enforcement(self, client, auth_headers):
        """Task endpoint enforces ownership policy."""
        ...

    def test_logs_search_scoped(self, client, auth_headers):
        """Search is scoped to own logs."""
        ...

    def test_logs_model_requires_ownership(self, client, auth_headers):
        """Model output requires task ownership."""
        ...
```

**Acceptance criteria:**
- [ ] >80% code coverage for new log modules
- [ ] All policy edge cases tested
- [ ] Integration with existing test infrastructure

---

### Phase 2: Container Isolation

**Objective:** Remove log directories from container mounts and create client library for gateway access.

#### Task 2.1: Modify Container Mount Configuration

**File:** `jib-container/jib_lib/setup_flow.py` (modify)

**Changes:**
1. Create selective mount for `~/.jib-sharing/` that excludes log directories
2. Add `JIB_CONTAINER=1` environment variable

**Current code (line ~376-379):**
```python
# Mount sharing directory (beads, notifications, incoming tasks, etc.)
if Config.SHARING_DIR.exists():
    sharing_container_path = "/home/jib/sharing"
    mount_args.extend(["-v", f"{Config.SHARING_DIR}:{sharing_container_path}:rw"])
```

**New code:**
```python
# Mount sharing directory components selectively (security: exclude log directories)
if Config.SHARING_DIR.exists():
    sharing_container_path = "/home/jib/sharing"

    # Mount specific subdirectories instead of entire ~/.jib-sharing/
    # This excludes container-logs/ and logs/ for security
    sharing_subdirs = [
        "incoming",        # Task files from Slack
        "responses",       # Outgoing responses
        "notifications",   # Notification files
        "context",         # Context save/load
        "tmp",             # Temporary files
    ]

    for subdir in sharing_subdirs:
        host_path = Config.SHARING_DIR / subdir
        container_path = f"{sharing_container_path}/{subdir}"
        if host_path.exists():
            mount_args.extend(["-v", f"{host_path}:{container_path}:rw"])
        else:
            # Create the directory if it doesn't exist
            host_path.mkdir(parents=True, exist_ok=True)
            mount_args.extend(["-v", f"{host_path}:{container_path}:rw"])

    # Mount beads directory (read-write for task tracking)
    beads_path = Config.SHARING_DIR / "beads"
    if beads_path.exists():
        mount_args.extend(["-v", f"{beads_path}:/home/jib/beads:rw"])

    if not quiet:
        print("  • ~/sharing/ (beads, notifications, tasks) [logs excluded for security]")
```

**Also add JIB_CONTAINER environment variable in runtime.py:**

**File:** `jib-container/jib_lib/runtime.py` (modify)

Find where environment variables are set and add:
```python
# Add to container environment variables
env_args.extend(["-e", "JIB_CONTAINER=1"])
```

**Acceptance criteria:**
- [ ] Log directories (`container-logs/`, `logs/`) not mounted in container
- [ ] Beads, notifications, incoming tasks still accessible
- [ ] `JIB_CONTAINER=1` set in container environment
- [ ] Existing functionality (beads, Slack tasks) still works

---

#### Task 2.2: Create LogClient Python Module

**File:** `shared/jib_logging/log_client.py` (new file)

**Purpose:** Python client for containers to access logs via gateway API.

```python
# shared/jib_logging/log_client.py
"""Log client for gateway-mediated log access.

Use this client from within containers to access logs via the gateway API.
Direct filesystem access to logs is not available in containers.

Usage:
    from jib_logging import LogClient

    client = LogClient()
    logs = client.get_task_logs("task-20260128-123456")
    print(logs.content)

    results = client.search("error", limit=10)
    for entry in results:
        print(f"{entry.line_number}: {entry.content}")
"""

import json
import os
from dataclasses import dataclass
from typing import Any
from urllib.error import HTTPError, URLError
from urllib.request import Request, urlopen


@dataclass
class LogContent:
    """Log content returned from gateway."""
    task_id: str | None
    container_id: str | None
    log_file: str | None
    content: str
    lines: int
    truncated: bool


@dataclass
class LogEntry:
    """A log entry from the list endpoint."""
    container_id: str
    task_id: str | None
    thread_ts: str | None
    log_file: str | None
    timestamp: str


@dataclass
class SearchResult:
    """A single search result."""
    log_file: str
    line_number: int
    content: str
    task_id: str | None
    container_id: str | None


class LogClientError(Exception):
    """Error from log client operations."""
    def __init__(self, message: str, status_code: int | None = None, details: dict | None = None):
        super().__init__(message)
        self.status_code = status_code
        self.details = details


class LogClient:
    """Client for accessing logs via gateway API.

    Automatically reads configuration from environment variables:
    - GATEWAY_URL: Gateway base URL (default: http://jib-gateway:9847)
    - JIB_GATEWAY_SECRET: Authentication secret
    - JIB_CONTAINER_ID: Current container ID (set by orchestrator)
    - JIB_TASK_ID: Current task ID (if applicable)
    """

    def __init__(
        self,
        gateway_url: str | None = None,
        auth_secret: str | None = None,
        container_id: str | None = None,
        task_id: str | None = None,
    ):
        self.gateway_url = gateway_url or os.environ.get(
            "GATEWAY_URL", "http://jib-gateway:9847"
        )
        self.auth_secret = auth_secret or os.environ.get("JIB_GATEWAY_SECRET", "")
        self.container_id = container_id or os.environ.get("JIB_CONTAINER_ID", "")
        self.task_id = task_id or os.environ.get("JIB_TASK_ID")

        if not self.auth_secret:
            raise LogClientError("JIB_GATEWAY_SECRET not set")
        if not self.container_id:
            raise LogClientError("JIB_CONTAINER_ID not set")

    def _make_request(
        self,
        endpoint: str,
        params: dict[str, Any] | None = None,
    ) -> dict[str, Any]:
        """Make authenticated request to gateway."""
        url = f"{self.gateway_url}{endpoint}"
        if params:
            query = "&".join(f"{k}={v}" for k, v in params.items() if v is not None)
            url = f"{url}?{query}"

        headers = {
            "Authorization": f"Bearer {self.auth_secret}",
            "X-Container-ID": self.container_id,
            "Content-Type": "application/json",
        }
        if self.task_id:
            headers["X-Task-ID"] = self.task_id

        request = Request(url, headers=headers, method="GET")

        try:
            with urlopen(request, timeout=30) as response:
                data = json.loads(response.read().decode())
                return data
        except HTTPError as e:
            body = e.read().decode()
            try:
                error_data = json.loads(body)
                raise LogClientError(
                    error_data.get("message", str(e)),
                    status_code=e.code,
                    details=error_data.get("details"),
                )
            except json.JSONDecodeError:
                raise LogClientError(str(e), status_code=e.code)
        except URLError as e:
            raise LogClientError(f"Connection error: {e}")

    def list_logs(self, limit: int = 50, offset: int = 0) -> list[LogEntry]:
        """List recent log entries for this container."""
        response = self._make_request(
            "/api/v1/logs/list",
            params={"limit": limit, "offset": offset},
        )
        return [
            LogEntry(
                container_id=e["container_id"],
                task_id=e.get("task_id"),
                thread_ts=e.get("thread_ts"),
                log_file=e.get("log_file"),
                timestamp=e["timestamp"],
            )
            for e in response.get("data", {}).get("entries", [])
        ]

    def get_task_logs(self, task_id: str, max_lines: int = 1000) -> LogContent:
        """Get logs for a specific task."""
        response = self._make_request(
            f"/api/v1/logs/task/{task_id}",
            params={"lines": max_lines},
        )
        data = response.get("data", {})
        return LogContent(
            task_id=data.get("task_id"),
            container_id=data.get("container_id"),
            log_file=data.get("log_file"),
            content=data.get("content", ""),
            lines=data.get("lines", 0),
            truncated=data.get("truncated", False),
        )

    def get_container_logs(self, max_lines: int = 1000) -> LogContent:
        """Get logs for this container."""
        response = self._make_request(
            f"/api/v1/logs/container/{self.container_id}",
            params={"lines": max_lines},
        )
        data = response.get("data", {})
        return LogContent(
            task_id=data.get("task_id"),
            container_id=data.get("container_id"),
            log_file=data.get("log_file"),
            content=data.get("content", ""),
            lines=data.get("lines", 0),
            truncated=data.get("truncated", False),
        )

    def search(self, pattern: str, limit: int = 100) -> list[SearchResult]:
        """Search logs for a pattern."""
        response = self._make_request(
            "/api/v1/logs/search",
            params={"pattern": pattern, "limit": limit, "scope": "self"},
        )
        return [
            SearchResult(
                log_file=r["log_file"],
                line_number=r["line_number"],
                content=r["content"],
                task_id=r.get("task_id"),
                container_id=r.get("container_id"),
            )
            for r in response.get("data", {}).get("results", [])
        ]

    def get_model_output(self, task_id: str) -> LogContent:
        """Get model output for a specific task."""
        response = self._make_request(f"/api/v1/logs/model/{task_id}")
        data = response.get("data", {})
        return LogContent(
            task_id=data.get("task_id"),
            container_id=data.get("container_id"),
            log_file=data.get("log_file"),
            content=data.get("content", ""),
            lines=data.get("lines", 0),
            truncated=data.get("truncated", False),
        )
```

**Also add to `shared/jib_logging/__init__.py`:**
```python
from .log_client import LogClient, LogClientError, LogContent, LogEntry, SearchResult
```

**Acceptance criteria:**
- [ ] Client works from within container
- [ ] All endpoints accessible via client methods
- [ ] Proper error handling with meaningful messages
- [ ] Environment variable configuration

---

#### Task 2.3: Update jib-logs Utility

**File:** `host-services/utilities/jib-logs/jib-logs` (modify)

**Changes:** Add container detection and gateway routing.

**Add at top of file (after imports):**
```python
def is_in_container() -> bool:
    """Detect if running inside a jib container."""
    # Primary: Check for JIB_CONTAINER environment variable
    if os.environ.get("JIB_CONTAINER") == "1":
        return True

    # Secondary: Check for absence of ~/.jib-sharing/
    if not Path.home().joinpath(".jib-sharing").exists():
        return True

    # Tertiary: Check for /.dockerenv
    if Path("/.dockerenv").exists():
        return True

    return False


def use_gateway_api() -> bool:
    """Check if we should use gateway API for log access."""
    # In container: always use gateway
    if is_in_container():
        return True

    # On host: use filesystem directly
    return False
```

**Modify main() to route through gateway when in container:**
```python
def main():
    # ... existing argument parsing ...

    if use_gateway_api():
        # Route through gateway API
        try:
            from jib_logging import LogClient
            client = LogClient()

            if args.task_id:
                logs = client.get_task_logs(args.task_id)
                print(logs.content)
            elif args.search:
                results = client.search(args.search, limit=args.limit or 100)
                for r in results:
                    print(f"{r.log_file}:{r.line_number}: {r.content}")
            else:
                entries = client.list_logs(limit=args.limit or 20)
                for e in entries:
                    print(f"{e.timestamp} {e.task_id or e.container_id}")
        except Exception as e:
            print(f"Error accessing logs via gateway: {e}", file=sys.stderr)
            sys.exit(1)
    else:
        # Existing direct filesystem logic
        # ... keep existing implementation ...
```

**Acceptance criteria:**
- [ ] In container: uses gateway API
- [ ] On host: uses filesystem directly
- [ ] Same CLI interface in both modes
- [ ] Helpful error messages when gateway unavailable

---

#### Task 2.4: Integration Tests

**File:** `gateway-sidecar/tests/test_log_integration.py` (new file)

**Test scenarios:**
- [ ] Container can access own logs via gateway
- [ ] Container cannot access other container's logs (403)
- [ ] Search scoped to own logs
- [ ] jib-logs utility works in container mode
- [ ] jib-logs utility works on host
- [ ] LogClient error handling

**Acceptance criteria:**
- [ ] End-to-end tests pass in container environment
- [ ] Tests verify policy enforcement
- [ ] Tests cover error scenarios

---

### Phase 3: Migration

**Objective:** Migrate existing tools and documentation to use gateway-mediated log access.

#### Task 3.1: Add Environment Variables to Orchestrator

**File:** `jib-container/jib_lib/runtime.py` (modify)

**Changes:** Add container identity environment variables.

```python
# In _build_docker_command() or equivalent function, add:
env_args.extend([
    "-e", "JIB_CONTAINER=1",
    "-e", f"JIB_CONTAINER_ID={container_id}",
])

# If task_id is available:
if task_id:
    env_args.extend(["-e", f"JIB_TASK_ID={task_id}"])
```

**Acceptance criteria:**
- [ ] `JIB_CONTAINER=1` set in all containers
- [ ] `JIB_CONTAINER_ID` set to container's unique ID
- [ ] `JIB_TASK_ID` set when processing a task

---

#### Task 3.2: Audit Dependent Scripts

**Files to audit (identified via grep):**

| File | Current Usage | Migration Needed |
|------|---------------|------------------|
| `slack-receiver/host_command_handler.py` | Reads task files | No (host-side) |
| `slack-notifier/slack-notifier.py` | Writes responses | No (host-side) |
| `jib-logs/jib-logs` | Reads logs | Yes (done in 2.3) |
| `shared/jib_config/configs/github.py` | Config path | No (not log access) |
| `jib-container/entrypoint.py` | Path references | Review |
| `docs/*` | Documentation | Update paths |

**Task:** Review each file and update if needed to use LogClient when in container.

**Acceptance criteria:**
- [ ] All scripts using log paths audited
- [ ] Container-side scripts updated to use LogClient
- [ ] Host-side scripts unchanged (direct access OK)

---

#### Task 3.3: Update CLAUDE.md Documentation

**File:** `CLAUDE.md` (in container image)

**Changes:**
- Remove references to direct log file paths
- Document LogClient usage
- Update jib-logs examples

**Add section:**
```markdown
## Accessing Logs

Logs are accessed via the gateway sidecar for security. Direct filesystem access
to log directories is not available in containers.

### Using LogClient (Python)

```python
from jib_logging import LogClient

client = LogClient()

# Get logs for a task
logs = client.get_task_logs("task-20260128-123456")
print(logs.content)

# Search logs
results = client.search("error", limit=50)
for r in results:
    print(f"{r.line_number}: {r.content}")

# List recent logs
entries = client.list_logs(limit=20)
```

### Using jib-logs CLI

```bash
# View logs for a task
jib-logs task-20260128-123456

# Search logs
jib-logs --search "error"

# List recent logs
jib-logs
```

**Note:** The `--tail` option is not yet available in gateway mode. Real-time log
streaming will be added in a future phase.
```

**Acceptance criteria:**
- [ ] CLAUDE.md updated with new log access patterns
- [ ] Old direct-path references removed
- [ ] Examples work in container

---

#### Task 3.4: Deprecation Warnings (Optional)

**File:** `host-services/utilities/jib-logs/jib-logs`

**Changes:** Add deprecation warning for direct access attempts from container.

```python
# During migration period, warn about direct access
if is_in_container() and Path.home().joinpath(".jib-sharing").exists():
    print(
        "WARNING: Direct log access from container is deprecated. "
        "Update to use gateway API (jib-logs or LogClient). "
        "Direct access will be removed in v2.x.",
        file=sys.stderr,
    )
```

**Acceptance criteria:**
- [ ] Warning emitted for deprecated access patterns
- [ ] Warning includes migration guidance
- [ ] Does not break existing functionality

---

#### Task 3.5: Monitoring and Alerting

**Files:** Gateway audit logs, monitoring configuration

**Changes:**
- Add metrics for log access operations
- Alert on elevated 403 rates (may indicate migration issues)
- Dashboard for log access patterns

**Metrics to track:**
- `gateway.logs.access.success` - Successful log access by operation type
- `gateway.logs.access.denied` - Policy denials by reason
- `gateway.logs.access.latency` - Response time distribution
- `gateway.logs.direct_access.attempts` - Direct filesystem attempts (during migration)

**Acceptance criteria:**
- [ ] Audit logs capture all log access
- [ ] Metrics exportable for monitoring
- [ ] Alert thresholds defined

---

### Phase 4: Enhancements (Future)

**Objective:** Add advanced features for improved usability and security.

#### Task 4.1: Real-time Log Streaming (WebSocket)

**Priority:** High (debugging workflow impact)

**Design:**
- New endpoint: `GET /api/v1/logs/stream/<task_id>`
- Upgrade to WebSocket connection
- Same policy enforcement as read access
- Server-side: use `inotify` to detect new log content
- Client-side: update LogClient with `stream_logs()` method

**Implementation notes:**
- Flask-SocketIO or similar for WebSocket support
- Consider chunking for large log output
- Heartbeat to detect disconnections

---

#### Task 4.2: Role-Based Access Control

**Priority:** Medium (admin debugging scenarios)

**Design:**
- Add admin role for cross-container log access
- Separate authentication mechanism (not container token)
- Audit all admin access with elevated logging

**Implementation notes:**
- Admin token stored securely (not in container)
- Time-limited access sessions
- Requires human approval for access

---

#### Task 4.3: Log Retention Policies

**Priority:** Low (operational)

**Design:**
- Gateway API for log cleanup: `POST /api/v1/logs/cleanup`
- Policy-based retention (by age, by task status)
- Audit trail for deletions

---

#### Task 4.4: Search Indexing

**Priority:** Low (performance optimization)

**Design:**
- Background indexer for log content
- Full-text search with better performance
- Index updates on log writes

---

## Implementation Checklist Summary

### Phase 1: Gateway Log Endpoints
- [ ] Task 1.1: Create `log_index.py`
- [ ] Task 1.2: Create `log_policy.py`
- [ ] Task 1.3: Create `log_reader.py`
- [ ] Task 1.4: Add endpoints to `gateway.py`
- [ ] Task 1.5: Unit tests for log access

### Phase 2: Container Isolation
- [ ] Task 2.1: Modify container mounts (selective ~/.jib-sharing/)
- [ ] Task 2.2: Create `LogClient` module
- [ ] Task 2.3: Update `jib-logs` utility
- [ ] Task 2.4: Integration tests

### Phase 3: Migration
- [ ] Task 3.1: Add environment variables to orchestrator
- [ ] Task 3.2: Audit dependent scripts
- [ ] Task 3.3: Update CLAUDE.md documentation
- [ ] Task 3.4: Deprecation warnings (optional)
- [ ] Task 3.5: Monitoring and alerting

### Phase 4: Enhancements (Future)
- [ ] Task 4.1: Real-time log streaming (WebSocket)
- [ ] Task 4.2: Role-based access control
- [ ] Task 4.3: Log retention policies
- [ ] Task 4.4: Search indexing

## Related ADRs

| ADR | Relationship |
|-----|--------------|
| [ADR-Git-Isolation-Architecture](../implemented/ADR-Git-Isolation-Architecture.md) | Model for gateway-based isolation |
| [ADR-Internet-Tool-Access-Lockdown](./ADR-Internet-Tool-Access-Lockdown.md) | Parent security framework |
| [ADR-Standardized-Logging-Interface](./ADR-Standardized-Logging-Interface.md) | Defines log format and generation |
| [ADR-Autonomous-Software-Engineer](./ADR-Autonomous-Software-Engineer.md) | Parent ADR defining security requirements |

---

**Last Updated:** 2026-01-28
**Next Review:** 2026-02-28 (Monthly)
**Status:** Proposed
