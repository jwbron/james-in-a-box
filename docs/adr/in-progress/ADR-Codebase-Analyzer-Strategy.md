# ADR: Codebase Analyzer Strategy

**Driver:** James Wiesebron
**Approver:** TBD
**Contributors:** James Wiesebron, jib (AI Pair Programming)
**Informed:** Engineering teams
**Proposed:** November 2025
**Status:** Not Implemented

## Table of Contents

- [Context](#context)
- [Decision](#decision)
- [Decision Matrix](#decision-matrix)
- [Implementation Details](#implementation-details)
- [Analysis Layers](#analysis-layers)
- [External Research Integration](#external-research-integration)
- [PR-Based Workflow](#pr-based-workflow)
- [Evaluation and Success Metrics](#evaluation-and-success-metrics)
- [Migration Strategy](#migration-strategy)
- [Consequences](#consequences)
- [Decision Permanence](#decision-permanence)
- [Alternatives Considered](#alternatives-considered)
- [References](#references)

## Context

### Background

**Problem Statement:**

LLM-powered agents like james-in-a-box need comprehensive understanding of codebases to make high-quality contributions. The current approach relies on ad-hoc code exploration during tasks, which leads to:

1. **Inconsistent Code Quality:** Without systematic analysis, the agent may not follow established patterns or may introduce inconsistencies
2. **Missed Technical Debt:** Existing issues (poor test coverage, outdated dependencies, security vulnerabilities) go undetected
3. **Pattern Drift:** Codebases evolve without documentation of how patterns should be applied, leading to divergent implementations
4. **Reactive vs. Proactive:** The agent only discovers issues when they cause problems, rather than identifying them proactively
5. **No External Validation:** Internal patterns are followed even if they deviate from industry best practices

**Relationship to LLM Documentation Index Strategy:**

This ADR complements the [LLM Documentation Index Strategy](../implemented/ADR-LLM-Documentation-Index-Strategy.md) by focusing on automated analysis rather than documentation authoring. While that ADR establishes how documentation is structured and maintained, this ADR defines how the codebase itself is systematically analyzed to:

- Feed into documentation generation (Phase 2 of the Documentation Index Strategy)
- Validate code against documented patterns
- Identify gaps between current practice and best practice
- Enable the "External Best Practices Integration" (Phase 5) with codebase-specific context

### What We're Deciding

This ADR establishes the strategy for:

1. **High-Level Analysis:** Architectural patterns, consistency, dependency usage, design patterns
2. **Low-Level Analysis:** File-by-file analysis, test coverage, code quality metrics
3. **External Research Integration:** Web-based best practices research, industry trends monitoring
4. **Continuous Improvement:** Feedback loops from analysis to documentation and code changes

### Key Requirements

1. **Comprehensive Coverage:** Analyze all aspects from architecture to individual files
2. **External Validation:** Incorporate industry best practices, not just internal patterns
3. **Actionable Output:** Generate specific, prioritized recommendations
4. **Integration with Workflows:** Analysis feeds into PRs, documentation, and task planning
5. **Minimal False Positives:** High signal-to-noise ratio through context-aware analysis

### Current State

**james-in-a-box currently:**
- Has basic codebase index generation via `codebase.json` (from Documentation Index Strategy Phase 2)
- Uses ad-hoc grep/glob/read for code exploration
- Relies on human-provided context for pattern guidance
- No systematic test coverage analysis
- No automated dependency vulnerability scanning
- No external best practices validation

### Industry Context

The landscape for codebase analysis has evolved significantly:

| Approach | Description | Examples | Limitations |
|----------|-------------|----------|-------------|
| **Traditional Static Analysis** | Rule-based pattern matching | SonarQube, ESLint, Pylint | High false positives, limited context |
| **AI-Powered Code Review** | LLM-based semantic analysis | Qodo, CodeRabbit, Graphite | Often per-PR focused, not codebase-wide |
| **Software Composition Analysis** | Dependency vulnerability scanning | Snyk, OWASP Dependency-Check | Security-focused, misses patterns |
| **Code Intelligence Platforms** | Full codebase understanding | CodeScene, Sourcegraph | Expensive, complex infrastructure |
| **LLM Codebase Understanding** | Large context window analysis | Claude, GPT-4 | Requires orchestration, no persistence |

**Emerging Best Practices (2024-2025):**

- AI-powered analysis with 200K+ token context windows enables true architectural understanding
- Combining static analysis with LLM semantic understanding reduces false positives
- Baseline approach for legacy code: focus enforcement on new code while planning remediation
- Integration into CI/CD pipelines catches issues as early as possible
- Mutation testing validates test suite effectiveness beyond simple coverage metrics

## Decision

**We will implement a multi-layered codebase analyzer with external research integration that produces actionable recommendations via PRs.**

### Core Principles

1. **Layered Analysis:** Separate high-level (architecture, patterns) from low-level (files, coverage) concerns
2. **External Validation:** Every analysis category includes external best practices research
3. **PR-Based Output:** All findings and recommendations surface as reviewable PRs
4. **Continuous, Not One-Time:** Analysis runs on schedule and on significant changes
5. **Context-Aware:** Use LLM semantic understanding to reduce false positives

### Approach Summary

| Layer | Analysis Focus | Output |
|-------|----------------|--------|
| **Architecture** | Design patterns, component boundaries, dependency flow | Architecture reports, ADR suggestions |
| **Consistency** | Pattern adherence, code style, naming conventions | Consistency reports, refactoring PRs |
| **Dependencies** | Version currency, security vulnerabilities, license compliance | Dependency update PRs, security alerts |
| **Test Coverage** | Coverage gaps, test quality, mutation testing results | Coverage reports, test improvement PRs |
| **File-Level** | Complexity, dead code, documentation coverage | File health reports, cleanup PRs |
| **Feature Discovery** | New features, components, capabilities | FEATURES.md updates, feature documentation PRs |
| **External Trends** | Industry best practices, emerging patterns, evolving standards | Research updates to ADRs, recommendations |

## Decision Matrix

| Decision Area | Chosen Approach | Key Rationale | Rejected Alternatives |
|---------------|-----------------|---------------|----------------------|
| **Analysis Engine** | LLM + tree-sitter hybrid | Semantic understanding + structural accuracy | Pure LLM (less reliable), Pure static (limited context) |
| **Coverage Analysis** | Multi-metric approach | Line + branch + mutation for comprehensive view | Line coverage only (misleading) |
| **Dependency Scanning** | SCA + reachability analysis | Prioritize actually-used vulnerabilities | Alert on all CVEs (noise) |
| **External Research** | Scheduled web research + PR output | Current info, reviewable before adoption | Static docs only (stale) |
| **Output Format** | PR-based recommendations | Enables review, discussion, iteration | Direct commits (no review) |

## Implementation Details

### 1. Analysis Engine Architecture

**Multi-Pass Analysis Pipeline:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Codebase Analyzer Pipeline                    â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 PHASE 1: STRUCTURAL ANALYSIS              â”‚   â”‚
â”‚  â”‚  - Parse code with tree-sitter for accurate AST          â”‚   â”‚
â”‚  â”‚  - Extract dependency graph (imports, calls, inheritance)â”‚   â”‚
â”‚  â”‚  - Generate component boundaries and relationships       â”‚   â”‚
â”‚  â”‚  - Identify entry points and data flow paths             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 PHASE 2: SEMANTIC ANALYSIS                â”‚   â”‚
â”‚  â”‚  - LLM analyzes code purpose and intent                  â”‚   â”‚
â”‚  â”‚  - Pattern recognition (design patterns, anti-patterns)  â”‚   â”‚
â”‚  â”‚  - Consistency check against documented conventions      â”‚   â”‚
â”‚  â”‚  - Complexity and maintainability assessment             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 PHASE 3: COVERAGE ANALYSIS                â”‚   â”‚
â”‚  â”‚  - Run test suite with coverage instrumentation          â”‚   â”‚
â”‚  â”‚  - Calculate line, branch, and function coverage         â”‚   â”‚
â”‚  â”‚  - Optional: mutation testing for test effectiveness     â”‚   â”‚
â”‚  â”‚  - Identify untested critical paths                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 PHASE 4: SECURITY ANALYSIS                â”‚   â”‚
â”‚  â”‚  - Dependency vulnerability scan (CVE database)          â”‚   â”‚
â”‚  â”‚  - Reachability analysis for used vulnerabilities        â”‚   â”‚
â”‚  â”‚  - License compliance check                              â”‚   â”‚
â”‚  â”‚  - Secret detection scan                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚              PHASE 5: EXTERNAL RESEARCH                   â”‚   â”‚
â”‚  â”‚  - Web search for current best practices                 â”‚   â”‚
â”‚  â”‚  - Check authoritative sources (official docs, RFCs)     â”‚   â”‚
â”‚  â”‚  - Compare internal patterns to industry standards       â”‚   â”‚
â”‚  â”‚  - Identify emerging trends and evolving practices       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚               PHASE 6: SYNTHESIS & OUTPUT                 â”‚   â”‚
â”‚  â”‚  - Prioritize findings by severity and impact            â”‚   â”‚
â”‚  â”‚  - Generate actionable recommendations                   â”‚   â”‚
â”‚  â”‚  - Create PRs for code changes                           â”‚   â”‚
â”‚  â”‚  - Update documentation and ADRs                         â”‚   â”‚
â”‚  â”‚  - Update FEATURES.md with discovered features           â”‚   â”‚
â”‚  â”‚  - Produce summary reports                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. High-Level Analysis Components

#### 2.1 Architecture Analysis

**Purpose:** Understand and validate the system's architectural design.

**Analysis Points:**

| Aspect | What We Analyze | How We Validate |
|--------|-----------------|-----------------|
| **Component Boundaries** | Module structure, dependency directions | Check against documented architecture |
| **Design Patterns** | Observable patterns (Factory, Observer, etc.) | Compare to pattern best practices |
| **Coupling & Cohesion** | Inter-module dependencies, module responsibilities | Flag high coupling, low cohesion |
| **Layer Violations** | Cross-layer imports, bypassed abstractions | Detect violations of documented layers |
| **Circular Dependencies** | Dependency cycles between modules | Flag all cycles for review |

**Output Format:**

```json
{
  "architecture_analysis": {
    "timestamp": "2025-11-29T12:00:00Z",
    "components": [
      {
        "name": "watchers",
        "path": "src/watchers/",
        "responsibility": "Event-driven monitoring and analysis",
        "patterns_detected": ["observer", "event-driven"],
        "dependencies": ["mcp", "notifications", "beads"],
        "issues": []
      }
    ],
    "violations": [
      {
        "type": "layer_violation",
        "severity": "medium",
        "location": "src/api/handlers.py:45",
        "description": "Direct database access bypassing service layer",
        "recommendation": "Route through service layer for consistency"
      }
    ],
    "patterns": {
      "consistent": ["event-driven", "dependency-injection"],
      "inconsistent": [
        {
          "pattern": "error-handling",
          "variations": 3,
          "locations": ["src/watchers/", "src/api/", "src/workers/"],
          "recommendation": "Standardize error handling approach"
        }
      ]
    }
  }
}
```

#### 2.2 Consistency Analysis

**Purpose:** Ensure uniform application of patterns and conventions.

**Analysis Points:**

| Category | What We Check | Validation Method |
|----------|---------------|-------------------|
| **Naming Conventions** | Variable, function, class, file names | Regex patterns + LLM semantic check |
| **Code Style** | Formatting, indentation, imports | Linter output + custom rules |
| **Pattern Application** | Consistent use of established patterns | AST matching + LLM validation |
| **Error Handling** | Consistent exception/error handling | Pattern matching across codebase |
| **Logging** | Consistent log formats and levels | Log statement analysis |
| **Documentation** | Docstring presence and format | Coverage analysis |

**Inconsistency Report Format:**

```markdown
## Consistency Analysis Report - 2025-11-29

### Naming Convention Violations

| Location | Current | Expected | Severity |
|----------|---------|----------|----------|
| `src/utils/helpers.py:getData` | camelCase | snake_case | Low |
| `src/api/UserManager.py` | PascalCase file | snake_case file | Medium |

### Pattern Inconsistencies

**Error Handling** (3 variations detected):
1. `src/watchers/` - Uses custom ErrorHandler class
2. `src/api/` - Uses try/except with logging
3. `src/workers/` - Uses result types

**Recommendation:** Standardize on approach #1 (ErrorHandler) per ADR-Error-Handling.

### Action Items
- [ ] PR #X: Rename `getData` to `get_data`
- [ ] PR #Y: Standardize error handling in `src/api/`
```

#### 2.3 Dependency Analysis

**Purpose:** Manage external and internal dependencies for security and currency.

**Analysis Points:**

| Category | What We Analyze | Tools/Methods |
|----------|-----------------|---------------|
| **Version Currency** | Outdated packages | pip-audit, npm audit, dependabot |
| **Security Vulnerabilities** | CVEs in dependencies | NVD database, Snyk, OWASP DC |
| **Reachability** | Which vulnerabilities are actually used | Call graph analysis |
| **License Compliance** | License compatibility | License checker tools |
| **Unused Dependencies** | Packages imported but never used | Import analysis |
| **Transitive Dependencies** | Indirect dependency risks | Dependency tree analysis |

**Dependency Report Format:**

```json
{
  "dependency_analysis": {
    "timestamp": "2025-11-29T12:00:00Z",
    "summary": {
      "total_dependencies": 45,
      "outdated": 8,
      "vulnerabilities": {
        "critical": 0,
        "high": 1,
        "medium": 3,
        "low": 5
      },
      "unused": 2
    },
    "vulnerabilities": [
      {
        "package": "requests",
        "version": "2.28.0",
        "vulnerability": "CVE-2024-XXXXX",
        "severity": "high",
        "fixed_in": "2.31.0",
        "reachable": true,
        "usage_locations": ["src/api/client.py:23", "src/workers/fetcher.py:89"],
        "recommendation": "Upgrade to 2.31.0 immediately"
      }
    ],
    "outdated": [
      {
        "package": "anthropic",
        "current": "0.30.0",
        "latest": "0.39.0",
        "age_days": 45,
        "breaking_changes": true,
        "recommendation": "Review changelog before upgrade"
      }
    ],
    "unused": [
      {
        "package": "deprecated-lib",
        "reason": "No imports detected",
        "recommendation": "Remove from requirements.txt"
      }
    ]
  }
}
```

### 3. Low-Level Analysis Components

#### 3.1 Test Coverage Analysis

**Purpose:** Ensure adequate test coverage with focus on quality, not just quantity.

**Multi-Metric Approach:**

| Metric | Target | Purpose |
|--------|--------|---------|
| **Line Coverage** | â‰¥80% | Basic execution coverage |
| **Branch Coverage** | â‰¥70% | Decision path coverage |
| **Function Coverage** | â‰¥90% | Entry point coverage |
| **Mutation Score** | â‰¥60% | Test effectiveness |

**Coverage Analysis Process:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Test Coverage Analysis                        â”‚
â”‚                                                                  â”‚
â”‚  1. RUN TEST SUITE WITH INSTRUMENTATION                         â”‚
â”‚     pytest --cov=src --cov-report=json --cov-branch             â”‚
â”‚                                                                  â”‚
â”‚  2. CALCULATE MULTI-METRIC COVERAGE                             â”‚
â”‚     - Line coverage by file and module                          â”‚
â”‚     - Branch coverage for conditionals                          â”‚
â”‚     - Function coverage for entry points                        â”‚
â”‚                                                                  â”‚
â”‚  3. IDENTIFY COVERAGE GAPS                                       â”‚
â”‚     - Untested files (0% coverage)                              â”‚
â”‚     - Under-tested files (<50% coverage)                        â”‚
â”‚     - Untested branches in critical paths                       â”‚
â”‚                                                                  â”‚
â”‚  4. ASSESS TEST QUALITY (OPTIONAL MUTATION TESTING)             â”‚
â”‚     - Run mutation testing on critical modules                  â”‚
â”‚     - Calculate mutation score                                   â”‚
â”‚     - Identify surviving mutants                                â”‚
â”‚                                                                  â”‚
â”‚  5. PRIORITIZE IMPROVEMENTS                                      â”‚
â”‚     - Critical paths without tests                              â”‚
â”‚     - High-complexity code with low coverage                    â”‚
â”‚     - Security-sensitive code without tests                     â”‚
â”‚                                                                  â”‚
â”‚  6. GENERATE RECOMMENDATIONS                                     â”‚
â”‚     - Specific test cases to add                                â”‚
â”‚     - Files needing test improvement                            â”‚
â”‚     - Test quality issues to address                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Coverage Report Format:**

```markdown
## Test Coverage Report - 2025-11-29

### Summary

| Metric | Current | Target | Status |
|--------|---------|--------|--------|
| Line Coverage | 75.2% | 80% | âš ï¸ Below target |
| Branch Coverage | 68.4% | 70% | âš ï¸ Below target |
| Function Coverage | 92.1% | 90% | âœ… Meets target |
| Mutation Score | 58.3% | 60% | âš ï¸ Below target |

### Critical Coverage Gaps

| File | Coverage | Complexity | Priority |
|------|----------|------------|----------|
| `src/auth/jwt_validator.py` | 45% | High | ğŸ”´ Critical |
| `src/api/handlers.py` | 52% | Medium | ğŸŸ¡ High |
| `src/workers/processor.py` | 61% | Medium | ğŸŸ¡ Medium |

### Recommended Test Cases

1. **`src/auth/jwt_validator.py`** (Security-critical)
   - Test: Expired token rejection
   - Test: Invalid signature handling
   - Test: Missing claims validation

2. **`src/api/handlers.py`**
   - Test: Error response formatting
   - Test: Rate limiting edge cases

### Mutation Testing Results

**Top Surviving Mutants:**
- `src/utils/validators.py:23` - Boundary condition change survived
- `src/api/auth.py:45` - Null check removal survived

**Action:** These indicate tests pass but don't verify exact behavior.
```

#### 3.2 File-Level Analysis

**Purpose:** Analyze individual files for quality, complexity, and maintainability.

**Analysis Metrics:**

| Metric | Description | Threshold |
|--------|-------------|-----------|
| **Cyclomatic Complexity** | Number of independent paths | â‰¤10 per function |
| **Lines of Code** | File size indicator | â‰¤500 lines per file |
| **Function Length** | Function size indicator | â‰¤50 lines per function |
| **Documentation Coverage** | Docstring presence | 100% for public APIs |
| **Dead Code** | Unused functions/classes | 0 instances |
| **Code Duplication** | Copy-paste detection | <5% duplication |

**File Health Report:**

```json
{
  "file_analysis": {
    "timestamp": "2025-11-29T12:00:00Z",
    "files_analyzed": 127,
    "summary": {
      "healthy": 98,
      "needs_attention": 21,
      "critical": 8
    },
    "critical_files": [
      {
        "path": "src/api/handlers.py",
        "issues": [
          {
            "type": "high_complexity",
            "detail": "Function `process_request` has complexity 23",
            "line": 145,
            "recommendation": "Split into smaller functions"
          },
          {
            "type": "missing_documentation",
            "detail": "3 public functions without docstrings",
            "lines": [45, 89, 156]
          }
        ],
        "health_score": 42
      }
    ],
    "dead_code": [
      {
        "path": "src/utils/legacy.py",
        "symbol": "deprecated_function",
        "line": 23,
        "last_used": "2024-06-15",
        "recommendation": "Remove or document if intentionally kept"
      }
    ],
    "duplication": [
      {
        "locations": ["src/api/v1/auth.py:45-67", "src/api/v2/auth.py:52-74"],
        "lines": 22,
        "recommendation": "Extract to shared module"
      }
    ]
  }
}
```

### 4. Feature Discovery and FEATURES.md Integration

**Purpose:** Automatically discover and document features, components, and capabilities through codebase analysis.

**Integration with Feature Analyzer ADR:**

This section implements the "Weekly Code Analysis" component from the [Feature Analyzer - Documentation Sync](../implemented/ADR-Feature-Analyzer-Documentation-Sync.md) ADR. While that ADR focuses on ADR-triggered documentation updates, the codebase analyzer provides the automated feature discovery mechanism.

#### 4.1 Feature Discovery Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Feature Discovery and Classification                  â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 1. FEATURE IDENTIFICATION                 â”‚   â”‚
â”‚  â”‚  From structural analysis:                                â”‚   â”‚
â”‚  â”‚  - Public APIs and entry points                           â”‚   â”‚
â”‚  â”‚  - Major components and modules                           â”‚   â”‚
â”‚  â”‚  - Integration points (external services, databases)      â”‚   â”‚
â”‚  â”‚  From semantic analysis:                                  â”‚   â”‚
â”‚  â”‚  - Feature purpose and scope (via LLM)                    â”‚   â”‚
â”‚  â”‚  - User-facing vs internal capabilities                   â”‚   â”‚
â”‚  â”‚  - Related features and dependencies                      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 2. FEATURE CLASSIFICATION                 â”‚   â”‚
â”‚  â”‚  Classify each feature:                                   â”‚   â”‚
â”‚  â”‚  - Category (integration, automation, infrastructure)     â”‚   â”‚
â”‚  â”‚  - Maturity (experimental, stable, deprecated)            â”‚   â”‚
â”‚  â”‚  - Scope (core, optional, extension)                      â”‚   â”‚
â”‚  â”‚  - Source locations (files, directories)                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 3. FEATURES.md SYNC                       â”‚   â”‚
â”‚  â”‚  Compare with existing FEATURES.md:                       â”‚   â”‚
â”‚  â”‚  - New features: Add to FEATURES.md                       â”‚   â”‚
â”‚  â”‚  - Changed features: Update description/location          â”‚   â”‚
â”‚  â”‚  - Removed features: Mark as deprecated or remove         â”‚   â”‚
â”‚  â”‚  - Status updates: Sync with ADR status (if applicable)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 4. PR GENERATION                          â”‚   â”‚
â”‚  â”‚  Create PR with:                                          â”‚   â”‚
â”‚  â”‚  - Updated FEATURES.md                                    â”‚   â”‚
â”‚  â”‚  - Summary of changes (added, updated, removed)           â”‚   â”‚
â”‚  â”‚  - Links to source code locations                         â”‚   â”‚
â”‚  â”‚  - Recommendations for documentation updates              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.2 Feature Detection Heuristics

| Indicator | What It Suggests | Confidence |
|-----------|------------------|------------|
| **Public API with documentation** | User-facing feature | High |
| **Entry point script (main.py, CLI)** | Core capability | High |
| **Integration module (e.g., slack/, github/)** | External integration feature | High |
| **Service class with multiple methods** | Component or subsystem | Medium |
| **Configuration section in settings** | Configurable feature | Medium |
| **Test suite with "feature" in name** | Distinct feature with tests | Medium |
| **README or docs mention** | Documented feature | High |

#### 4.3 Symbiotic Relationship with Documentation Pipeline

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Codebase Analyzer   â”‚
                    â”‚  (discovers features â”‚
                    â”‚   from code)         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    FEATURES.md       â”‚
                    â”‚  (structured list of â”‚
                    â”‚   features + locs)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚              â”‚              â”‚
                â–¼              â–¼              â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Doc Pipelineâ”‚  â”‚Drift Detectorâ”‚  â”‚ Analyzer     â”‚
       â”‚ (generates  â”‚  â”‚ (validates   â”‚  â”‚ Feedback     â”‚
       â”‚  guides)    â”‚  â”‚  FEATURES.md â”‚  â”‚ Loop (learns â”‚
       â”‚             â”‚  â”‚  vs code)    â”‚  â”‚  patterns)   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Integration Points:**
1. **Codebase Analyzer â†’ FEATURES.md**: Discovers features, updates FEATURES.md
2. **FEATURES.md â†’ Documentation Pipeline**: Provides structure for user guides
3. **Drift Detector â†’ Codebase Analyzer**: Reports drift for re-analysis
4. **Codebase Analyzer â†’ Feedback Loop**: Learns from past feature classifications

#### 4.4 FEATURES.md Update PR Template

```markdown
## Summary

Updates FEATURES.md based on latest codebase analysis.

### Changes

| Feature | Change | Location |
|---------|--------|----------|
| Slack Integration | Added | `jib-tasks/slack-receiver.py`, `watchers/slack-watcher.py` |
| GitHub Event Processor | Updated description | `jib-tasks/github-processor.py` |
| Legacy Auth Module | Marked deprecated | `auth/legacy.py` |

### New Features Discovered

**Slack Integration**
- **Purpose**: Bidirectional Slack communication for task management
- **Entry Points**: `slack-receiver.py` (incoming), `slack-notifier.py` (outgoing)
- **Category**: Integration
- **Maturity**: Stable
- **Related ADRs**: ADR-Slack-Integration-Strategy

### Features Removed

- **Legacy Auth Module**: Code removed in commit abc123

### Recommendations

- Consider creating user guide for Slack Integration
- Update setup docs to reference new FEATURES.md entries

## Test Plan

- [x] FEATURES.md syntax is valid
- [x] All file paths in FEATURES.md exist
- [x] Cross-references to ADRs are accurate

---
ğŸ¤– Generated by jib codebase analyzer
```

### 5. External Research Integration

**Purpose:** Validate internal practices against industry best practices and identify opportunities for improvement.

**Integration with LLM Documentation Index Strategy:**

This section mirrors Phase 6 (PR-Based Research Workflow) from the [LLM Documentation Index Strategy](../implemented/ADR-LLM-Documentation-Index-Strategy.md), but focuses on codebase-specific research rather than ADR research.

#### 4.1 Research Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              External Research Integration Workflow              â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 1. TOPIC IDENTIFICATION                   â”‚   â”‚
â”‚  â”‚  Based on analysis findings, identify topics to research: â”‚   â”‚
â”‚  â”‚  - Technologies used (Python, React, Docker, etc.)        â”‚   â”‚
â”‚  â”‚  - Patterns detected (error handling, auth, caching)      â”‚   â”‚
â”‚  â”‚  - Problem areas (security, performance, testing)         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 2. RESEARCH EXECUTION                     â”‚   â”‚
â”‚  â”‚  For each topic, gather external information:             â”‚   â”‚
â”‚  â”‚  - Official documentation (language/framework docs)       â”‚   â”‚
â”‚  â”‚  - Standards bodies (OWASP, NIST, W3C)                   â”‚   â”‚
â”‚  â”‚  - Technical blogs and conference talks                   â”‚   â”‚
â”‚  â”‚  - Popular open-source implementations                    â”‚   â”‚
â”‚  â”‚  - Recent publications and papers                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 3. GAP ANALYSIS                           â”‚   â”‚
â”‚  â”‚  Compare internal practices to external best practices:   â”‚   â”‚
â”‚  â”‚  - Aligned: Internal matches external recommendation      â”‚   â”‚
â”‚  â”‚  - Divergent: Internal differs (note if intentional)      â”‚   â”‚
â”‚  â”‚  - Missing: External practice not implemented            â”‚   â”‚
â”‚  â”‚  - Outdated: Internal practice superseded externally     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                            â”‚                                     â”‚
â”‚                            â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                 4. OUTPUT GENERATION                      â”‚   â”‚
â”‚  â”‚  Create actionable outputs:                               â”‚   â”‚
â”‚  â”‚  - Best practice comparison reports                       â”‚   â”‚
â”‚  â”‚  - ADR update PRs with research findings                  â”‚   â”‚
â”‚  â”‚  - Code improvement recommendations                       â”‚   â”‚
â”‚  â”‚  - New ADR proposals for significant gaps                â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 4.2 Research Categories and Sources

| Category | Topics | Authoritative Sources |
|----------|--------|----------------------|
| **Security** | Auth patterns, input validation, secrets management | OWASP, NIST, CWE database |
| **Testing** | Test patterns, coverage strategies, mutation testing | Martin Fowler, Testing Library docs |
| **Architecture** | Design patterns, microservices, event-driven | Domain-Driven Design resources, AWS/GCP patterns |
| **Performance** | Caching, database optimization, async patterns | Framework-specific docs, benchmark studies |
| **Dependencies** | Version management, security scanning, licensing | Snyk, Dependabot docs, OSS licensing guides |
| **Language-Specific** | Python/JS/Go best practices | Official language docs, PEPs, TC39 |

#### 4.3 Research Output Format

**Best Practice Comparison Report:**

```markdown
## Best Practice Comparison: Authentication

### Research Date: 2025-11-29
### Sources Consulted:
- OWASP Authentication Cheat Sheet (2024)
- Python Security Best Practices (python.org)
- JWT Best Practices (RFC 8725)

### Current Practice vs. Best Practice

| Aspect | Our Practice | Best Practice | Status | Action |
|--------|--------------|---------------|--------|--------|
| Token Algorithm | RS256 | RS256 or ES256 | âœ… Aligned | None |
| Token Expiration | 24 hours | 15-60 minutes + refresh | âš ï¸ Divergent | Consider shorter expiry |
| Secret Storage | Env vars | Secret Manager | âœ… Aligned | None |
| Rate Limiting | None | Per-user limits | âŒ Missing | Implement rate limiting |

### Recommendations

1. **High Priority:** Implement rate limiting per OWASP recommendations
2. **Medium Priority:** Consider shorter token expiration with refresh tokens
3. **Low Priority:** Add jti claim for token revocation support

### References
- [OWASP Auth Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Authentication_Cheat_Sheet.html)
- [RFC 8725: JWT Best Practices](https://tools.ietf.org/html/rfc8725)
```

#### 4.4 Scheduled Research Tasks

```yaml
# codebase-research-schedule.yaml
schedules:
  - name: "Security Best Practices"
    cron: "0 0 * * 0"  # Weekly on Sunday
    topics:
      - "authentication best practices {year}"
      - "API security OWASP {year}"
      - "Python security vulnerabilities {year}"
      - "secrets management best practices"
    compare_with:
      - "src/auth/"
      - "src/api/security/"

  - name: "Framework Updates"
    cron: "0 0 1 * *"  # Monthly
    topics:
      - "Python {version} new features best practices"
      - "pytest best practices {year}"
      - "Docker security hardening {year}"
    compare_with:
      - "requirements.txt"
      - "Dockerfile"
      - "tests/"

  - name: "Testing Practices"
    cron: "0 0 15 * *"  # Mid-month
    topics:
      - "unit testing best practices {year}"
      - "mutation testing Python"
      - "test coverage strategies"
    compare_with:
      - "tests/"
      - "pytest.ini"
```

### 6. PR-Based Output Workflow

**Purpose:** Surface all analysis findings as reviewable, discussable PRs.

#### 6.1 PR Categories

| Category | Trigger | PR Type | Priority |
|----------|---------|---------|----------|
| **Security Vulnerabilities** | CVE detection | Dependency update | Critical |
| **Test Coverage Gaps** | Coverage below threshold | Test addition | High |
| **Consistency Fixes** | Pattern violations | Refactoring | Medium |
| **Feature Discovery** | New features detected | FEATURES.md update | Medium |
| **Dead Code Removal** | Unused code detection | Cleanup | Low |
| **Best Practice Alignment** | External research gaps | Enhancement | Medium |
| **Documentation Updates** | Drift detection | Doc update | Low |

#### 6.2 PR Templates

**Security Update PR:**

```markdown
## Summary

Updates dependencies to address security vulnerabilities.

### Vulnerabilities Addressed

| Package | CVE | Severity | Fix Version |
|---------|-----|----------|-------------|
| requests | CVE-2024-XXXXX | High | 2.31.0 |

### Breaking Changes

- None expected

### Testing

- [x] All existing tests pass
- [x] Verified vulnerability is resolved
- [x] Dependency compatibility verified

## Test Plan

1. Run full test suite
2. Verify affected code paths still function
3. Deploy to staging and monitor

---
ğŸ¤– Generated by jib codebase analyzer
```

**Test Coverage PR:**

```markdown
## Summary

Adds tests to improve coverage in critical modules.

### Coverage Impact

| File | Before | After | Change |
|------|--------|-------|--------|
| `src/auth/jwt_validator.py` | 45% | 82% | +37% |

### Tests Added

- `test_jwt_validator_expired_token`
- `test_jwt_validator_invalid_signature`
- `test_jwt_validator_missing_claims`

### Analysis Context

These tests address gaps identified in the codebase analysis report.
The module is security-critical and was flagged for inadequate coverage.

## Test Plan

- [x] New tests pass
- [x] No existing tests broken
- [x] Coverage targets met

---
ğŸ¤– Generated by jib codebase analyzer
```

### 7. Analysis Report Structure

**Comprehensive Analysis Report:**

```markdown
# Codebase Analysis Report

**Generated:** 2025-11-29T12:00:00Z
**Repository:** james-in-a-box
**Commit:** abc123def

## Executive Summary

| Category | Status | Issues | Action Items |
|----------|--------|--------|--------------|
| Architecture | âœ… Healthy | 2 minor | 0 PRs needed |
| Consistency | âš ï¸ Needs Attention | 8 issues | 2 PRs created |
| Dependencies | ğŸ”´ Critical | 1 CVE | 1 PR created |
| Test Coverage | âš ï¸ Below Target | 75% (target 80%) | 3 PRs created |
| File Health | âœ… Healthy | 3 files need attention | 0 PRs needed |
| Best Practices | âš ï¸ Gaps Found | 2 gaps | Research PR created |

## Detailed Findings

### 1. Architecture Analysis
[Details from architecture analysis...]

### 2. Consistency Analysis
[Details from consistency analysis...]

### 3. Dependency Analysis
[Details from dependency analysis...]

### 4. Test Coverage Analysis
[Details from coverage analysis...]

### 5. File Health Analysis
[Details from file analysis...]

### 6. Best Practice Comparison
[Details from external research...]

## PRs Created

| PR | Category | Priority | Description |
|----|----------|----------|-------------|
| #234 | Security | Critical | Update requests to fix CVE-2024-XXXXX |
| #235 | Coverage | High | Add tests for jwt_validator.py |
| #236 | Coverage | Medium | Add tests for api/handlers.py |
| #237 | Consistency | Medium | Standardize error handling |
| #238 | Research | Low | Auth best practices comparison |

## Next Actions

1. **Immediate:** Review and merge PR #234 (security)
2. **This Sprint:** Review coverage PRs #235, #236
3. **Backlog:** Address consistency and best practice gaps

---
*Report generated by jib codebase analyzer v1.0*
```

## Evaluation and Success Metrics

Measuring the effectiveness of the codebase analyzer is critical for iterative improvement and demonstrating value. This section establishes evaluation methodology, success metrics, and infrastructure for evidence-based decision-making.

### Top-Line Success Metrics

The following 3 metrics serve as primary indicators of system success:

| Metric | Definition | Target | Measurement Method |
|--------|------------|--------|-------------------|
| **PR Acceptance Rate** | Percentage of analyzer-generated PRs that are merged (with or without modifications) | â‰¥75% | Track merged/closed status of generated PRs |
| **Time to Remediation** | Average time from issue detection to merged fix | <48h for critical, <7d for high | Timestamp analysis of PR lifecycle |
| **False Positive Rate** | Percentage of findings marked as "not an issue" or closed without action | <15% | Track PR rejection reasons and feedback |

### LLM-as-a-Judge Evaluation Framework

For subjective quality dimensions (documentation quality, code recommendation quality), we employ an LLM judge panel approach based on [recent research](https://arxiv.org/abs/2411.15594) showing LLM judges can achieve 85% alignment with human judgment.

#### Quality Dimensions for Evaluation

| Dimension | Definition | Evaluation Criteria |
|-----------|------------|---------------------|
| **Accuracy** | Did the analysis correctly identify the issue? | No hallucinated issues; findings match actual code state |
| **Comprehensiveness** | Did it miss anything critical? | Coverage of security, performance, maintainability concerns |
| **Parsimony** | Did it avoid extraneous or unnecessary detail? | Concise recommendations; no redundant findings |
| **Actionability** | Are recommendations specific enough to implement? | Clear steps; specific file/line references |
| **Prioritization** | Are severity levels appropriate? | Critical items are truly critical; noise is filtered |

#### Multi-Model Judge Panel

To mitigate individual model bias, we use a panel of 3 LLM judges with diverse perspectives:

```yaml
# judge-panel-config.yaml
judges:
  - model: "claude-3-5-sonnet"
    role: "technical_accuracy"
    focus: "Correctness of findings, false positive detection"

  - model: "gpt-4o"
    role: "completeness_check"
    focus: "Missed issues, coverage gaps"

  - model: "claude-3-5-haiku"
    role: "actionability"
    focus: "Clarity and implementability of recommendations"

consensus_threshold: 2  # 2 of 3 judges must agree
```

#### Evaluation Output Format

```json
{
  "evaluation_id": "eval-2025-12-01-001",
  "analysis_run_id": "run-abc123",
  "judge_results": [
    {
      "judge": "claude-3-5-sonnet",
      "scores": {
        "accuracy": 4.2,
        "comprehensiveness": 3.8,
        "parsimony": 4.5,
        "actionability": 4.0,
        "prioritization": 3.9
      },
      "consensus_findings": ["finding-001", "finding-003"],
      "disputed_findings": ["finding-002"],
      "reasoning": "Finding-002 flagged as medium but is low severity..."
    }
  ],
  "consensus_score": 4.1,
  "improvement_recommendations": [
    "Reduce false positives in unused import detection",
    "Improve specificity of refactoring suggestions"
  ]
}
```

### Human Feedback Collection

Since the ultimate consumer of documentation and recommendations may be human, we validate LLM judges against human judgment through structured feedback collection.

#### PR Review Feedback Template

For every rejected or heavily-modified analyzer-generated PR:

```markdown
## Rejection/Modification Feedback

**PR:** #XXX
**Reason:** [ ] False positive [ ] Low priority [ ] Incorrect fix [ ] Style preference [ ] Other

**Feedback Category:**
- [ ] Analysis was wrong (false positive)
- [ ] Analysis was right, but fix was wrong
- [ ] Analysis was right, fix was right, but priority was wrong
- [ ] Analysis was right, but not worth the churn

**Detailed Feedback:**
[Free-form explanation]

**Would this feedback apply to similar findings?** [ ] Yes [ ] No
```

This feedback feeds back into the analyzer's calibration and the LLM judge panel's evaluation criteria.

### A/B Testing Infrastructure

To compare different analysis approaches or model configurations:

#### Test Codebase Collection

Maintain a collection of "toy" codebases that serve as standardized test cases:

| Codebase | Purpose | Characteristics |
|----------|---------|-----------------|
| **toy-webapp-python** | Python web app patterns | Flask/FastAPI, ~5K LoC, known issues injected |
| **toy-cli-python** | CLI tool patterns | Click/Typer, ~2K LoC, good coverage |
| **toy-legacy-python** | Legacy code patterns | Mixed styles, low coverage, tech debt |
| **toy-microservices** | Multi-service patterns | Docker, 3 services, integration complexity |

These codebases include:
- **Known issues** (injected bugs, security vulnerabilities, coverage gaps)
- **Ground truth labels** (what the analyzer should find)
- **Diverse patterns** (to avoid overfitting to webapp or any single codebase)

#### A/B Comparison Protocol

```
For each test codebase:
1. Run Analysis Version A â†’ Collect findings A
2. Run Analysis Version B â†’ Collect findings B
3. LLM Judge Panel evaluates both against ground truth
4. Calculate:
   - Precision: % of findings that are correct
   - Recall: % of known issues detected
   - F1 score: Harmonic mean of precision/recall
   - Time: Analysis duration
   - Cost: Token consumption
5. Human tiebreaker for disputed cases
```

### Measuring Impact on Agent Effectiveness

To measure whether codebase documentation improves agent task completion:

#### Experimental Design

```
Task Suite: Collection of 20 realistic tasks requiring codebase understanding
  - "Add rate limiting to the API"
  - "Fix the authentication bypass in /admin"
  - "Refactor duplicate code in utils/"

For each task:
  - Branch A: Agent with Analysis Version A documentation
  - Branch B: Agent with Analysis Version B documentation

Measures:
  - Time to completion (wall clock)
  - Tokens consumed
  - Solution quality (LLM judge panel)
  - Test pass rate
  - Human review verdict (approve/reject)
```

### Intermediate Artifact Caching

To support incremental analysis and failure/retry/resume scenarios:

#### Cache Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Analysis Cache Structure                      â”‚
â”‚                                                                  â”‚
â”‚  cache/                                                          â”‚
â”‚  â”œâ”€â”€ structural/                                                 â”‚
â”‚  â”‚   â”œâ”€â”€ {file_hash}.ast.json     # AST for unchanged files     â”‚
â”‚  â”‚   â””â”€â”€ dependency_graph.json    # Updated incrementally       â”‚
â”‚  â”œâ”€â”€ semantic/                                                   â”‚
â”‚  â”‚   â””â”€â”€ {file_hash}.analysis.json # LLM analysis cache         â”‚
â”‚  â”œâ”€â”€ coverage/                                                   â”‚
â”‚  â”‚   â””â”€â”€ {test_hash}.coverage.json # Coverage for test runs     â”‚
â”‚  â””â”€â”€ manifest.json                 # Cache validity tracking     â”‚
â”‚                                                                  â”‚
â”‚  Invalidation: File hash change â†’ Invalidate file + dependents  â”‚
â”‚  Retention: 7 days for semantic, 30 days for structural         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Incremental Analysis Benefits

| Scenario | Without Cache | With Cache | Improvement |
|----------|---------------|------------|-------------|
| Full repo analysis (no changes) | ~30 min | ~2 min | 15x faster |
| Single file change | ~30 min | ~5 min | 6x faster |
| Analysis failure at Phase 4 | Restart from Phase 1 | Resume from Phase 4 | Significant time savings |

### Continuous Improvement Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Continuous Improvement Loop                         â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Analysis   â”‚â”€â”€â”€â–¶â”‚  PR Output   â”‚â”€â”€â”€â–¶â”‚ Human Review â”‚      â”‚
â”‚  â”‚    Run       â”‚    â”‚              â”‚    â”‚  + Feedback  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                  â”‚               â”‚
â”‚                                                  â–¼               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Calibrate  â”‚â—€â”€â”€â”€â”‚  LLM Judge   â”‚â—€â”€â”€â”€â”‚   Collect    â”‚      â”‚
â”‚  â”‚   Analyzer   â”‚    â”‚  Evaluation  â”‚    â”‚  Feedback    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Migration Strategy

### Phase 1: Foundation

**Dependencies:** None

1. Implement structural analysis using tree-sitter
2. Create analysis report schema
3. Build CLI interface for manual analysis runs
4. Integrate with existing codebase.json generation

**Success Criteria:** Can generate structural analysis report for any repo

### Phase 2: Coverage Integration

**Dependencies:** Phase 1 (structural analysis for prioritization)

1. Integrate with pytest-cov, coverage.py
2. Build coverage gap identification logic
3. Implement test suggestion generation
4. Create coverage improvement PRs

**Success Criteria:** Automated coverage reports with actionable recommendations

### Phase 3: Dependency Scanning

**Dependencies:** Phase 1

1. Integrate with pip-audit, safety, or Snyk
2. Implement reachability analysis
3. Build dependency update PR generation
4. Add license compliance checking

**Success Criteria:** Security vulnerabilities detected and PRs created automatically

### Phase 4: Semantic Analysis

**Dependencies:** Phases 1-3 (structural data available)

1. Implement LLM-based pattern recognition
2. Build consistency checking logic
3. Add complexity and maintainability scoring
4. Create refactoring suggestion PRs
5. Implement feature discovery and FEATURES.md integration

**Success Criteria:**
- Context-aware analysis with low false positive rate
- FEATURES.md accurately reflects all discovered features

### Phase 5: External Research Integration

**Dependencies:** Phase 4 (semantic understanding available)

1. Implement web research workflow (per LLM Documentation Index Strategy Phase 6)
2. Build best practice comparison reports
3. Create research update PRs
4. Set up scheduled research tasks

**Success Criteria:** Regular external validation of internal practices

### Phase 6: Full Automation

**Dependencies:** All previous phases

1. Set up scheduled analysis runs
2. Integrate with CI/CD pipeline
3. Build dashboard for analysis trends
4. Implement baseline management for legacy code

**Success Criteria:** Fully automated analysis with PR-based output

## Consequences

### Benefits

1. **Proactive Issue Detection:** Problems found before they cause incidents
2. **Consistent Quality:** Automated enforcement of patterns and standards
3. **Security Posture:** Continuous vulnerability monitoring and remediation
4. **External Validation:** Practices validated against industry standards
5. **Test Confidence:** Multi-metric coverage ensures test effectiveness
6. **Reviewable Output:** PR-based workflow enables discussion and iteration
7. **Knowledge Building:** Analysis reports document codebase state over time

### Drawbacks

1. **Implementation Effort:** Significant work to build full pipeline
2. **False Positives:** Some analysis will need tuning to reduce noise
3. **Compute Costs:** Running analysis on large codebases requires resources
4. **Maintenance:** Analysis rules and thresholds need ongoing adjustment
5. **Review Burden:** Generated PRs require human review time

### Risks and Mitigations

| Risk | Mitigation |
|------|------------|
| Analysis produces too much noise | Implement confidence scoring, start with high-confidence findings only |
| External research is unreliable | Prioritize authoritative sources, require human review |
| Coverage metrics gamified | Use mutation testing to validate test quality |
| Analysis becomes stale | Schedule regular runs, trigger on significant changes |
| PR flood overwhelms reviewers | Batch low-priority items, prioritize by severity |
| Security sandboxing requirements | Coordinate with security team review; see Dependencies below |

### Dependencies and Blockers

**Security Team Review (Blocking):**

This ADR depends on the security team's ongoing review of model sandboxing and autonomous edit modes ([Slack thread](https://khanacademy.slack.com/archives/C0SAJPXCP/p1763130874358999)). Specifically:

- **Model autonomy requirements:** The codebase analyzer requires the ability to run code analysis tools, which may trigger security concerns
- **Sandboxing assurances:** We need clarity on what isolation guarantees the security team requires
- **Container isolation:** Current jib execution in Docker containers may or may not meet requirements

**Recommendation:** Before Phase 4 (Semantic Analysis with LLM), confirm security team approval for the level of autonomy required. Consider:
- WebAssembly sandboxing for code execution ([NVIDIA's approach](https://developer.nvidia.com/blog/sandboxing-agentic-ai-workflows-with-webassembly/))
- gVisor for stronger container isolation
- Restricted tool access patterns

### Model/Provider Strategy

**MVP Approach (v0):**
- Target Claude (Sonnet) as primary model since it's currently SOTA for code understanding
- Single-provider implementation to minimize complexity

**Future Evolution:**
- Design abstractions that allow model swapping without architectural changes
- Consider multi-model pipelines (e.g., cheaper models for structural analysis, frontier models for semantic)
- Enable provider redundancy for reliability

**Decision:** Build v0 on Claude, but structure code with provider abstraction layer for future flexibility. Model/provider agnosticism is a Phase 6+ concern, not a v0 requirement.

## Decision Permanence

**Medium permanence.**

The multi-layered analysis approach and PR-based output pattern are established practices. Specific tooling and thresholds can be adjusted without changing the core strategy.

**Low-permanence elements:**
- Specific tools used (pytest-cov, pip-audit, etc.)
- Threshold values for metrics
- Research schedules and topics
- Report formats

**Higher-permanence elements:**
- Multi-pass analysis pipeline
- External validation principle
- PR-based output workflow
- Integration with LLM Documentation Index Strategy

## Alternatives Considered

### Alternative 1: Use Existing SaaS Platform

**Description:** Adopt SonarQube, CodeClimate, or similar existing platform.

**Pros:**
- Mature, battle-tested
- Extensive language support
- Good CI/CD integration

**Cons:**
- Limited LLM integration
- No external research capability
- May not integrate well with our workflows
- Cost for enterprise features

**Rejected because:** We need LLM-powered semantic analysis and external research integration that existing platforms don't provide.

### Alternative 2: Manual Analysis Only

**Description:** Human reviews codebase periodically without automation.

**Pros:**
- High quality human judgment
- No infrastructure needed
- Context-aware

**Cons:**
- Doesn't scale
- Inconsistent coverage
- Delays detection

**Rejected because:** Not sustainable for continuous improvement; too slow.

### Alternative 3: LLM-Only Analysis

**Description:** Use LLM for all analysis without static tools.

**Pros:**
- Semantic understanding
- Context-aware
- Single system

**Cons:**
- Hallucination risk for structural data
- High token costs
- Less reliable for coverage metrics

**Rejected because:** Structural analysis (AST, coverage) is more reliable with traditional tools; LLM best for semantic layer.

### Alternative 4: Analysis Without PR Output

**Description:** Generate reports but don't create PRs automatically.

**Pros:**
- Less review burden
- Human decides what to action

**Cons:**
- Findings get ignored
- No forcing function
- Knowledge lost

**Rejected because:** PR-based output ensures findings are actionable and tracked.

## References

### Static Analysis and Code Quality
- [13 Best Static Code Analysis Tools For 2025](https://www.qodo.ai/blog/best-static-code-analysis-tools/) - Qodo
- [Static Code Analysis: 10 Enterprise Tips](https://www.augmentcode.com/guides/static-code-analysis-best-practices) - Augment Code
- [Guide to Static Code Analysis in 2025](https://www.codeant.ai/blogs/static-code-analysis-tools) - CodeAnt

### LLM Code Analysis
- [Large Language Models for Source Code Analysis](https://arxiv.org/html/2503.17502v1) - arXiv survey
- [AI-Powered Code Reviews 2025](https://medium.com/@API4AI/ai-powered-code-reviews-2025-key-llm-trends-shaping-software-development-eac78e51ee59) - Medium
- [Best Coding LLMs That Actually Work](https://www.augmentcode.com/guides/best-coding-llms-that-actually-work) - Augment Code

### Test Coverage
- [Code Coverage - Atlassian](https://www.atlassian.com/continuous-delivery/software-testing/code-coverage)
- [5 Best Test Coverage Tools for 2025](https://appsurify.com/resources/5-best-test-coverage-tools-for-2025/) - Appsurify
- [7 Metrics for Measuring Code Quality](https://blog.codacy.com/code-quality-metrics) - Codacy

### Security and Dependencies
- [Code Vulnerability Detection with Program Dependency Graphs](https://www.nature.com/articles/s41598-025-23029-4) - Scientific Reports
- [How We Built An AI-Assisted Dependency Vulnerability Scanner](https://dev.to/kirodotdev/how-we-built-an-ai-assisted-dependency-vulnerability-scanner-5270) - DEV
- [Top 3 Open Source Vulnerability Scanners in 2025](https://www.backslash.security/blog/open-source-vulnerability-scanner) - Backslash

### AI Code Review
- [Best AI Code Review Tools 2024](https://graphite.com/guides/best-ai-code-review-tools-2024) - Graphite
- [AI-Assisted Assessment of Coding Practices](https://arxiv.org/html/2405.13565v1) - arXiv
- [Static Code Analyzers vs AI Code Reviewers](https://www.qodo.ai/blog/static-code-analyzers-vs-ai-code-reviewers-best-choice/) - Qodo

### LLM-as-a-Judge Evaluation
- [A Survey on LLM-as-a-Judge](https://arxiv.org/abs/2411.15594) - arXiv comprehensive survey
- [LLM-as-a-judge: A Complete Guide](https://www.evidentlyai.com/llm-guide/llm-as-a-judge) - Evidently AI
- [LLMs-as-Judges: Comprehensive Survey on LLM-based Evaluation](https://arxiv.org/html/2412.05579v2) - arXiv

### Security and Sandboxing
- [Sandboxing Agentic AI Workflows with WebAssembly](https://developer.nvidia.com/blog/sandboxing-agentic-ai-workflows-with-webassembly/) - NVIDIA
- [Agentic AI and Security](https://martinfowler.com/articles/agentic-ai-security.html) - Martin Fowler
- [LLM Security in 2025: Risks and Best Practices](https://www.oligo.security/academy/llm-security-in-2025-risks-examples-and-best-practices) - Oligo Security

### Related ADRs

| ADR | Relationship |
|-----|--------------|
| [LLM Documentation Index Strategy](../implemented/ADR-LLM-Documentation-Index-Strategy.md) | Provides documentation infrastructure; Phase 6 research workflow shared |
| [Feature Analyzer - Documentation Sync](../implemented/ADR-Feature-Analyzer-Documentation-Sync.md) | Codebase analyzer provides feature discovery for FEATURES.md maintenance |
| [Continuous System Reinforcement](ADR-Continuous-System-Reinforcement.md) | Analysis feeds into system improvement loop |

---

**Last Updated:** 2025-11-29
**Next Review:** 2025-12-29 (Monthly)
**Status:** Not Implemented
