#!/usr/bin/env python3
"""
jib (james-in-a-box) - Run Claude Code CLI in an isolated Docker container

Prevents AI agents from accessing credentials while allowing full code editing.

Platform Support:
  - Linux (x86_64, ARM64): Fully supported
  - macOS (Intel, Apple Silicon): Fully supported
"""

import argparse
import atexit
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List, Optional

from statusbar import StatusBar, init_statusbar, status, status_success, status_error, status_warn, status_finish


# Global quiet mode flag
_quiet_mode = False


# Default log directory for container logs
CONTAINER_LOGS_DIR = Path.home() / ".jib-sharing" / "container-logs"


class Colors:
    """ANSI color codes for terminal output"""
    BLUE = '\033[0;34m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    RED = '\033[0;31m'
    BOLD = '\033[1m'
    NC = '\033[0m'


def info(msg: str) -> None:
    """Show info message. In quiet mode, updates statusbar instead."""
    if _quiet_mode:
        status(msg)
    else:
        print(f"{Colors.BLUE}[INFO]{Colors.NC} {msg}")


def success(msg: str) -> None:
    """Show success message. In quiet mode, shows as persistent success."""
    if _quiet_mode:
        status_success(msg)
    else:
        print(f"{Colors.GREEN}[SUCCESS]{Colors.NC} {msg}")


def warn(msg: str) -> None:
    """Show warning message. Always visible."""
    if _quiet_mode:
        status_warn(msg)
    else:
        print(f"{Colors.YELLOW}[WARNING]{Colors.NC} {msg}")


def error(msg: str) -> None:
    """Show error message. Always visible."""
    if _quiet_mode:
        status_error(msg)
    else:
        print(f"{Colors.RED}[ERROR]{Colors.NC} {msg}", file=sys.stderr)


def check_claude_credentials() -> bool:
    """
    Check if Claude credentials exist.

    Returns:
        bool: credentials_exist
    """
    claude_dir = Path.home() / ".claude"
    credentials_file = claude_dir / ".credentials.json"

    return credentials_file.exists()


class Config:
    """Configuration paths and constants"""
    CONFIG_DIR = Path.home() / ".jib"  # Docker staging directory
    CONFIG_FILE = CONFIG_DIR / "mounts.conf"
    DOCKERFILE = CONFIG_DIR / "Dockerfile"
    USER_CONFIG_DIR = Path.home() / ".config" / "jib"  # User config (secrets, preferences)
    GITHUB_TOKEN_FILE = USER_CONFIG_DIR / "github-token"
    IMAGE_NAME = "james-in-a-box"
    CONTAINER_NAME = "jib"
    KHAN_SOURCE = Path.home() / "khan"
    # Persistent directory for all shared data
    SHARING_DIR = Path.home() / ".jib-sharing"
    TMP_DIR = SHARING_DIR / "tmp"      # Persistent tmp workspace
    # Shared Claude auth files only (NOT full .claude directory to avoid history sharing)
    # Auth files: .credentials.json, settings.json
    # Container creates its own ephemeral: history.jsonl, debug/, session-env/, etc.
    CLAUDE_AUTH_DIR = CONFIG_DIR / "claude-auth"
    # Worktree base directory (ephemeral workspaces per container)
    WORKTREE_BASE = Path.home() / ".jib-worktrees"

    # Note: Each container gets its own worktree to isolate changes
    # Host repos (~/khan/) stay clean while containers work independently

    # Directories that are dangerous to mount (contain credentials)
    DANGEROUS_DIRS = [
        Path.home() / ".ssh",
        Path.home() / ".config" / "gcloud",
        Path.home() / ".gitconfig",
        Path.home() / ".netrc",
        Path.home() / ".aws",
        Path.home() / ".kube",
        Path.home() / ".gnupg",
        Path.home() / ".docker",
    ]


def get_platform() -> str:
    """Detect platform: linux or macos"""
    import platform
    system = platform.system().lower()
    if system == "linux":
        return "linux"
    elif system == "darwin":
        return "macos"
    return "unknown"


def get_github_token() -> Optional[str]:
    """Get GitHub PAT using the unified HostConfig system.

    Uses HostConfig to load the token from (in order of precedence):
    - Environment variable GITHUB_TOKEN (highest priority)
    - ~/.config/jib/secrets.env (GITHUB_TOKEN=...)
    - ~/.config/jib/github-token (dedicated file)

    This follows the same configuration pattern as other jib secrets
    (Slack tokens, Confluence tokens, etc.) via config/host_config.py.

    Returns:
        Token string if found and valid, None otherwise
    """
    try:
        # Import HostConfig from project root
        script_dir = Path(__file__).resolve().parent
        project_root = script_dir.parent
        if str(project_root) not in sys.path:
            sys.path.insert(0, str(project_root))

        from config.host_config import HostConfig
        config = HostConfig()
        token = config.github_token

        if token and token.startswith(("ghp_", "github_pat_")):
            return token
    except ImportError as e:
        warn(f"Could not import HostConfig: {e}")
    except Exception as e:
        warn(f"Error loading GitHub token from config: {e}")
    return None


def get_github_app_token() -> Optional[str]:
    """Generate GitHub App installation token for container use.

    Uses the github-app-token.py script to generate a fresh installation token
    from App credentials (App ID, Installation ID, private key).

    Returns:
        Installation token string if successful, None otherwise
    """
    # Check if App credentials exist
    app_id_file = Config.USER_CONFIG_DIR / "github-app-id"
    installation_id_file = Config.USER_CONFIG_DIR / "github-app-installation-id"
    private_key_file = Config.USER_CONFIG_DIR / "github-app.pem"

    if not all(f.exists() for f in [app_id_file, installation_id_file, private_key_file]):
        return None  # App not configured, fall back to PAT

    # Find the token generation script
    script_dir = Path(__file__).resolve().parent
    token_script = script_dir / "jib-tools" / "github-app-token.py"

    if not token_script.exists():
        warn(f"GitHub App token script not found: {token_script}")
        return None

    # Use the host-services venv Python which has cryptography installed
    # Fall back to system python3 if venv doesn't exist
    jib_root = script_dir.parent
    venv_python = jib_root / "host-services" / ".venv" / "bin" / "python"
    python_cmd = str(venv_python) if venv_python.exists() else "python3"

    try:
        result = subprocess.run(
            [python_cmd, str(token_script), "--config-dir", str(Config.USER_CONFIG_DIR)],
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            token = result.stdout.strip()
            if token and token.startswith("ghs_"):  # Installation tokens start with ghs_
                return token
            elif token:
                # Token format might vary, accept if non-empty
                return token

        # Log error but don't fail - we can fall back to PAT
        if result.stderr:
            warn(f"GitHub App token generation failed: {result.stderr.strip()}")

    except subprocess.TimeoutExpired:
        warn("GitHub App token generation timed out")
    except Exception as e:
        warn(f"GitHub App token generation error: {e}")

    return None


def write_github_token_file(token: str) -> bool:
    """Write GitHub token to the shared file for container consumption.

    The token file is written to ~/.jib-sharing/.github-token and contains
    JSON with the token and metadata. This allows long-running containers
    to read fresh tokens even after the initial env var becomes stale.

    The github-token-refresher service will continuously update this file,
    but we write an initial version here so containers have a valid token
    immediately at startup.

    Args:
        token: The GitHub token to write

    Returns:
        True if successful, False otherwise
    """
    import json
    from datetime import datetime, timezone

    token_file = Config.SHARING_DIR / ".github-token"
    validity_seconds = 60 * 60  # 1 hour

    # Ensure sharing directory exists
    Config.SHARING_DIR.mkdir(parents=True, exist_ok=True)

    now = datetime.now(timezone.utc)
    expires_at = now.timestamp() + validity_seconds

    data = {
        "token": token,
        "generated_at": now.isoformat(),
        "expires_at_unix": expires_at,
        "expires_at": datetime.fromtimestamp(expires_at, timezone.utc).isoformat(),
        "generated_by": "jib-launcher",
        "validity_seconds": validity_seconds
    }

    try:
        # Write atomically using temp file
        temp_file = token_file.with_suffix(".tmp")
        temp_file.write_text(json.dumps(data, indent=2) + "\n")
        temp_file.chmod(0o600)  # Restrict permissions
        temp_file.rename(token_file)
        return True
    except Exception as e:
        warn(f"Failed to write token file: {e}")
        return False


def check_docker_permissions() -> bool:
    """Check if user has permission to run Docker commands"""
    result = subprocess.run(
        ["docker", "ps"],
        capture_output=True,
        text=True
    )

    if result.returncode == 0:
        return True

    if "permission denied" in result.stderr.lower():
        error("Docker permission denied - you are not in the docker group")
        print()
        print("This usually means one of two things:")
        print("  1. You just installed Docker and need to log out/in for group membership")
        print("  2. You need to be added to the docker group")
        print()
        print("Solutions:")
        print()
        print("Option 1: Add yourself to docker group and re-login")
        print("  sudo usermod -aG docker $USER")
        print("  then LOG OUT and LOG BACK IN")
        print()
        print("Option 2: Run with sudo (temporary workaround)")
        print("  sudo $(which jib)")
        print()
        return False

    return False


def get_setup_script_path() -> Optional[Path]:
    """Find the setup.sh script relative to the jib launcher location"""
    # Try to find setup.sh relative to the jib script
    jib_script = Path(__file__).resolve()

    # jib is at jib-container/jib, setup.sh is at repo root
    repo_root = jib_script.parent.parent
    setup_script = repo_root / "setup.sh"

    if setup_script.exists():
        return setup_script

    # Fallback: check ~/khan/james-in-a-box/setup.sh
    fallback = Path.home() / "khan" / "james-in-a-box" / "setup.sh"
    if fallback.exists():
        return fallback

    return None


def run_setup_script() -> bool:
    """Run the setup.sh script to configure jib"""
    setup_script = get_setup_script_path()

    if not setup_script:
        error("Could not find setup.sh script")
        print()
        print("Please run setup manually:")
        print("  cd ~/khan/james-in-a-box")
        print("  ./setup.sh")
        return False

    info(f"Running setup: {setup_script}")
    print()

    try:
        # Run setup.sh in its directory
        result = subprocess.run(
            ["bash", str(setup_script)],
            cwd=setup_script.parent
        )
        return result.returncode == 0
    except Exception as e:
        error(f"Failed to run setup.sh: {e}")
        return False


def check_host_setup() -> bool:
    """Check if host setup is complete (services installed, directories exist)"""
    # Critical systemd services that should be installed
    critical_services = [
        "slack-notifier.service",
        "slack-receiver.service"
    ]

    # Important directories that should exist
    critical_dirs = [
        Path.home() / ".jib-sharing" / "notifications",
        Path.home() / ".jib-sharing" / "incoming",
        Path.home() / ".jib-sharing" / "responses",
    ]

    # Configuration file
    config_file = Path.home() / ".config" / "jib-notifier" / "config.json"

    issues_found = []

    # Check if systemd services are installed
    for service in critical_services:
        result = subprocess.run(
            ["systemctl", "--user", "list-unit-files", service],
            capture_output=True,
            text=True
        )
        if result.returncode != 0 or service not in result.stdout:
            issues_found.append(f"Service not installed: {service}")

    # Check if critical directories exist
    for dir_path in critical_dirs:
        if not dir_path.exists():
            issues_found.append(f"Directory not found: {dir_path}")

    # Check if config exists (warning only, not critical)
    config_warning = None
    if not config_file.exists():
        config_warning = f"Configuration file not found: {config_file}"

    # If critical issues found, automatically run setup
    if issues_found:
        warn("Host setup appears incomplete")
        print()

        error("Critical issues found:")
        for issue in issues_found:
            print(f"  âœ— {issue}")
        print()

        if config_warning:
            warn(config_warning)
            print()

        print("JIB requires host services to be installed for full functionality:")
        print("  â€¢ Slack integration (notifier and receiver)")
        print("  â€¢ Shared directories for notifications and task communication")
        print()

        # Auto-run setup.sh when config is missing
        info("Running setup.sh to configure jib...")
        print()
        if run_setup_script():
            success("Setup completed!")
            return True
        else:
            error("Setup failed")
            return False

    # Config warning only (not critical) - just warn and continue
    if config_warning:
        warn(config_warning)
        print()

    return True


def check_docker() -> bool:
    """Check if Docker is installed and offer to install if not"""
    platform_name = get_platform()

    if subprocess.run(["which", "docker"], capture_output=True).returncode != 0:
        error("Docker is not installed.")

        if platform_name == "macos":
            # TODO: Add macOS Docker Desktop installation
            info("On macOS, please install Docker Desktop from:")
            info("  https://www.docker.com/products/docker-desktop")
            return False

        # Linux installation
        response = input("Install Docker now? (yes/no): ").strip().lower()
        if response == "yes":
            info("Installing Docker...")
            try:
                # Download installer
                subprocess.run(
                    ["curl", "-fsSL", "https://get.docker.com", "-o", "/tmp/get-docker.sh"],
                    check=True
                )
                # Run installer
                subprocess.run(["sudo", "sh", "/tmp/get-docker.sh"], check=True)
                # Add user to docker group
                subprocess.run(["sudo", "usermod", "-aG", "docker", os.environ["USER"]], check=True)
                # Cleanup
                os.remove("/tmp/get-docker.sh")

                success("Docker installed successfully!")
                print()
                warn("IMPORTANT: You need to log out and back in for group membership to take effect.")
                print("After logging back in, run this script again.")
                sys.exit(0)
            except Exception as e:
                error(f"Docker installation failed: {e}")
                return False
        else:
            error("Docker is required")
            return False

    # Check Docker daemon is running and we have permissions
    return check_docker_permissions()



def _copy_directory_atomic(src: Path, dest: Path, name: str, quiet: bool = False) -> bool:
    """Copy a directory atomically with retry logic for race conditions.

    When multiple jib --exec instances run simultaneously, they may all try to
    update the same build context directories. This function uses atomic operations
    to handle race conditions:
    1. Copy to a temporary directory
    2. Remove existing destination (with retry on ENOTEMPTY/ENOENT)
    3. Rename temp to destination (atomic on same filesystem)

    Args:
        src: Source directory to copy
        dest: Destination path
        name: Human-readable name for logging
        quiet: If True, suppress info messages

    Returns:
        True if successful, False otherwise
    """
    import shutil
    import tempfile
    import uuid

    max_retries = 3
    retry_delay = 0.1  # seconds

    for attempt in range(max_retries):
        try:
            # Create a unique temp directory in the same parent (for atomic rename)
            temp_dir = dest.parent / f".tmp-{uuid.uuid4().hex[:8]}"

            # Copy source to temp location
            shutil.copytree(src, temp_dir)

            # Try to remove existing destination
            if dest.exists():
                try:
                    shutil.rmtree(dest)
                except FileNotFoundError:
                    # Another process already removed it - that's fine
                    pass
                except OSError as e:
                    # Directory not empty (ENOTEMPTY) - another process is writing
                    # Clean up temp and retry
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay * (attempt + 1))
                        continue
                    raise

            # Atomic rename from temp to destination
            try:
                temp_dir.rename(dest)
            except OSError:
                # Destination appeared between rmtree and rename - another process won
                # Clean up our temp and use their copy
                shutil.rmtree(temp_dir, ignore_errors=True)
                if dest.exists():
                    # Other process succeeded, we're done
                    if not quiet:
                        info(f"{name} directory ready (from another process)")
                    return True
                # Neither exists - retry
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                raise

            if not quiet:
                info(f"{name} copied to build context")
            return True

        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(retry_delay * (attempt + 1))
                continue
            warn(f"Failed to copy {name} directory after {max_retries} attempts: {e}")
            # Clean up temp if it exists
            if 'temp_dir' in locals() and temp_dir.exists():
                shutil.rmtree(temp_dir, ignore_errors=True)
            return False

    return False


def is_dangerous_dir(path: Path) -> bool:
    """Check if a directory is dangerous to mount (contains credentials)"""
    for dangerous in Config.DANGEROUS_DIRS:
        try:
            # Check if path is dangerous or contains dangerous
            if path.resolve() == dangerous.resolve():
                return True
            if path.resolve() in dangerous.resolve().parents:
                return True
            if dangerous.resolve() in path.resolve().parents:
                return True
        except Exception:
            pass
    return False


def create_dockerfile() -> None:
    """Create the Dockerfile for the container"""
    import shutil

    # Resolve symlinks to find the actual project directory
    script_dir = Path(__file__).resolve().parent

    # Copy docker-setup.py to config directory
    setup_script = script_dir / "docker-setup.py"
    setup_dest = Config.CONFIG_DIR / "docker-setup.py"

    if setup_script.exists():
        shutil.copy(setup_script, setup_dest)
        setup_dest.chmod(0o755)
    else:
        warn("docker-setup.py not found, skipping dev tools installation")

    # Copy claude-commands directory to config directory
    # Use atomic copy with retry to handle race conditions when multiple
    # jib --exec instances run simultaneously
    commands_src = script_dir / "claude-commands"
    commands_dest = Config.CONFIG_DIR / "claude-commands"
    if commands_src.exists():
        _copy_directory_atomic(commands_src, commands_dest, "Claude commands", _quiet_mode)
    else:
        warn("claude-commands directory not found")

    # Copy claude-rules directory to build context
    # Use atomic copy with retry to handle race conditions
    rules_src = script_dir / "claude-rules"
    rules_dest = Config.CONFIG_DIR / "claude-rules"
    if rules_src.exists():
        _copy_directory_atomic(rules_src, rules_dest, "Claude rules", _quiet_mode)
    else:
        warn("claude-rules directory not found, skipping agent rules")

    # Copy .claude/hooks directory to build context
    # Use atomic copy with retry to handle race conditions
    hooks_src = script_dir / ".claude" / "hooks"
    hooks_dest = Config.CONFIG_DIR / ".claude" / "hooks"
    if hooks_src.exists():
        # Ensure parent directory exists
        hooks_dest.parent.mkdir(parents=True, exist_ok=True)
        _copy_directory_atomic(hooks_src, hooks_dest, "Claude hooks", _quiet_mode)
    else:
        warn(".claude/hooks directory not found, skipping hooks")

    # Note: Claude credentials are mounted at runtime (not copied at build time)
    # This ensures the container always uses the host's CURRENT credentials
    # Avoids issues with stale/revoked OAuth tokens from previous builds
    if not _quiet_mode:
        info("Claude credentials will be mounted from host at runtime (see setup output above)")

    # Copy Dockerfile from script directory
    dockerfile_src = script_dir / "Dockerfile"
    if dockerfile_src.exists():
        shutil.copy(dockerfile_src, Config.DOCKERFILE)
        if not _quiet_mode:
            success("Build context prepared")
    else:
        error(f"Dockerfile not found at {dockerfile_src}")
        error("Cannot build without Dockerfile")


def build_image() -> bool:
    """Build the Docker image (Docker's cache makes this fast when nothing changed)"""
    # Always sync files to build context - Docker detects changes and rebuilds only what's needed
    create_dockerfile()

    try:
        cmd = [
            "docker", "build",
            "--build-arg", f"USER_NAME={os.environ['USER']}",
            "--build-arg", f"USER_UID={os.getuid()}",
            "--build-arg", f"USER_GID={os.getgid()}",
            "-t", Config.IMAGE_NAME,
            "-f", str(Config.DOCKERFILE),
            str(Config.CONFIG_DIR)
        ]

        # In quiet mode, suppress Docker build output
        if _quiet_mode:
            cmd.insert(2, "--quiet")
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                # Show error output if build failed
                error("Docker build failed")
                if result.stderr:
                    print(result.stderr, file=sys.stderr)
                return False
        else:
            # Docker automatically uses cache for unchanged layers
            subprocess.run(cmd, check=True)
        return True
    except subprocess.CalledProcessError:
        error("Docker build failed")
        return False


def image_exists() -> bool:
    """Check if Docker image exists"""
    return subprocess.run(
        ["docker", "image", "inspect", Config.IMAGE_NAME],
        capture_output=True
    ).returncode == 0


def generate_container_id() -> str:
    """Generate unique container ID based on timestamp and process ID"""
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    pid = os.getpid()
    return f"jib-{timestamp}-{pid}"


def get_docker_log_config(container_id: str, task_id: Optional[str] = None) -> list:
    """Generate Docker logging configuration arguments.

    Uses the json-file logging driver with:
    - Log rotation (max 10MB per file, 5 files)
    - Correlation labels for easy searching
    - Timestamps included

    Args:
        container_id: Unique container identifier
        task_id: Optional task ID for correlation (e.g., "task-20251129-222239")

    Returns:
        List of Docker command arguments for logging configuration
    """
    # Ensure container logs directory exists
    CONTAINER_LOGS_DIR.mkdir(parents=True, exist_ok=True)

    # Log file path based on container ID
    log_file = CONTAINER_LOGS_DIR / f"{container_id}.log"

    log_args = [
        "--log-driver", "json-file",
        "--log-opt", "max-size=10m",
        "--log-opt", "max-file=5",
        # Add labels for correlation - these appear in docker inspect
        "--label", f"jib.container_id={container_id}",
    ]

    if task_id:
        log_args.extend(["--label", f"jib.task_id={task_id}"])

    return log_args


def extract_task_id_from_command(command: List[str]) -> Optional[str]:
    """Extract task ID from the command if it's processing a task file.

    Looks for patterns like:
    - incoming-processor.py /path/to/task-20251129-222239.md
    - task-20251129-222239

    Args:
        command: Command list passed to jib --exec

    Returns:
        Task ID if found, None otherwise
    """
    import re
    task_pattern = r"(task-\d{8}-\d{6})"

    for arg in command:
        match = re.search(task_pattern, arg)
        if match:
            return match.group(1)

    return None


def extract_thread_ts_from_task_file(task_file_path: str) -> Optional[str]:
    """Extract thread_ts from a task file's YAML frontmatter.

    Task files contain YAML frontmatter like:
        ---
        task_id: "task-20251129-222239"
        thread_ts: "1764483758.159619"
        ---

    Args:
        task_file_path: Path to the task file (may be container path)

    Returns:
        Thread timestamp if found, None otherwise
    """
    import re

    # Convert container path back to host path
    host_path = task_file_path
    if "/sharing/" in task_file_path:
        # Container path like /home/user/sharing/incoming/task.md
        # -> Host path like ~/.jib-sharing/incoming/task.md
        parts = task_file_path.split("/sharing/", 1)
        if len(parts) == 2:
            host_path = str(Path.home() / ".jib-sharing" / parts[1])

    try:
        path = Path(host_path)
        if path.exists():
            content = path.read_text()
            # Look for thread_ts in YAML frontmatter
            match = re.search(r'thread_ts:\s*["\']?(\d+\.\d+)["\']?', content)
            if match:
                return match.group(1)
    except Exception:
        pass

    return None


def update_log_index(
    container_id: str,
    task_id: Optional[str] = None,
    thread_ts: Optional[str] = None,
    log_file: Optional[str] = None,
) -> None:
    """Update the log index file with correlation information.

    The log index enables quick lookups:
    - task_id -> container_id
    - thread_ts -> task_id
    - List of all recent container runs

    Uses file locking to prevent race conditions when multiple containers
    finish simultaneously.

    Args:
        container_id: Docker container ID
        task_id: Optional task ID (e.g., "task-20251129-222239")
        thread_ts: Optional Slack thread timestamp
        log_file: Path to the log file
    """
    import json
    import fcntl

    index_file = CONTAINER_LOGS_DIR / "log-index.json"
    CONTAINER_LOGS_DIR.mkdir(parents=True, exist_ok=True)

    # Use file locking to prevent concurrent modifications
    # Open in 'a+' mode to create file if it doesn't exist
    with open(index_file, 'a+') as f:
        # Acquire exclusive lock
        fcntl.flock(f.fileno(), fcntl.LOCK_EX)
        try:
            # Seek to beginning to read
            f.seek(0)
            content = f.read()

            # Load existing index
            index = {"task_to_container": {}, "thread_to_task": {}, "entries": []}
            if content:
                try:
                    index = json.loads(content)
                except Exception:
                    pass

            # Update correlation maps
            if task_id:
                index["task_to_container"][task_id] = container_id
            if thread_ts and task_id:
                index["thread_to_task"][thread_ts] = task_id

            # Add entry
            entry = {
                "container_id": container_id,
                "task_id": task_id,
                "thread_ts": thread_ts,
                "log_file": str(log_file) if log_file else None,
                "timestamp": datetime.now().isoformat(),
            }
            index["entries"].append(entry)

            # Keep last 1000 entries
            if len(index["entries"]) > 1000:
                index["entries"] = index["entries"][-1000:]

            # Write updated index
            f.seek(0)
            f.truncate()
            f.write(json.dumps(index, indent=2))
        finally:
            # Release lock
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)


def save_container_logs(
    container_id: str,
    task_id: Optional[str] = None,
    thread_ts: Optional[str] = None,
) -> Optional[Path]:
    """Save Docker container logs to persistent storage.

    Uses `docker logs` to capture all container output and saves it to
    ~/.jib-sharing/container-logs/{container_id}.log

    Also creates a symlink from task_id.log -> container_id.log for easy lookup,
    and updates the log index for correlation searches.

    Args:
        container_id: Docker container ID/name
        task_id: Optional task ID for symlink creation
        thread_ts: Optional Slack thread timestamp for index

    Returns:
        Path to the saved log file, or None if saving failed
    """
    CONTAINER_LOGS_DIR.mkdir(parents=True, exist_ok=True)

    log_file = CONTAINER_LOGS_DIR / f"{container_id}.log"

    try:
        # Get container logs using docker logs command
        result = subprocess.run(
            ["docker", "logs", container_id],
            capture_output=True,
            text=True,
            timeout=30
        )

        # Check log size before writing to prevent disk space exhaustion
        max_log_size = 100 * 1024 * 1024  # 100MB
        total_log_size = len(result.stdout) + len(result.stderr)
        if total_log_size > max_log_size:
            warn(f"Container logs exceed {max_log_size / (1024 * 1024):.0f}MB, truncating...")
            # Truncate stderr/stdout proportionally
            if result.stdout:
                result.stdout = result.stdout[:max_log_size // 2] + "\n\n[... truncated ...]\n"
            if result.stderr:
                result.stderr = result.stderr[:max_log_size // 2] + "\n\n[... truncated ...]\n"

        # Write logs (both stdout and stderr)
        with open(log_file, "w") as f:
            f.write(f"=== Container: {container_id} ===\n")
            f.write(f"=== Saved: {datetime.now().isoformat()} ===\n")
            if task_id:
                f.write(f"=== Task ID: {task_id} ===\n")
            if thread_ts:
                f.write(f"=== Thread TS: {thread_ts} ===\n")
            f.write("=" * 50 + "\n\n")

            if result.stdout:
                f.write("=== STDOUT ===\n")
                f.write(result.stdout)
                f.write("\n")

            if result.stderr:
                f.write("\n=== STDERR ===\n")
                f.write(result.stderr)

        # Create symlink from task_id if provided
        if task_id:
            task_log_link = CONTAINER_LOGS_DIR / f"{task_id}.log"
            # Remove existing symlink if present
            if task_log_link.is_symlink():
                task_log_link.unlink()
            # Create relative symlink
            task_log_link.symlink_to(f"{container_id}.log")

        # Update log index for correlation lookups
        update_log_index(container_id, task_id, thread_ts, str(log_file))

        if not _quiet_mode:
            info(f"Container logs saved: {log_file}")
            if task_id:
                info(f"  Symlink: {task_id}.log -> {container_id}.log")

        return log_file

    except subprocess.TimeoutExpired:
        warn("Timed out getting container logs")
    except FileNotFoundError:
        # Container doesn't exist or already removed
        pass
    except Exception as e:
        warn(f"Failed to save container logs: {e}")

    return None


def get_default_branch(repo_path: Path) -> str:
    """Get the default branch (main or master) for a git repository.

    Args:
        repo_path: Path to the git repository

    Returns:
        The default branch name ('main' or 'master'), or 'main' as fallback
    """
    # Try to get the default branch from git config (set by clone)
    result = subprocess.run(
        ["git", "config", "--get", "init.defaultBranch"],
        cwd=repo_path,
        capture_output=True,
        text=True
    )
    if result.returncode == 0 and result.stdout.strip():
        return result.stdout.strip()

    # Check if origin/HEAD exists and points to a branch
    result = subprocess.run(
        ["git", "symbolic-ref", "refs/remotes/origin/HEAD"],
        cwd=repo_path,
        capture_output=True,
        text=True
    )
    if result.returncode == 0 and result.stdout.strip():
        # Format: refs/remotes/origin/main -> main
        return result.stdout.strip().split("/")[-1]

    # Fall back to checking which branches exist
    for branch in ["main", "master"]:
        result = subprocess.run(
            ["git", "rev-parse", "--verify", f"refs/heads/{branch}"],
            cwd=repo_path,
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            return branch

    # Default to 'main' if nothing found
    return "main"


def _acquire_git_lock(repo_path: Path, timeout: float = 30.0) -> Optional[int]:
    """Acquire an exclusive lock for git operations on a repository.

    Git doesn't handle concurrent operations well. When multiple jib --exec
    containers try to create worktrees simultaneously, they can conflict on
    the same repo's config/index files. This function provides file-based
    locking to serialize git operations per repository.

    Args:
        repo_path: Path to the git repository
        timeout: Maximum time to wait for lock in seconds

    Returns:
        File descriptor for the lock (must be closed to release), or None if failed
    """
    import fcntl
    import errno

    # Create lock file in the repo's .git directory
    git_dir = repo_path / ".git"
    if git_dir.is_file():
        # Worktree - read the actual git dir path
        try:
            content = git_dir.read_text().strip()
            if content.startswith("gitdir:"):
                git_dir = Path(content[7:].strip())
        except Exception:
            pass

    if not git_dir.is_dir():
        return None

    lock_file = git_dir / ".jib-worktree-lock"

    start_time = time.time()
    retry_delay = 0.1

    while True:
        try:
            # Open lock file (create if doesn't exist)
            fd = os.open(str(lock_file), os.O_CREAT | os.O_RDWR)

            # Try to acquire exclusive lock (non-blocking)
            fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)

            # Got the lock
            return fd

        except (OSError, IOError) as e:
            if e.errno in (errno.EAGAIN, errno.EWOULDBLOCK, errno.EACCES):
                # Lock held by another process
                elapsed = time.time() - start_time
                if elapsed >= timeout:
                    warn(f"Timeout waiting for git lock on {repo_path.name}")
                    if 'fd' in locals():
                        os.close(fd)
                    return None

                time.sleep(retry_delay)
                retry_delay = min(retry_delay * 1.5, 1.0)  # Exponential backoff, max 1s
                continue
            else:
                # Other error
                if 'fd' in locals():
                    os.close(fd)
                return None


def _release_git_lock(fd: int) -> None:
    """Release a git lock acquired with _acquire_git_lock."""
    import fcntl

    if fd is not None:
        try:
            fcntl.flock(fd, fcntl.LOCK_UN)
            os.close(fd)
        except Exception:
            pass


def create_worktrees(container_id: str) -> dict:
    """Create git worktrees for all repos in ~/khan/

    Worktrees are always based on the default branch (main or master),
    regardless of what branch is currently checked out on the host.

    Uses file-based locking to prevent conflicts when multiple jib --exec
    containers try to create worktrees simultaneously.

    Returns:
        Dictionary mapping repo names to worktree paths
    """
    worktree_dir = Config.WORKTREE_BASE / container_id
    worktree_dir.mkdir(parents=True, exist_ok=True)

    worktrees = {}

    if not Config.KHAN_SOURCE.exists():
        warn(f"Khan source directory not found: {Config.KHAN_SOURCE}")
        return worktrees

    # Iterate through all repos in ~/khan/
    for repo_path in Config.KHAN_SOURCE.iterdir():
        if not repo_path.is_dir():
            continue

        # Check if it's a git repository with a .git DIRECTORY
        # If .git is a file, the host repo is itself a worktree, which is unsupported
        git_dir = repo_path / ".git"
        if not git_dir.exists():
            continue

        repo_name = repo_path.name

        # Note: git worktree add works even when source repo is a worktree
        # It creates a new worktree of the same underlying main repository
        worktree_path = worktree_dir / repo_name

        # Acquire lock to prevent concurrent git operations on same repo
        lock_fd = _acquire_git_lock(repo_path, timeout=60.0)
        if lock_fd is None:
            warn(f"  âœ— Could not acquire lock for {repo_name}, skipping worktree")
            continue

        try:
            # Determine the default branch for this repo
            default_branch = get_default_branch(repo_path)

            if not _quiet_mode:
                info(f"Creating worktree for {repo_name} (from {default_branch})...")

            # Create temporary branch for this container, starting from the default branch
            branch_name = f"jib-temp-{container_id}"

            # Create worktree based on the default branch (not current HEAD)
            result = subprocess.run(
                ["git", "worktree", "add", str(worktree_path), "-b", branch_name, default_branch],
                cwd=repo_path,
                capture_output=True,
                text=True
            )

            if result.returncode == 0:
                worktrees[repo_name] = worktree_path
                if not _quiet_mode:
                    success(f"  âœ“ {repo_name} -> {worktree_path}")
            else:
                warn(f"  âœ— Failed to create worktree for {repo_name}: {result.stderr}")

        except Exception as e:
            warn(f"  âœ— Error creating worktree for {repo_name}: {e}")
        finally:
            _release_git_lock(lock_fd)

    return worktrees


def cleanup_worktrees(container_id: str) -> None:
    """Clean up worktrees for a container

    Args:
        container_id: Unique container identifier
    """
    import shutil

    worktree_dir = Config.WORKTREE_BASE / container_id

    if not worktree_dir.exists():
        return

    if not _quiet_mode:
        info(f"Cleaning up worktrees for {container_id}...")

    # Collect repos and their worktree admin directories
    # We need to forcibly remove admin dirs because container modified .git files
    # to point to container-only paths, which breaks git worktree prune
    repos_to_clean = {}  # repo_path -> list of admin dir names
    for worktree_path in worktree_dir.iterdir():
        if worktree_path.is_dir():
            repo_name = worktree_path.name
            original_repo = Config.KHAN_SOURCE / repo_name
            if original_repo.exists():
                # Find the admin directory name from the worktree's .git file
                git_file = worktree_path / ".git"
                if git_file.is_file():
                    try:
                        content = git_file.read_text().strip()
                        # Format: "gitdir: /path/to/.git/worktrees/NAME" or
                        # "gitdir: /path/to/.git-main/repo/worktrees/NAME"
                        if "worktrees/" in content:
                            admin_name = content.split("worktrees/")[-1]
                            if original_repo not in repos_to_clean:
                                repos_to_clean[original_repo] = []
                            repos_to_clean[original_repo].append(admin_name)
                    except Exception:
                        pass

    # Remove container worktree directory first
    try:
        shutil.rmtree(worktree_dir)
        if not _quiet_mode:
            info(f"  âœ“ Removed worktree directory")
    except Exception as e:
        warn(f"  âœ— Failed to remove directory {worktree_dir}: {e}")

    # Forcibly remove worktree admin directories
    # Don't rely on git worktree prune - it fails when .git files are corrupted
    admin_dirs_removed = 0
    for repo_path, admin_names in repos_to_clean.items():
        worktrees_dir = repo_path / ".git" / "worktrees"
        if worktrees_dir.is_dir():
            for admin_name in admin_names:
                admin_dir = worktrees_dir / admin_name
                if admin_dir.exists():
                    try:
                        shutil.rmtree(admin_dir)
                        admin_dirs_removed += 1
                    except Exception as e:
                        warn(f"  âœ— Failed to remove admin dir {admin_dir}: {e}")

    # Also run git worktree prune as a fallback for any we missed
    for repo_path in repos_to_clean.keys():
        try:
            subprocess.run(
                ["git", "worktree", "prune", "-v"],
                cwd=repo_path,
                capture_output=True,
                text=True,
                check=False  # Don't fail if prune has issues
            )
        except Exception:
            pass

    if admin_dirs_removed > 0 and not _quiet_mode:
        info(f"  âœ“ Removed {admin_dirs_removed} worktree admin dir(s)")
        info(f"  Commits preserved on branch: jib-temp-{container_id}")


def setup() -> bool:
    """Interactive setup process"""
    print()
    info("=== Autonomous Software Engineering Agent - Setup ===")
    print()
    print("ðŸ¤– AUTONOMOUS ENGINEERING AGENT")
    print()
    print("This sets up a sandboxed environment for Claude to work as an autonomous")
    print("software engineer with minimal supervision.")
    print()
    print("OPERATING MODEL:")
    print("  â€¢ Agent: Plans, implements, tests, documents, creates PRs")
    print("  â€¢ Human: Reviews, approves, deploys")
    print()
    print("AGENT CAPABILITIES:")
    print("  âœ“ Edit code and create commits in ~/khan/")
    print("  âœ“ Run tests, linters, development servers")
    print("  âœ“ Access Confluence docs (ADRs, runbooks, best practices)")
    print("  âœ“ Create pull requests with @create-pr command")
    print("  âœ“ Build accumulated knowledge with @save-context")
    print("  âœ“ Network access for Claude API and package installs")
    print()
    print("SECURITY ISOLATION:")
    print("  âœ— NO access to SSH keys (cannot git push)")
    print("  âœ— NO access to gcloud credentials (cannot deploy)")
    print("  âœ— NO access to GSM secrets")
    print()
    print("HOW IT WORKS:")
    print("  1. Your ~/khan is MOUNTED into the container as ~/khan (read-write)")
    print("  2. Changes in container = changes on host (same files)")
    print("  3. Agent works on code, creates commits")
    print("  4. YOU review commits and push from host (with credentials)")
    print()
    print("FUTURE CAPABILITIES (Roadmap):")
    print("  ðŸ”„ GitHub PR context")
    print("  ðŸ”„ Slack message context")
    print("  ðŸ”„ JIRA ticket context")
    print("  ðŸ”„ Email thread context")
    print()

    response = input("Continue? (yes/no): ").strip().lower()
    if response != "yes":
        info("Setup cancelled")
        return False

    print()
    info("Setting up mounts...")
    print()

    Config.CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    mounts = []

    # Khan directory - mount READ-WRITE so agent can work directly
    if Config.KHAN_SOURCE.exists():
        khan_container_path = f"/home/{os.environ['USER']}/khan"
        # Mount as read-write with SELinux relabeling
        mounts.append(f"{Config.KHAN_SOURCE}:{khan_container_path}:rw,z")
        info(f"Khan workspace: {Config.KHAN_SOURCE}")
        print(f"    Mounted as: ~/khan/ (READ-WRITE, SELinux relabeled)")
        print(f"    Purpose: Agent works directly on code, makes commits")
        print(f"    Changes: Agent modifies files in place - you review and push")
    else:
        warn(f"{Config.KHAN_SOURCE} not found - workspace will not be available")

    # Add context-sync directory (read-only) - includes Confluence, JIRA, and more
    print()
    context_sync_dir = Path.home() / "context-sync"
    if context_sync_dir.exists():
        context_container_path = f"/home/{os.environ['USER']}/context-sync"
        # Add :z flag for SELinux systems
        mounts.append(f"{context_sync_dir}:{context_container_path}:ro,z")
        print(f"  âœ“ Context sources: {context_sync_dir}")
        print(f"    Mounted as: ~/context-sync/ (read-only, SELinux relabeled)")

        # Show available context sources
        subdirs = []
        if (context_sync_dir / "confluence").exists():
            subdirs.append("confluence (ADRs, runbooks, docs)")
        if (context_sync_dir / "jira").exists():
            subdirs.append("jira (tickets, issues)")
        if (context_sync_dir / "github").exists():
            subdirs.append("github (PRs, issues)")
        if (context_sync_dir / "slack").exists():
            subdirs.append("slack (messages)")

        if subdirs:
            print(f"    Contains: {', '.join(subdirs)}")
        else:
            print(f"    Note: No context subdirectories found yet")
    else:
        warn(f"Context sync directory not found: {context_sync_dir}")
        warn(f"Expected directory with confluence/, jira/, etc. subdirectories")

    # Create and mount persistent directories for agent
    print()
    info("Setting up persistent directories...")

    # Sharing directory - single location for ALL persistent data
    Config.SHARING_DIR.mkdir(parents=True, exist_ok=True)
    Config.TMP_DIR.mkdir(parents=True, exist_ok=True)    # tmp/ inside sharing/
    Config.CLAUDE_AUTH_DIR.mkdir(parents=True, exist_ok=True)  # claude-auth/ for auth files only

    sharing_container_path = f"/home/{os.environ['USER']}/sharing"
    # Add :z flag for SELinux systems
    mounts.append(f"{Config.SHARING_DIR}:{sharing_container_path}:rw,z")
    print(f"  âœ“ Sharing: {Config.SHARING_DIR}")
    print(f"    Mounted as: ~/sharing/ (read-write, SELinux relabeled)")
    print(f"    Purpose: All persistent data")
    print(f"    - ~/sharing/tmp/           Persistent workspace (also at ~/tmp)")
    print(f"    - ~/sharing/context/       Context documents (@save-context)")
    print(f"    - ~/sharing/notifications/ Notifications to human")
    print(f"    - ~/sharing/incoming/      Incoming tasks from Slack")
    print(f"    - ~/sharing/analysis/      Analysis reports")

    # Create convenience symlink in container for tmp
    # Note: Actual symlink creation happens in container entrypoint

    # Mount shared Claude auth files (NOT the whole .claude directory)
    # This prevents history/session data from being shared across containers
    # while still sharing authentication credentials
    print()
    print(f"{Colors.BOLD}Claude Code authentication...{Colors.NC}")

    # Mount Claude auth directory (contains .credentials.json and settings.json)
    # Container's entrypoint will copy these to ~/.claude/ on startup
    claude_auth_container_path = f"/home/{os.environ['USER']}/.claude-auth"
    mounts.append(f"{Config.CLAUDE_AUTH_DIR}:{claude_auth_container_path}:rw,z")

    # Mount ~/.claude.json file (user state with userID)
    claude_json_file = Config.CONFIG_DIR / "claude.json"
    if not claude_json_file.exists():
        # Initialize with empty JSON object (Claude Code will populate on first run)
        claude_json_file.write_text("{}\n")
    claude_json_container_path = f"/home/{os.environ['USER']}/.claude.json"
    mounts.append(f"{claude_json_file}:{claude_json_container_path}:rw,z")

    # Check if credentials already exist in shared directory
    shared_creds = Config.CLAUDE_AUTH_DIR / ".credentials.json"
    if shared_creds.exists():
        success("Claude credentials found in shared directory")
        print(f"  Auth files shared via: {Config.CLAUDE_AUTH_DIR}")
        print(f"  Session data: ephemeral (per-container)")
    else:
        info("Claude credentials not yet configured")
        print(f"  Auth files will be saved to: {Config.CLAUDE_AUTH_DIR}")
        print()
        info("The first container will authenticate and save credentials.")
        info("All other containers will automatically use the same credentials.")
        info("Session history is NOT shared (each container has its own).")

    print()
    print("Add additional directories? (optional)")
    print("Format: /path/to/dir        (read-write)")
    print("    or: /path/to/dir:ro     (read-only)")
    print("Press Enter on empty line when done")
    print()

    # Collect additional directories
    while True:
        dir_input = input("Additional directory (or Enter to finish): ").strip()
        if not dir_input:
            break

        # Parse mode
        if ":ro" in dir_input or ":rw" in dir_input:
            mount_path_str, mode = dir_input.rsplit(":", 1)
            if mode not in ["ro", "rw"]:
                warn(f"Invalid mode '{mode}', use 'ro' or 'rw'")
                continue
        else:
            mount_path_str = dir_input
            mode = "rw"

        # Expand and validate path
        mount_path = Path(mount_path_str).expanduser().resolve()

        # Check if dangerous
        if is_dangerous_dir(mount_path):
            print(f"â›” BLOCKED: {mount_path}")
            print("   This directory contains credentials and will not be mounted.")
            print("   This is intentional to prevent AI from accessing sensitive files.")
            continue

        if not mount_path.exists():
            warn(f"Directory does not exist: {mount_path}")
            create = input("Create it? (yes/no): ").strip().lower()
            if create == "yes":
                try:
                    mount_path.mkdir(parents=True, exist_ok=True)
                    success(f"Created: {mount_path}")
                except Exception as e:
                    error(f"Failed to create directory: {e}")
                    continue
            else:
                continue

        # Add SELinux label for Fedora/RHEL compatibility
        mounts.append(f"{mount_path}:{mode},z")
        print(f"Added: {mount_path} ({mode}, SELinux relabeled)")

    # Save configuration
    Config.CONFIG_FILE.write_text("\n".join(mounts))

    print()
    info("Summary of mounted directories:")
    for mount in mounts:
        print(f"  â€¢ {mount}")
    print()

    proceed = input("Proceed with this configuration? (yes/no): ").strip().lower()
    if proceed != "yes":
        info("Setup cancelled")
        return False

    # Create Dockerfile and build image
    create_dockerfile()
    print()

    # Let Docker's cache handle what needs rebuilding
    info("Building Docker image (Docker will cache unchanged layers)...")
    if not build_image():
        return False

    print()
    success("Setup complete!")
    print()
    return True


def run_claude() -> bool:
    """Run Claude Code CLI in the sandboxed container (interactive mode)"""
    # Check if image exists
    if _quiet_mode:
        status("Checking Docker image...")
    if not image_exists():
        info("Docker image not found. Running initial setup...")
        if not setup():
            return False

    # Load mount configuration
    if not Config.CONFIG_FILE.exists():
        info("Configuration not found. Running initial setup...")
        if not setup():
            return False

    # Ensure shared authentication directory exists
    if _quiet_mode:
        status("Checking authentication...")
    Config.CLAUDE_AUTH_DIR.mkdir(parents=True, exist_ok=True)
    claude_json_file = Config.CONFIG_DIR / "claude.json"
    if not claude_json_file.exists():
        # Initialize with empty JSON object (Claude Code will populate on first run)
        claude_json_file.write_text("{}\n")

    # Migration: copy credentials from old shared directory if it exists
    old_claude_dir = Config.CONFIG_DIR / "claude"  # Old location: ~/.jib/claude/
    old_creds = old_claude_dir / ".credentials.json"
    new_creds = Config.CLAUDE_AUTH_DIR / ".credentials.json"
    if old_creds.exists() and not new_creds.exists():
        import shutil
        info("Migrating credentials from old shared directory...")
        shutil.copy2(old_creds, new_creds)
        new_creds.chmod(0o600)
        success("Credentials migrated to new auth-only directory")
        print(f"  Old: {old_claude_dir} (can be deleted)")
        print(f"  New: {Config.CLAUDE_AUTH_DIR}")
        print()

    # Check Claude Code authentication in shared auth directory
    if not _quiet_mode:
        print()
        print(f"{Colors.BOLD}Checking Claude Code authentication...{Colors.NC}")

        shared_creds = Config.CLAUDE_AUTH_DIR / ".credentials.json"

        if shared_creds.exists():
            try:
                with open(shared_creds, 'r') as f:
                    creds = json.load(f)

                scopes = creds.get("claudeAiOauth", {}).get("scopes", [])
                required_scopes = {
                    "user:inference",
                    "user:profile",
                    "user:sessions:claude_code"
                }

                if required_scopes.issubset(set(scopes)):
                    success("Claude Code authenticated with full scopes")
                    print(f"  Auth shared via: {Config.CLAUDE_AUTH_DIR}")
                    print(f"  Session history: ephemeral (per-container)")
                else:
                    warn("Claude Code credentials found but missing required scopes")
                    warn("  Container will work with limited functionality (--print mode only)")
                    print(f"  Location: {Config.CLAUDE_AUTH_DIR}")
            except Exception as e:
                warn(f"Could not read shared credentials: {e}")
                warn("Container will prompt for authentication on first run")
        else:
            info("Claude Code not yet authenticated")
            info("Container will prompt for authentication on first launch")
            info("Credentials will be saved and shared across all containers")
            info("Session history is NOT shared (each container has its own)")
            print(f"  Auth location: {Config.CLAUDE_AUTH_DIR}")

        print()

    # Build/update image (Docker uses cache for unchanged layers - usually instant)
    if _quiet_mode:
        status("Building Docker image...")
    if not build_image():
        error("Docker build failed")
        return False

    # Generate unique container ID
    if _quiet_mode:
        status("Preparing container...")
    container_id = generate_container_id()

    if not _quiet_mode:
        info("Launching sandboxed Claude Code environment...")
        print()
        info(f"Container ID: {container_id}")
        print()

    # Create worktrees for repos (isolates container from host repos)
    if _quiet_mode:
        status("Creating isolated worktrees...")
    else:
        info("Creating isolated worktrees...")
        print()
    worktrees = create_worktrees(container_id)

    if worktrees and not _quiet_mode:
        print()
        info(f"Created {len(worktrees)} worktree(s)")
        print()

    # Register cleanup on exit
    def cleanup_on_exit():
        cleanup_worktrees(container_id)

    atexit.register(cleanup_on_exit)

    # Parse mount configuration
    if _quiet_mode:
        status("Configuring mounts...")
    mount_args = []
    mounts = Config.CONFIG_FILE.read_text().strip().split("\n")
    for mount in mounts:
        if not mount:
            continue
        parts = mount.rsplit(":", 2)  # Split from right, max 2 splits

        if len(parts) == 2:
            # Old format: path:mode
            mount_path, mode = parts
            container_path = mount_path
        elif len(parts) == 3:
            # New format: host_path:container_path:mode
            mount_path, container_path, mode = parts
        else:
            warn(f"Invalid mount format: {mount}")
            continue

        # Check if this is a khan repo mount - replace with worktree
        if Config.KHAN_SOURCE and mount_path.startswith(str(Config.KHAN_SOURCE)):
            # This is a khan repo - skip it, we'll add worktrees instead
            continue

        # Ensure SELinux label is present (add if missing for backwards compatibility)
        if not mode.endswith(",z") and not mode.endswith(":z"):
            mode = f"{mode},z"

        mount_args.extend(["-v", f"{mount_path}:{container_path}:{mode}"])
        if not _quiet_mode:
            mode_str = "READ-ONLY" if "ro" in mode else "READ-WRITE"
            display_path = container_path if container_path != mount_path else mount_path
            print(f"  â€¢ {display_path} ({mode_str}, SELinux relabeled)")

    # Add worktree mounts
    # Also track which main repos need their .git directories mounted
    main_repos_needed = set()
    for repo_name, worktree_path in worktrees.items():
        container_path = f"/home/{os.environ['USER']}/khan/{repo_name}"
        mount_args.extend(["-v", f"{worktree_path}:{container_path}:rw,z"])
        if not _quiet_mode:
            print(f"  â€¢ ~/khan/{repo_name} (WORKTREE, isolated from host)")
        main_repos_needed.add(repo_name)

    # Mount main repo .git directories (read-write) so worktrees can commit changes
    for repo_name in main_repos_needed:
        main_repo = Config.KHAN_SOURCE / repo_name
        git_path = main_repo / ".git"
        if git_path.is_dir():
            # Normal repo - mount .git directory directly
            git_container_path = f"/home/{os.environ['USER']}/.git-main/{repo_name}"
            mount_args.extend(["-v", f"{git_path}:{git_container_path}:rw,z"])
            if not _quiet_mode:
                print(f"  â€¢ ~/.git-main/{repo_name} (git metadata for worktree, read-write)")
        elif git_path.is_file():
            # Host repo is a worktree - read .git file to find actual git directory
            # Format is: "gitdir: /path/to/repo.git/worktrees/name"
            try:
                gitdir_content = git_path.read_text().strip()
                if gitdir_content.startswith("gitdir:"):
                    gitdir_path = Path(gitdir_content[7:].strip())
                    # Navigate up from worktrees/<name> to the main .git directory
                    # e.g., /path/.git/worktrees/foo -> /path/.git
                    if "worktrees" in gitdir_path.parts:
                        worktrees_idx = gitdir_path.parts.index("worktrees")
                        main_git_path = Path(*gitdir_path.parts[:worktrees_idx])
                        if main_git_path.is_dir():
                            git_container_path = f"/home/{os.environ['USER']}/.git-main/{repo_name}"
                            mount_args.extend(["-v", f"{main_git_path}:{git_container_path}:rw,z"])
                            if not _quiet_mode:
                                print(f"  â€¢ ~/.git-main/{repo_name} (git metadata, from host worktree)")
                        else:
                            warn(f"Could not find git directory for {repo_name}: {main_git_path}")
                    else:
                        warn(f"Unexpected gitdir format for {repo_name}: {gitdir_path}")
                else:
                    warn(f"Invalid .git file format for {repo_name}")
            except Exception as e:
                warn(f"Error reading .git file for {repo_name}: {e}")

    # Mount worktree base directory (used by both interactive and --exec modes)
    worktree_base_container = f"/home/{os.environ['USER']}/.jib-worktrees"
    mount_args.extend(["-v", f"{Config.WORKTREE_BASE}:{worktree_base_container}:rw,z"])
    if not _quiet_mode:
        print(f"  â€¢ ~/.jib-worktrees/ (worktree base directory)")

    if not _quiet_mode:
        print()

    # Remove old container if exists (cleanup any previous runs)
    subprocess.run(["docker", "rm", "-f", container_id],
                  stdout=subprocess.DEVNULL,
                  stderr=subprocess.DEVNULL)

    # Build docker run command with bridge networking (more secure than --net host)
    # Bridge mode allows outbound HTTP/HTTPS (for Claude API and packages) but no inbound access
    cmd = [
        "docker", "run",
        "--rm",  # Auto-remove container after exit
        "-it",   # Interactive with TTY
        "--name", container_id,
        "-e", f"RUNTIME_USER={os.environ['USER']}",
        "-e", f"RUNTIME_UID={os.getuid()}",
        "-e", f"RUNTIME_GID={os.getgid()}",
        "-e", f"CONTAINER_ID={container_id}",
        "-e", f"JIB_QUIET={'1' if _quiet_mode else '0'}",
        # Bridge networking: outbound HTTP allowed, no inbound ports exposed
    ]

    # Add GitHub token for container (prefer App token, fall back to PAT)
    # Also write token to shared file for long-running container support
    if _quiet_mode:
        status("Configuring GitHub access...")
    github_app_token = get_github_app_token()
    if github_app_token:
        cmd.extend(["-e", f"GITHUB_TOKEN={github_app_token}"])
        # Write token to shared file for auto-refresh support
        write_github_token_file(github_app_token)
        if not _quiet_mode:
            info("GitHub auth: App installation token (auto-refresh enabled)")
            print("  Token written to ~/.jib-sharing/.github-token")
            print("  github-token-refresher service will keep it fresh")
            print("  Full GitHub API access including Checks API")
    else:
        # Fall back to PAT if App not configured
        github_token = get_github_token()
        if github_token:
            cmd.extend(["-e", f"GITHUB_TOKEN={github_token}"])
            # Also write PAT to shared file (won't be auto-refreshed but provides consistency)
            write_github_token_file(github_token)
            if not _quiet_mode:
                info("GitHub auth: Personal Access Token (fallback)")
                warn("  Note: PAT cannot access Checks API. Configure GitHub App for full access.")
        else:
            # Only warn if not in quiet mode - GitHub access is optional
            if not _quiet_mode:
                warn("GitHub auth: Not configured (GitHub operations disabled)")
                print("  Configure GitHub App: ~/.config/jib/github-app-*.pem")

    if not _quiet_mode:
        info("Network mode: Bridge (isolated from host, outbound HTTP only)")
        print("  Container can: Access Claude API, download packages")
        print("  Container cannot: Access host services, accept inbound connections")
        print()

        info("Claude authentication: Will authenticate with browser on first run")
        print()

    # Add mount arguments
    cmd.extend(mount_args)

    # Add image name
    cmd.append(Config.IMAGE_NAME)

    # Final status update before launching
    if _quiet_mode:
        status("Launching Claude...")

    # Run container
    try:
        subprocess.run(cmd)
        return True
    except KeyboardInterrupt:
        print()
        warn("Interrupted by user")
        return False
    except Exception as e:
        error(f"Failed to run container: {e}")
        return False


def exec_in_new_container(
    command: List[str],
    timeout_minutes: int = 30,
    task_id: Optional[str] = None,
    thread_ts: Optional[str] = None,
) -> bool:
    """Execute a command in a new ephemeral container with isolated worktrees.

    Creates worktrees for all repos to isolate changes (same as interactive mode):
    - Total isolation from interactive sessions and main repos
    - Worktrees allow parallel work without conflicts
    - Automatic cleanup (--rm)
    - All commits go to temporary branches (jib-temp-jib-exec-*)
    - Container logs persisted to ~/.jib-sharing/container-logs/

    Args:
        command: Command to execute
        timeout_minutes: Timeout in minutes (default: 30)
        task_id: Optional task ID for log correlation (auto-detected from command if not provided)
        thread_ts: Optional Slack thread timestamp for correlation

    Returns:
        True if successful, False otherwise
    """
    # Check if image exists
    if not image_exists():
        info("Docker image not found. Running initial setup...")
        if not setup():
            return False

    # Load mount configuration
    if not Config.CONFIG_FILE.exists():
        info("Configuration not found. Running initial setup...")
        if not setup():
            return False

    # Build/update image
    if not build_image():
        error("Docker build failed")
        return False

    # Generate unique container ID for this exec
    container_id = f"jib-exec-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{os.getpid()}"

    # Auto-detect task_id from command if not provided
    if not task_id:
        task_id = extract_task_id_from_command(command)

    # Auto-detect thread_ts from task file if not provided
    if not thread_ts and task_id:
        for arg in command:
            if ".md" in arg:
                thread_ts = extract_thread_ts_from_task_file(arg)
                break

    info(f"Executing command in new container: {container_id}")
    if task_id:
        info(f"Task ID: {task_id}")
    if thread_ts:
        info(f"Thread TS: {thread_ts}")
    print(f"Command: {' '.join(command)}")
    print(f"Timeout: {timeout_minutes} minutes")
    print()

    # Create worktrees for repos (isolates container from host repos)
    info("Creating isolated worktrees...")
    worktrees = create_worktrees(container_id)

    if worktrees:
        info(f"Created {len(worktrees)} worktree(s)")
        print()

    # Register cleanup on exit (even if container fails)
    def cleanup_on_exit():
        cleanup_worktrees(container_id)

    atexit.register(cleanup_on_exit)

    # Parse mount configuration
    mount_args = []
    mounts = Config.CONFIG_FILE.read_text().strip().split("\n")

    for mount in mounts:
        if not mount:
            continue
        parts = mount.rsplit(":", 2)

        if len(parts) == 2:
            mount_path, mode = parts
            container_path = mount_path
        elif len(parts) == 3:
            mount_path, container_path, mode = parts
        else:
            warn(f"Invalid mount format: {mount}")
            continue

        # Check if this is a khan repo mount - skip it, we'll add worktrees instead
        if Config.KHAN_SOURCE and mount_path.startswith(str(Config.KHAN_SOURCE)):
            continue

        # Ensure SELinux label
        if not mode.endswith(",z") and not mode.endswith(":z"):
            mode = f"{mode},z"

        mount_args.extend(["-v", f"{mount_path}:{container_path}:{mode}"])

    # Add worktree mounts
    main_repos_needed = set()
    for repo_name, worktree_path in worktrees.items():
        container_path = f"/home/{os.environ['USER']}/khan/{repo_name}"
        mount_args.extend(["-v", f"{worktree_path}:{container_path}:rw,z"])
        print(f"  â€¢ ~/khan/{repo_name} (WORKTREE, isolated)")
        main_repos_needed.add(repo_name)

    # Mount main repo .git directories (read-write) so worktrees can commit changes
    for repo_name in main_repos_needed:
        main_repo = Config.KHAN_SOURCE / repo_name
        git_path = main_repo / ".git"
        if git_path.is_dir():
            # Normal repo - mount .git directory directly
            git_container_path = f"/home/{os.environ['USER']}/.git-main/{repo_name}"
            mount_args.extend(["-v", f"{git_path}:{git_container_path}:rw,z"])
            print(f"  â€¢ ~/.git-main/{repo_name} (git metadata)")
        elif git_path.is_file():
            # Host repo is a worktree - read .git file to find actual git directory
            try:
                gitdir_content = git_path.read_text().strip()
                if gitdir_content.startswith("gitdir:"):
                    gitdir_path = Path(gitdir_content[7:].strip())
                    if "worktrees" in gitdir_path.parts:
                        worktrees_idx = gitdir_path.parts.index("worktrees")
                        main_git_path = Path(*gitdir_path.parts[:worktrees_idx])
                        if main_git_path.is_dir():
                            git_container_path = f"/home/{os.environ['USER']}/.git-main/{repo_name}"
                            mount_args.extend(["-v", f"{main_git_path}:{git_container_path}:rw,z"])
                            print(f"  â€¢ ~/.git-main/{repo_name} (git metadata, from host worktree)")
                        else:
                            warn(f"Could not find git directory for {repo_name}: {main_git_path}")
                    else:
                        warn(f"Unexpected gitdir format for {repo_name}: {gitdir_path}")
                else:
                    warn(f"Invalid .git file format for {repo_name}")
            except Exception as e:
                warn(f"Error reading .git file for {repo_name}: {e}")

    print()

    # Build docker run command with logging configuration
    # Note: We don't use --rm so we can save logs before cleanup
    cmd = [
        "docker", "run",
        "--name", container_id,
        "-e", f"RUNTIME_USER={os.environ['USER']}",
        "-e", f"RUNTIME_UID={os.getuid()}",
        "-e", f"RUNTIME_GID={os.getgid()}",
        "-e", f"CONTAINER_ID={container_id}",
        "-e", "PYTHONUNBUFFERED=1",  # Force Python to use unbuffered output for real-time streaming
    ]

    # Add logging configuration for log persistence
    log_config = get_docker_log_config(container_id, task_id)
    cmd.extend(log_config)

    # Add correlation environment variables for log tracing
    if task_id:
        cmd.extend(["-e", f"JIB_TASK_ID={task_id}"])
    if thread_ts:
        cmd.extend(["-e", f"JIB_THREAD_TS={thread_ts}"])

    # Add GitHub token for container (prefer App token, fall back to PAT)
    # Also write token to shared file for auto-refresh support
    github_app_token = get_github_app_token()
    if github_app_token:
        cmd.extend(["-e", f"GITHUB_TOKEN={github_app_token}"])
        write_github_token_file(github_app_token)
    else:
        github_token = get_github_token()
        if github_token:
            cmd.extend(["-e", f"GITHUB_TOKEN={github_token}"])
            write_github_token_file(github_token)

    # Add mount arguments
    cmd.extend(mount_args)

    # Add image name
    cmd.append(Config.IMAGE_NAME)

    # Add the command to execute
    cmd.extend(command)

    # Run container with configurable timeout
    timeout_seconds = timeout_minutes * 60
    success = False

    def cleanup_container():
        """Save logs and remove container."""
        try:
            # Save container logs before removal (with correlation info)
            save_container_logs(container_id, task_id, thread_ts)
        except Exception as e:
            error(f"Failed to save container logs: {e}")
            # Don't re-raise - continue with container removal
        finally:
            # Remove container
            try:
                subprocess.run(
                    ["docker", "rm", "-f", container_id],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL
                )
            except Exception as e:
                error(f"Failed to remove container: {e}")
                # Don't re-raise - original error is more important

    try:
        result = subprocess.run(cmd, timeout=timeout_seconds)
        success = result.returncode == 0
    except subprocess.TimeoutExpired:
        print()
        error(f"Container execution timed out after {timeout_minutes} minutes")
        # Kill the container if it's still running
        subprocess.run(["docker", "kill", container_id],
                      stdout=subprocess.DEVNULL,
                      stderr=subprocess.DEVNULL)
    except KeyboardInterrupt:
        print()
        warn("Interrupted by user")
        # Kill container on interrupt
        subprocess.run(["docker", "kill", container_id],
                      stdout=subprocess.DEVNULL,
                      stderr=subprocess.DEVNULL)
    except Exception as e:
        error(f"Failed to run container: {e}")
    finally:
        # Always save logs and cleanup container
        cleanup_container()

    return success


def main():
    parser = argparse.ArgumentParser(
        description="Run Claude Code CLI in an isolated Docker container (james-in-a-box)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  jib                                      # Run Claude Code (progress bar by default, auto-setup if needed)
  jib -v                                   # Run in verbose mode (detailed output)
  jib --setup                              # Run full setup (delegates to setup.sh)
  jib --reset                              # Reset configuration and remove Docker image
  jib --exec <command> [args...]          # Execute command in new ephemeral container
  jib --timeout 60 --exec <command>       # Execute with custom timeout (60 minutes)

Note: --exec spawns a new container for each execution (automatic cleanup with --rm)
      Default timeout is 30 minutes, configurable via --timeout
      If setup is incomplete, jib will prompt to run setup automatically
      Default shows progress bar; use -v for verbose output
        """
    )
    parser.add_argument(
        "--setup",
        action="store_true",
        help="Run full jib setup (services, config, Docker image)"
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="Clear configuration and start over"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=30,
        metavar="MINUTES",
        help="Timeout in minutes for --exec commands (default: 30)"
    )
    parser.add_argument(
        "--exec",
        nargs=argparse.REMAINDER,
        help="Execute a command in a new ephemeral container (automatic cleanup)"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Verbose mode: show detailed output instead of progress bar (default: quiet with progress bar)"
    )

    args = parser.parse_args()

    # Initialize quiet mode globally
    # Quiet is the default; verbose (-v) overrides it
    global _quiet_mode
    _quiet_mode = not args.verbose
    if _quiet_mode:
        # Initialize statusbar with estimated steps for interactive mode
        # Steps: check docker image, check auth, build image, prepare container,
        #        create worktrees, configure mounts, configure github, launch
        init_statusbar(total_steps=8, enabled=True)


    # Handle reset
    if args.reset:
        warn("Resetting configuration...")
        import shutil
        if Config.CONFIG_DIR.exists():
            shutil.rmtree(Config.CONFIG_DIR)

        # Ask about persistent directories
        if Config.TOOLS_DIR.exists() or Config.SHARING_DIR.exists():
            print()
            warn("Persistent directories found:")
            if Config.TOOLS_DIR.exists():
                print(f"  â€¢ {Config.TOOLS_DIR} (reusable scripts/tools)")
            if Config.SHARING_DIR.exists():
                print(f"  â€¢ {Config.SHARING_DIR} (shared artifacts, context documents)")
            print()
            response = input("Remove these as well? (yes/no): ").strip().lower()
            if response == "yes":
                if Config.TOOLS_DIR.exists():
                    shutil.rmtree(Config.TOOLS_DIR)
                    warn(f"Removed: {Config.TOOLS_DIR}")
                if Config.SHARING_DIR.exists():
                    shutil.rmtree(Config.SHARING_DIR)
                    warn(f"Removed: {Config.SHARING_DIR}")
            else:
                info("Preserved persistent directories")

        success("Configuration reset. Run again to set up fresh.")
        return 0

    # Check prerequisites
    if not check_docker():
        return 1

    if not check_docker_permissions():
        return 1

    # Check host setup (services and directories)
    if not check_host_setup():
        return 1

    # Handle setup - delegate to setup.sh
    if args.setup:
        info("Delegating to setup.sh for complete jib configuration...")
        print()
        if not run_setup_script():
            return 1
        return 0

    # Handle exec - execute in a new ephemeral container
    if args.exec:
        if not exec_in_new_container(args.exec, timeout_minutes=args.timeout):
            return 1
        return 0

    # Normal run
    if not run_claude():
        return 1

    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print()
        warn("Interrupted by user")
        sys.exit(0)
    except Exception as e:
        error(f"Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
