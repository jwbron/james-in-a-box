#!/usr/bin/env python3
"""
jib (james-in-a-box) - Run Claude Code CLI in an isolated Docker container

Prevents AI agents from accessing credentials while allowing full code editing.

Platform Support:
  - Linux (x86_64, ARM64): Fully supported
  - macOS (Intel, Apple Silicon): Fully supported
"""

import argparse
import atexit
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List, Optional

# Add shared modules to path (relative to jib-container directory)
_SCRIPT_DIR = Path(__file__).parent.resolve()
_SHARED_DIR = _SCRIPT_DIR.parent / "shared"
if _SHARED_DIR.exists():
    sys.path.insert(0, str(_SHARED_DIR))

from statusbar import StatusBar, init_statusbar, status, status_success, status_error, status_warn, status_finish


# Global quiet mode flag
_quiet_mode = False

# =============================================================================
# Startup Timing (Debug)
# =============================================================================


class StartupTimer:
    """Collects timing data for startup phases (host-side)."""

    def __init__(self, enabled: bool = False):
        self.enabled = enabled
        self.timings: list[tuple[str, float]] = []
        self.start_time: float = time.perf_counter()
        self._phase_start: float | None = None
        self._phase_name: str | None = None

    def start_phase(self, name: str) -> None:
        """Start timing a phase."""
        if not self.enabled:
            return
        self._phase_name = name
        self._phase_start = time.perf_counter()

    def end_phase(self) -> None:
        """End timing the current phase."""
        if not self.enabled or self._phase_start is None:
            return
        elapsed = (time.perf_counter() - self._phase_start) * 1000  # ms
        self.timings.append((self._phase_name, elapsed))
        self._phase_name = None
        self._phase_start = None

    def phase(self, name: str):
        """Context manager for timing a phase."""
        timer = self
        phase_name = name

        class PhaseContext:
            def __enter__(self):
                timer.start_phase(phase_name)
                return self

            def __exit__(self, *args):
                timer.end_phase()

        return PhaseContext()

    def print_summary(self) -> None:
        """Print timing summary."""
        if not self.enabled or not self.timings:
            return

        total_time = (time.perf_counter() - self.start_time) * 1000

        print("\n" + "=" * 60)
        print("HOST-SIDE STARTUP TIMING SUMMARY")
        print("=" * 60)
        print(f"{'Phase':<35} {'Time (ms)':>10} {'%':>6}")
        print("-" * 60)

        for name, elapsed in self.timings:
            pct = (elapsed / total_time) * 100 if total_time > 0 else 0
            bar = "█" * int(pct / 5)  # Simple bar graph
            print(f"{name:<35} {elapsed:>10.1f} {pct:>5.1f}% {bar}")

        print("-" * 60)
        print(f"{'TOTAL':<35} {total_time:>10.1f}")
        print("=" * 60 + "\n")


# Global timer instance (disabled by default, enabled via --time flag)
_host_timer = StartupTimer(enabled=False)


# Default log directory for container logs
CONTAINER_LOGS_DIR = Path.home() / ".jib-sharing" / "container-logs"


class Colors:
    """ANSI color codes for terminal output"""
    BLUE = '\033[0;34m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    RED = '\033[0;31m'
    BOLD = '\033[1m'
    NC = '\033[0m'


def info(msg: str) -> None:
    """Show info message. In quiet mode, updates statusbar instead."""
    if _quiet_mode:
        status(msg)
    else:
        print(f"{Colors.BLUE}[INFO]{Colors.NC} {msg}")


def success(msg: str) -> None:
    """Show success message. In quiet mode, shows as persistent success."""
    if _quiet_mode:
        status_success(msg)
    else:
        print(f"{Colors.GREEN}[SUCCESS]{Colors.NC} {msg}")


def warn(msg: str) -> None:
    """Show warning message. Always visible."""
    if _quiet_mode:
        status_warn(msg)
    else:
        print(f"{Colors.YELLOW}[WARNING]{Colors.NC} {msg}")


def error(msg: str) -> None:
    """Show error message. Always visible."""
    if _quiet_mode:
        status_error(msg)
    else:
        print(f"{Colors.RED}[ERROR]{Colors.NC} {msg}", file=sys.stderr)


def get_anthropic_api_key() -> Optional[str]:
    """
    Get Anthropic API key from environment or config file.

    Returns:
        API key string if found, None otherwise.
    """
    # Check environment variable first
    if os.environ.get("ANTHROPIC_API_KEY"):
        return os.environ["ANTHROPIC_API_KEY"]
    
    # Check config file
    api_key_file = Config.USER_CONFIG_DIR / "anthropic-api-key"
    if api_key_file.exists():
        return api_key_file.read_text().strip()
    
    return None


def get_google_api_key() -> Optional[str]:
    """
    Get Google API key from environment or config file.

    Returns:
        API key string if found, None otherwise.
    """
    # Check environment variables (support both names)
    if os.environ.get("GOOGLE_API_KEY"):
        return os.environ["GOOGLE_API_KEY"]
    if os.environ.get("GEMINI_API_KEY"):
        return os.environ["GEMINI_API_KEY"]
    
    # Check config file
    api_key_file = Config.USER_CONFIG_DIR / "google-api-key"
    if api_key_file.exists():
        return api_key_file.read_text().strip()
    
    return None


def get_openai_api_key() -> Optional[str]:
    """
    Get OpenAI API key from environment or config file.

    Returns:
        API key string if found, None otherwise.
    """
    if os.environ.get("OPENAI_API_KEY"):
        return os.environ["OPENAI_API_KEY"]
    
    # Check config file
    api_key_file = Config.USER_CONFIG_DIR / "openai-api-key"
    if api_key_file.exists():
        return api_key_file.read_text().strip()
    
    return None


def get_llm_provider() -> str:
    """
    Get the LLM provider preference from environment or config file.

    Returns:
        Provider name: 'anthropic', 'openai', or 'google'
    """
    # Check environment variable
    provider = os.environ.get("LLM_PROVIDER", "").lower()
    if provider in ("anthropic", "openai", "google", "gemini"):
        return "google" if provider == "gemini" else provider
    
    # Check config file
    provider_file = Config.USER_CONFIG_DIR / "llm-provider"
    if provider_file.exists():
        provider = provider_file.read_text().strip().lower()
        if provider in ("anthropic", "openai", "google", "gemini"):
            return "google" if provider == "gemini" else provider
    
    # Default to anthropic
    return "anthropic"


class Config:
    """Configuration paths and constants"""
    CONFIG_DIR = Path.home() / ".jib"  # Docker staging directory
    CONFIG_FILE = CONFIG_DIR / "mounts.conf"
    DOCKERFILE = CONFIG_DIR / "Dockerfile"
    USER_CONFIG_DIR = Path.home() / ".config" / "jib"  # User config (secrets, preferences)
    REPOS_CONFIG_FILE = USER_CONFIG_DIR / "repositories.yaml"
    GITHUB_TOKEN_FILE = USER_CONFIG_DIR / "github-token"
    IMAGE_NAME = "james-in-a-box"
    CONTAINER_NAME = "jib"
    # Persistent directory for all shared data
    SHARING_DIR = Path.home() / ".jib-sharing"
    TMP_DIR = SHARING_DIR / "tmp"      # Persistent tmp workspace
    # Worktree base directory (ephemeral workspaces per container)
    WORKTREE_BASE = Path.home() / ".jib-worktrees"

    # Note: Each container gets its own worktree to isolate changes
    # Host repos stay clean while containers work independently

    # Directories that are dangerous to mount (contain credentials)
    DANGEROUS_DIRS = [
        Path.home() / ".ssh",
        Path.home() / ".config" / "gcloud",
        Path.home() / ".gitconfig",
        Path.home() / ".netrc",
        Path.home() / ".aws",
        Path.home() / ".kube",
        Path.home() / ".gnupg",
        Path.home() / ".docker",
    ]


# Gateway container constants (containerized gateway sidecar)
GATEWAY_CONTAINER_NAME = "jib-gateway"
GATEWAY_IMAGE_NAME = "jib-gateway"
JIB_NETWORK_NAME = "jib-network"
GATEWAY_PORT = 9847


# Import shared config module for get_local_repos
# This ensures jib and gateway-sidecar use identical config parsing
try:
    from jib_config import get_local_repos
except ImportError:
    # Fallback if shared module not available (shouldn't happen in normal use)
    def get_local_repos() -> List[Path]:
        """Fallback: Load local repository paths from configuration."""
        if not Config.REPOS_CONFIG_FILE.exists():
            return []
        try:
            import yaml
            with open(Config.REPOS_CONFIG_FILE) as f:
                config = yaml.safe_load(f) or {}
            local_repos_config = config.get("local_repos", {})
            paths = local_repos_config.get("paths", []) if isinstance(local_repos_config, dict) else []
            result = []
            for path_str in paths:
                path = Path(path_str).expanduser().resolve()
                if path.exists() and path.is_dir():
                    result.append(path)
            return result
        except Exception:
            return []


def get_platform() -> str:
    """Detect platform: linux or macos"""
    import platform
    system = platform.system().lower()
    if system == "linux":
        return "linux"
    elif system == "darwin":
        return "macos"
    return "unknown"


def get_github_token() -> Optional[str]:
    """Get GitHub PAT using the unified HostConfig system.

    Uses HostConfig to load the token from (in order of precedence):
    - Environment variable GITHUB_TOKEN (highest priority)
    - ~/.config/jib/secrets.env (GITHUB_TOKEN=...)
    - ~/.config/jib/github-token (dedicated file)

    This follows the same configuration pattern as other jib secrets
    (Slack tokens, Confluence tokens, etc.) via config/host_config.py.

    Returns:
        Token string if found and valid, None otherwise
    """
    try:
        # Import HostConfig from project root
        script_dir = Path(__file__).resolve().parent
        project_root = script_dir.parent
        if str(project_root) not in sys.path:
            sys.path.insert(0, str(project_root))

        from config.host_config import HostConfig
        config = HostConfig()
        token = config.github_token

        if token and token.startswith(("ghp_", "github_pat_")):
            return token
    except ImportError as e:
        warn(f"Could not import HostConfig: {e}")
    except Exception as e:
        warn(f"Error loading GitHub token from config: {e}")
    return None


def get_github_readonly_token() -> Optional[str]:
    """Get read-only GitHub token for external repositories.

    This token is used for repos outside the primary GitHub App's scope,
    such as Khan/webapp when the App is only installed on jwbron/james-in-a-box.

    Returns:
        Token string if found, None otherwise
    """
    try:
        script_dir = Path(__file__).resolve().parent
        project_root = script_dir.parent
        if str(project_root) not in sys.path:
            sys.path.insert(0, str(project_root))

        from config.host_config import HostConfig
        config = HostConfig()
        # Note: github_readonly_token falls back to github_token if not set
        token = config.get_secret("GITHUB_READONLY_TOKEN")
        if token:
            return token
    except ImportError:
        pass
    except Exception:
        pass
    return None


def get_github_app_token() -> Optional[str]:
    """Generate GitHub App installation token for container use.

    Uses the github-app-token.py script to generate a fresh installation token
    from App credentials (App ID, Installation ID, private key).

    Returns:
        Installation token string if successful, None otherwise
    """
    # Check if App credentials exist
    app_id_file = Config.USER_CONFIG_DIR / "github-app-id"
    installation_id_file = Config.USER_CONFIG_DIR / "github-app-installation-id"
    private_key_file = Config.USER_CONFIG_DIR / "github-app.pem"

    if not all(f.exists() for f in [app_id_file, installation_id_file, private_key_file]):
        return None  # App not configured, fall back to PAT

    # Find the token generation script
    script_dir = Path(__file__).resolve().parent
    token_script = script_dir / "jib-tools" / "github-app-token.py"

    if not token_script.exists():
        warn(f"GitHub App token script not found: {token_script}")
        return None

    # Use the host-services venv Python which has cryptography installed
    # Fall back to system python3 if venv doesn't exist
    jib_root = script_dir.parent
    venv_python = jib_root / "host-services" / ".venv" / "bin" / "python"
    python_cmd = str(venv_python) if venv_python.exists() else "python3"

    try:
        result = subprocess.run(
            [python_cmd, str(token_script), "--config-dir", str(Config.USER_CONFIG_DIR)],
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            token = result.stdout.strip()
            if token and token.startswith("ghs_"):  # Installation tokens start with ghs_
                return token
            elif token:
                # Token format might vary, accept if non-empty
                return token

        # Log error but don't fail - we can fall back to PAT
        if result.stderr:
            warn(f"GitHub App token generation failed: {result.stderr.strip()}")

    except subprocess.TimeoutExpired:
        warn("GitHub App token generation timed out")
    except Exception as e:
        warn(f"GitHub App token generation error: {e}")

    return None


def write_github_token_file(token: str) -> bool:
    """Write GitHub token to the shared file for container consumption.

    The token file is written to ~/.jib-sharing/.github-token and contains
    JSON with the token and metadata. This allows long-running containers
    to read fresh tokens even after the initial env var becomes stale.

    The github-token-refresher service will continuously update this file,
    but we write an initial version here so containers have a valid token
    immediately at startup.

    Args:
        token: The GitHub token to write

    Returns:
        True if successful, False otherwise
    """
    import json
    from datetime import datetime, timezone

    token_file = Config.SHARING_DIR / ".github-token"
    validity_seconds = 60 * 60  # 1 hour

    # Ensure sharing directory exists
    Config.SHARING_DIR.mkdir(parents=True, exist_ok=True)

    now = datetime.now(timezone.utc)
    expires_at = now.timestamp() + validity_seconds

    data = {
        "token": token,
        "generated_at": now.isoformat(),
        "expires_at_unix": expires_at,
        "expires_at": datetime.fromtimestamp(expires_at, timezone.utc).isoformat(),
        "generated_by": "jib-launcher",
        "validity_seconds": validity_seconds
    }

    try:
        # Write atomically using temp file
        temp_file = token_file.with_suffix(".tmp")
        temp_file.write_text(json.dumps(data, indent=2) + "\n")
        temp_file.chmod(0o600)  # Restrict permissions
        temp_file.rename(token_file)
        return True
    except Exception as e:
        warn(f"Failed to write token file: {e}")
        return False


def check_docker_permissions() -> bool:
    """Check if user has permission to run Docker commands"""
    result = subprocess.run(
        ["docker", "ps"],
        capture_output=True,
        text=True
    )

    if result.returncode == 0:
        return True

    if "permission denied" in result.stderr.lower():
        error("Docker permission denied - you are not in the docker group")
        print()
        print("This usually means one of two things:")
        print("  1. You just installed Docker and need to log out/in for group membership")
        print("  2. You need to be added to the docker group")
        print()
        print("Solutions:")
        print()
        print("Option 1: Add yourself to docker group and re-login")
        print("  sudo usermod -aG docker $USER")
        print("  then LOG OUT and LOG BACK IN")
        print()
        print("Option 2: Run with sudo (temporary workaround)")
        print("  sudo $(which jib)")
        print()
        return False

    return False


def get_setup_script_path() -> Optional[Path]:
    """Find the setup.py script relative to the jib launcher location"""
    # Try to find setup.py relative to the jib script
    jib_script = Path(__file__).resolve()

    # jib is at jib-container/jib, setup.py is at repo root
    repo_root = jib_script.parent.parent
    setup_script = repo_root / "setup.py"

    if setup_script.exists():
        return setup_script

    # Fallback: check ~/khan/james-in-a-box/setup.py
    fallback = Path.home() / "khan" / "james-in-a-box" / "setup.py"
    if fallback.exists():
        return fallback

    return None


def run_setup_script() -> bool:
    """Run the setup.py script to configure jib"""
    setup_script = get_setup_script_path()

    if not setup_script:
        error("Could not find setup.py script")
        print()
        print("Please run setup manually:")
        print("  cd ~/khan/james-in-a-box")
        print("  ./setup.py")
        return False

    info(f"Running setup: {setup_script}")
    print()

    try:
        # Run setup.py in its directory
        result = subprocess.run(
            [sys.executable, str(setup_script)],
            cwd=setup_script.parent
        )
        return result.returncode == 0
    except Exception as e:
        error(f"Failed to run setup.py: {e}")
        return False


def check_host_setup() -> bool:
    """Check if host setup is complete (services installed, directories exist)"""
    # Systemd services that should be installed (gateway can be containerized instead)
    slack_services = [
        "slack-notifier.service",
        "slack-receiver.service",
    ]

    # Important directories that should exist
    critical_dirs = [
        Path.home() / ".jib-sharing" / "notifications",
        Path.home() / ".jib-sharing" / "incoming",
        Path.home() / ".jib-sharing" / "responses",
    ]

    # Configuration file
    config_file = Path.home() / ".config" / "jib-notifier" / "config.json"

    issues_found = []

    # Check if Slack services are installed
    for service in slack_services:
        result = subprocess.run(
            ["systemctl", "--user", "list-unit-files", service],
            capture_output=True,
            text=True
        )
        if result.returncode != 0 or service not in result.stdout:
            issues_found.append(f"Service not installed: {service}")

    # Check gateway setup: either systemd service OR containerized gateway (secret file)
    # The containerized gateway is started on-demand by jib, so we just need the secret
    gateway_secret = Config.USER_CONFIG_DIR / "gateway-secret"
    gateway_systemd_result = subprocess.run(
        ["systemctl", "--user", "list-unit-files", "gateway-sidecar.service"],
        capture_output=True,
        text=True
    )
    gateway_systemd_ok = gateway_systemd_result.returncode == 0 and "gateway-sidecar.service" in gateway_systemd_result.stdout
    gateway_container_ok = gateway_secret.exists()

    if not gateway_systemd_ok and not gateway_container_ok:
        issues_found.append("Gateway not configured: run host-services/gateway-sidecar/setup.sh")

    # Check if critical directories exist
    for dir_path in critical_dirs:
        if not dir_path.exists():
            issues_found.append(f"Directory not found: {dir_path}")

    # Check if config exists (warning only, not critical)
    config_warning = None
    if not config_file.exists():
        config_warning = f"Configuration file not found: {config_file}"

    # If critical issues found, automatically run setup
    if issues_found:
        warn("Host setup appears incomplete")
        print()

        error("Critical issues found:")
        for issue in issues_found:
            print(f"  ✗ {issue}")
        print()

        if config_warning:
            warn(config_warning)
            print()

        print("JIB requires host services to be installed for full functionality:")
        print("  • Slack integration (notifier and receiver)")
        print("  • Gateway sidecar (git/gh policy enforcement)")
        print("  • Shared directories for notifications and task communication")
        print()

        # Auto-run setup.py when config is missing
        info("Running setup.py to configure jib...")
        print()
        if run_setup_script():
            success("Setup completed!")
            return True
        else:
            error("Setup failed")
            return False

    # Config warning only (not critical) - just warn and continue
    if config_warning:
        warn(config_warning)
        print()

    return True


def check_docker() -> bool:
    """Check if Docker is installed and offer to install if not"""
    platform_name = get_platform()

    if subprocess.run(["which", "docker"], capture_output=True).returncode != 0:
        error("Docker is not installed.")

        if platform_name == "macos":
            # TODO: Add macOS Docker Desktop installation
            info("On macOS, please install Docker Desktop from:")
            info("  https://www.docker.com/products/docker-desktop")
            return False

        # Linux installation
        response = input("Install Docker now? (yes/no): ").strip().lower()
        if response == "yes":
            info("Installing Docker...")
            try:
                # Download installer
                subprocess.run(
                    ["curl", "-fsSL", "https://get.docker.com", "-o", "/tmp/get-docker.sh"],
                    check=True
                )
                # Run installer
                subprocess.run(["sudo", "sh", "/tmp/get-docker.sh"], check=True)
                # Add user to docker group
                subprocess.run(["sudo", "usermod", "-aG", "docker", os.environ["USER"]], check=True)
                # Cleanup
                os.remove("/tmp/get-docker.sh")

                success("Docker installed successfully!")
                print()
                warn("IMPORTANT: You need to log out and back in for group membership to take effect.")
                print("After logging back in, run this script again.")
                sys.exit(0)
            except Exception as e:
                error(f"Docker installation failed: {e}")
                return False
        else:
            error("Docker is required")
            return False

    # Check Docker daemon is running and we have permissions
    return check_docker_permissions()



def _copy_directory_atomic(src: Path, dest: Path, name: str, quiet: bool = False) -> bool:
    """Copy a directory atomically with retry logic for race conditions.

    When multiple jib --exec instances run simultaneously, they may all try to
    update the same build context directories. This function uses atomic operations
    to handle race conditions:
    1. Copy to a temporary directory
    2. Remove existing destination (with retry on ENOTEMPTY/ENOENT)
    3. Rename temp to destination (atomic on same filesystem)

    Args:
        src: Source directory to copy
        dest: Destination path
        name: Human-readable name for logging
        quiet: If True, suppress info messages

    Returns:
        True if successful, False otherwise
    """
    import shutil
    import tempfile
    import uuid

    max_retries = 3
    retry_delay = 0.1  # seconds

    for attempt in range(max_retries):
        try:
            # Create a unique temp directory in the same parent (for atomic rename)
            temp_dir = dest.parent / f".tmp-{uuid.uuid4().hex[:8]}"

            # Copy source to temp location
            shutil.copytree(src, temp_dir)

            # Try to remove existing destination
            if dest.exists():
                try:
                    shutil.rmtree(dest)
                except FileNotFoundError:
                    # Another process already removed it - that's fine
                    pass
                except OSError as e:
                    # Directory not empty (ENOTEMPTY) - another process is writing
                    # Clean up temp and retry
                    shutil.rmtree(temp_dir, ignore_errors=True)
                    if attempt < max_retries - 1:
                        time.sleep(retry_delay * (attempt + 1))
                        continue
                    raise

            # Atomic rename from temp to destination
            try:
                temp_dir.rename(dest)
            except OSError:
                # Destination appeared between rmtree and rename - another process won
                # Clean up our temp and use their copy
                shutil.rmtree(temp_dir, ignore_errors=True)
                if dest.exists():
                    # Other process succeeded, we're done
                    if not quiet:
                        info(f"{name} directory ready (from another process)")
                    return True
                # Neither exists - retry
                if attempt < max_retries - 1:
                    time.sleep(retry_delay * (attempt + 1))
                    continue
                raise

            if not quiet:
                info(f"{name} copied to build context")
            return True

        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(retry_delay * (attempt + 1))
                continue
            warn(f"Failed to copy {name} directory after {max_retries} attempts: {e}")
            # Clean up temp if it exists
            if 'temp_dir' in locals() and temp_dir.exists():
                shutil.rmtree(temp_dir, ignore_errors=True)
            return False

    return False


def is_dangerous_dir(path: Path) -> bool:
    """Check if a directory is dangerous to mount (contains credentials)"""
    for dangerous in Config.DANGEROUS_DIRS:
        try:
            # Check if path is dangerous or contains dangerous
            if path.resolve() == dangerous.resolve():
                return True
            if path.resolve() in dangerous.resolve().parents:
                return True
            if dangerous.resolve() in path.resolve().parents:
                return True
        except Exception:
            pass
    return False


def create_dockerfile() -> None:
    """Create the Dockerfile for the container"""
    import shutil

    # Resolve symlinks to find the actual project directory
    script_dir = Path(__file__).resolve().parent

    # Copy docker-setup.py to config directory
    setup_script = script_dir / "docker-setup.py"
    setup_dest = Config.CONFIG_DIR / "docker-setup.py"

    if setup_script.exists():
        shutil.copy(setup_script, setup_dest)
        setup_dest.chmod(0o755)
    else:
        warn("docker-setup.py not found, skipping dev tools installation")

    # Copy claude-commands directory to config directory
    # Use atomic copy with retry to handle race conditions when multiple
    # jib --exec instances run simultaneously
    commands_src = script_dir / "claude-commands"
    commands_dest = Config.CONFIG_DIR / "claude-commands"
    if commands_src.exists():
        _copy_directory_atomic(commands_src, commands_dest, "Claude commands", _quiet_mode)
    else:
        warn("claude-commands directory not found")

    # Copy claude-rules directory to build context
    # Use atomic copy with retry to handle race conditions
    rules_src = script_dir / "claude-rules"
    rules_dest = Config.CONFIG_DIR / "claude-rules"
    if rules_src.exists():
        _copy_directory_atomic(rules_src, rules_dest, "Claude rules", _quiet_mode)
    else:
        warn("claude-rules directory not found, skipping agent rules")

    # Copy .claude/hooks directory to build context
    # Use atomic copy with retry to handle race conditions
    hooks_src = script_dir / ".claude" / "hooks"
    hooks_dest = Config.CONFIG_DIR / ".claude" / "hooks"
    if hooks_src.exists():
        # Ensure parent directory exists
        hooks_dest.parent.mkdir(parents=True, exist_ok=True)
        _copy_directory_atomic(hooks_src, hooks_dest, "Claude hooks", _quiet_mode)
    else:
        warn(".claude/hooks directory not found, skipping hooks")

    # Copy jib-runtime directories to build context
    # These provide container-resident executables and processors
    # The bin/ directory contains symlinks to executables (added to PATH in container)
    #
    # Directory structure must match the host layout so path calculations work:
    # Host:      james-in-a-box/jib-container/jib-tasks/... + james-in-a-box/shared/
    # Container: /opt/jib-runtime/jib-container/jib-tasks/... + /opt/jib-runtime/shared/
    #
    # The processors use: Path(__file__).parents[3] / "shared"
    # From jib-container/jib-tasks/analysis/*.py, this goes up 3 levels to repo root
    jib_container_dest = Config.CONFIG_DIR / "jib-container"
    jib_container_dest.mkdir(parents=True, exist_ok=True)

    runtime_dirs = ["bin", "llm", "jib-tasks", "jib-tools", "scripts"]
    for dir_name in runtime_dirs:
        src = script_dir / dir_name
        dest = jib_container_dest / dir_name
        if src.exists():
            _copy_directory_atomic(src, dest, f"Runtime {dir_name}", _quiet_mode)
        else:
            warn(f"{dir_name} directory not found, skipping")

    # Copy shared directory from repo root to build context (sibling of jib-container)
    # The jib-tasks processors import shared modules (e.g., jib_logging, notifications)
    # Note: llm module is in jib-container/llm/ (copied via runtime_dirs above)
    repo_root = script_dir.parent  # jib-container's parent is james-in-a-box
    shared_src = repo_root / "shared"
    shared_dest = Config.CONFIG_DIR / "shared"
    if shared_src.exists():
        _copy_directory_atomic(shared_src, shared_dest, "Shared modules", _quiet_mode)
    else:
        warn("shared directory not found, container processors may fail imports")

    # Copy pyproject.toml files for pip-installable packages
    # These make claude (from jib-container) and shared modules pip-installable
    pyproject_files = [
        (script_dir / "pyproject.toml", jib_container_dest / "pyproject.toml"),
        (shared_src / "pyproject.toml", shared_dest / "pyproject.toml"),
    ]
    for src, dest in pyproject_files:
        if src.exists():
            shutil.copy(src, dest)
        else:
            warn(f"pyproject.toml not found at {src}")

    # Note: Claude credentials are mounted at runtime (not copied at build time)
    # This ensures the container always uses the host's CURRENT credentials
    # Avoids issues with stale/revoked OAuth tokens from previous builds
    if not _quiet_mode:
        info("Claude credentials will be mounted from host at runtime (see setup output above)")

    # Copy entrypoint.py from script directory
    entrypoint_src = script_dir / "entrypoint.py"
    entrypoint_dest = Config.CONFIG_DIR / "entrypoint.py"
    if entrypoint_src.exists():
        shutil.copy(entrypoint_src, entrypoint_dest)
        entrypoint_dest.chmod(0o755)
    else:
        error(f"entrypoint.py not found at {entrypoint_src}")
        error("Cannot build without entrypoint script")

    # Copy Dockerfile from script directory
    dockerfile_src = script_dir / "Dockerfile"
    if dockerfile_src.exists():
        shutil.copy(dockerfile_src, Config.DOCKERFILE)
        if not _quiet_mode:
            success("Build context prepared")
    else:
        error(f"Dockerfile not found at {dockerfile_src}")
        error("Cannot build without Dockerfile")


def get_installed_claude_version() -> str | None:
    """Get the Claude Code version installed in the current image.

    Returns:
        Version string (e.g., "2.1.7") or None if not available
    """
    if not image_exists():
        return None

    try:
        result = subprocess.run(
            ["docker", "run", "--rm", "--entrypoint", "cat",
             Config.IMAGE_NAME, "/opt/claude/VERSION"],
            capture_output=True, text=True, timeout=30
        )
        if result.returncode == 0 and result.stdout.strip():
            # Version output is like "claude 2.1.7" - extract just the number
            version_line = result.stdout.strip()
            parts = version_line.split()
            return parts[-1] if parts else None
        return None
    except Exception:
        return None


def get_latest_claude_version() -> str | None:
    """Get the latest Claude Code version from npm registry.

    Returns:
        Version string (e.g., "2.1.17") or None if check fails
    """
    import json
    import urllib.request

    try:
        url = "https://registry.npmjs.org/@anthropic-ai/claude-code/latest"
        with urllib.request.urlopen(url, timeout=10) as response:
            data = json.loads(response.read().decode())
            return data.get("version")
    except Exception:
        return None


def check_claude_update() -> str | None:
    """Check if a Claude Code update is available.

    Returns:
        The new version string if update available, None otherwise
    """
    installed = get_installed_claude_version()
    latest = get_latest_claude_version()

    if not latest:
        # Can't check, don't force update
        return None

    if not installed:
        # No version installed, use latest
        return latest

    # Compare versions
    if installed != latest:
        if not _quiet_mode:
            info(f"Claude Code update available: {installed} → {latest}")
        return latest

    return None


def build_image() -> bool:
    """Build the Docker image (Docker's cache makes this fast when nothing changed)"""
    # Always sync files to build context - Docker detects changes and rebuilds only what's needed
    create_dockerfile()

    # Check for Claude Code updates
    claude_version = check_claude_update()

    try:
        cmd = [
            "docker", "build",
            "--build-arg", f"USER_NAME={os.environ['USER']}",
            "--build-arg", f"USER_UID={os.getuid()}",
            "--build-arg", f"USER_GID={os.getgid()}",
            "-t", Config.IMAGE_NAME,
            "-f", str(Config.DOCKERFILE),
            str(Config.CONFIG_DIR)
        ]

        # Pass Claude version to bust cache if update available
        if claude_version:
            cmd.insert(2, f"--build-arg")
            cmd.insert(3, f"CLAUDE_CODE_VERSION={claude_version}")

        # In quiet mode, suppress Docker build output
        if _quiet_mode:
            cmd.insert(2, "--quiet")
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                # Show error output if build failed
                error("Docker build failed")
                if result.stderr:
                    print(result.stderr, file=sys.stderr)
                return False
        else:
            # Docker automatically uses cache for unchanged layers
            subprocess.run(cmd, check=True)
        return True
    except subprocess.CalledProcessError:
        error("Docker build failed")
        return False


def image_exists() -> bool:
    """Check if Docker image exists"""
    return subprocess.run(
        ["docker", "image", "inspect", Config.IMAGE_NAME],
        capture_output=True
    ).returncode == 0


def ensure_jib_network() -> bool:
    """Create jib-network Docker network if it doesn't exist.

    Returns:
        True if network exists or was created, False on failure
    """
    # Check if network exists
    result = subprocess.run(
        ["docker", "network", "inspect", JIB_NETWORK_NAME],
        capture_output=True
    )
    if result.returncode == 0:
        return True

    # Create the network
    result = subprocess.run(
        ["docker", "network", "create", JIB_NETWORK_NAME],
        capture_output=True,
        text=True
    )
    if result.returncode == 0:
        info(f"Created Docker network: {JIB_NETWORK_NAME}")
        return True

    error(f"Failed to create Docker network: {result.stderr}")
    return False


def is_gateway_running() -> bool:
    """Check if the gateway container is running.

    Returns:
        True if gateway container is running, False otherwise
    """
    result = subprocess.run(
        ["docker", "container", "inspect", "-f", "{{.State.Running}}", GATEWAY_CONTAINER_NAME],
        capture_output=True,
        text=True
    )
    return result.returncode == 0 and result.stdout.strip() == "true"


def gateway_image_exists() -> bool:
    """Check if gateway Docker image exists."""
    return subprocess.run(
        ["docker", "image", "inspect", GATEWAY_IMAGE_NAME],
        capture_output=True
    ).returncode == 0


def build_gateway_image() -> bool:
    """Build the gateway sidecar Docker image.

    Builds from the repo root using the Dockerfile at
    host-services/gateway-sidecar/Dockerfile.

    Returns:
        True if build succeeded, False otherwise
    """
    # Find repo root (parent of jib-container directory)
    script_dir = Path(__file__).resolve().parent
    repo_root = script_dir.parent

    dockerfile_path = repo_root / "host-services" / "gateway-sidecar" / "Dockerfile"
    if not dockerfile_path.exists():
        error(f"Gateway Dockerfile not found at {dockerfile_path}")
        return False

    info("Building gateway sidecar image...")
    result = subprocess.run(
        [
            "docker", "build",
            "-t", GATEWAY_IMAGE_NAME,
            "-f", str(dockerfile_path),
            str(repo_root)
        ],
        capture_output=True,
        text=True
    )

    if result.returncode == 0:
        success("Gateway image built successfully")
        return True

    error(f"Gateway image build failed: {result.stderr}")
    return False


def wait_for_gateway_health(timeout: int = 30) -> bool:
    """Wait for the gateway to become healthy.

    Polls the health endpoint until it responds or timeout is reached.

    Args:
        timeout: Maximum seconds to wait for health

    Returns:
        True if gateway is healthy, False on timeout
    """
    import urllib.request
    import urllib.error

    # Use container name for health check since we're on the same network
    # But during startup from host, we need to use localhost or check via docker exec
    health_url = f"http://localhost:{GATEWAY_PORT}/api/v1/health"

    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            with urllib.request.urlopen(health_url, timeout=2) as response:
                if response.status == 200:
                    return True
        except (urllib.error.URLError, OSError):
            pass  # Gateway not ready yet
        time.sleep(0.5)

    return False


def start_gateway_container() -> bool:
    """Ensure the gateway sidecar is available.

    The gateway is managed by systemd (gateway-sidecar.service). This function
    checks if it's running and healthy. If not, it tells the user how to start it.

    Returns:
        True if gateway is healthy, False otherwise
    """
    # Check if gateway is healthy
    if wait_for_gateway_health(timeout=5):
        return True

    # Gateway not available - check systemd service status
    service_result = subprocess.run(
        ["systemctl", "--user", "is-active", "gateway-sidecar.service"],
        capture_output=True,
        text=True
    )

    if service_result.returncode != 0:
        error("Gateway sidecar service is not running")
        error("")
        error("To start the gateway:")
        error("  systemctl --user start gateway-sidecar.service")
        error("")
        error("To set up the gateway (if not installed):")
        error("  ./gateway-sidecar/setup.sh")
        return False

    # Service is active but not healthy - check logs
    error("Gateway sidecar service is running but not healthy")
    error("")
    error("Check service logs:")
    error("  journalctl --user -u gateway-sidecar.service -f")
    error("")
    error("Try restarting the service:")
    error("  systemctl --user restart gateway-sidecar.service")

    return False


def generate_container_id() -> str:
    """Generate unique container ID based on timestamp and process ID"""
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    pid = os.getpid()
    return f"jib-{timestamp}-{pid}"


def get_docker_log_config(container_id: str, task_id: Optional[str] = None) -> list:
    """Generate Docker logging configuration arguments.

    Uses the json-file logging driver with:
    - Log rotation (max 10MB per file, 5 files)
    - Correlation labels for easy searching
    - Timestamps included

    Args:
        container_id: Unique container identifier
        task_id: Optional task ID for correlation (e.g., "task-20251129-222239")

    Returns:
        List of Docker command arguments for logging configuration
    """
    # Ensure container logs directory exists
    CONTAINER_LOGS_DIR.mkdir(parents=True, exist_ok=True)

    # Log file path based on container ID
    log_file = CONTAINER_LOGS_DIR / f"{container_id}.log"

    log_args = [
        "--log-driver", "json-file",
        "--log-opt", "max-size=10m",
        "--log-opt", "max-file=5",
        # Add labels for correlation - these appear in docker inspect
        "--label", f"jib.container_id={container_id}",
    ]

    if task_id:
        log_args.extend(["--label", f"jib.task_id={task_id}"])

    return log_args


def extract_task_id_from_command(command: List[str]) -> Optional[str]:
    """Extract task ID from the command if it's processing a task file.

    Looks for patterns like:
    - incoming-processor.py /path/to/task-20251129-222239.md
    - task-20251129-222239

    Args:
        command: Command list passed to jib --exec

    Returns:
        Task ID if found, None otherwise
    """
    import re
    task_pattern = r"(task-\d{8}-\d{6})"

    for arg in command:
        match = re.search(task_pattern, arg)
        if match:
            return match.group(1)

    return None


def extract_thread_ts_from_task_file(task_file_path: str) -> Optional[str]:
    """Extract thread_ts from a task file's YAML frontmatter.

    Task files contain YAML frontmatter like:
        ---
        task_id: "task-20251129-222239"
        thread_ts: "1764483758.159619"
        ---

    Args:
        task_file_path: Path to the task file (may be container path)

    Returns:
        Thread timestamp if found, None otherwise
    """
    import re

    # Convert container path back to host path
    host_path = task_file_path
    if "/sharing/" in task_file_path:
        # Container path like /home/user/sharing/incoming/task.md
        # -> Host path like ~/.jib-sharing/incoming/task.md
        parts = task_file_path.split("/sharing/", 1)
        if len(parts) == 2:
            host_path = str(Path.home() / ".jib-sharing" / parts[1])

    try:
        path = Path(host_path)
        if path.exists():
            content = path.read_text()
            # Look for thread_ts in YAML frontmatter
            match = re.search(r'thread_ts:\s*["\']?(\d+\.\d+)["\']?', content)
            if match:
                return match.group(1)
    except Exception:
        pass

    return None


def update_log_index(
    container_id: str,
    task_id: Optional[str] = None,
    thread_ts: Optional[str] = None,
    log_file: Optional[str] = None,
) -> None:
    """Update the log index file with correlation information.

    The log index enables quick lookups:
    - task_id -> container_id
    - thread_ts -> task_id
    - List of all recent container runs

    Uses file locking to prevent race conditions when multiple containers
    finish simultaneously.

    Args:
        container_id: Docker container ID
        task_id: Optional task ID (e.g., "task-20251129-222239")
        thread_ts: Optional Slack thread timestamp
        log_file: Path to the log file
    """
    import json
    import fcntl

    index_file = CONTAINER_LOGS_DIR / "log-index.json"
    CONTAINER_LOGS_DIR.mkdir(parents=True, exist_ok=True)

    # Use file locking to prevent concurrent modifications
    # Open in 'a+' mode to create file if it doesn't exist
    with open(index_file, 'a+') as f:
        # Acquire exclusive lock
        fcntl.flock(f.fileno(), fcntl.LOCK_EX)
        try:
            # Seek to beginning to read
            f.seek(0)
            content = f.read()

            # Load existing index
            index = {"task_to_container": {}, "thread_to_task": {}, "entries": []}
            if content:
                try:
                    index = json.loads(content)
                except Exception:
                    pass

            # Update correlation maps
            if task_id:
                index["task_to_container"][task_id] = container_id
            if thread_ts and task_id:
                index["thread_to_task"][thread_ts] = task_id

            # Add entry
            entry = {
                "container_id": container_id,
                "task_id": task_id,
                "thread_ts": thread_ts,
                "log_file": str(log_file) if log_file else None,
                "timestamp": datetime.now().isoformat(),
            }
            index["entries"].append(entry)

            # Keep last 1000 entries
            if len(index["entries"]) > 1000:
                index["entries"] = index["entries"][-1000:]

            # Write updated index
            f.seek(0)
            f.truncate()
            f.write(json.dumps(index, indent=2))
        finally:
            # Release lock
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)


def save_container_logs(
    container_id: str,
    task_id: Optional[str] = None,
    thread_ts: Optional[str] = None,
) -> Optional[Path]:
    """Save Docker container logs to persistent storage.

    Uses `docker logs` to capture all container output and saves it to
    ~/.jib-sharing/container-logs/{container_id}.log

    Also creates a symlink from task_id.log -> container_id.log for easy lookup,
    and updates the log index for correlation searches.

    Args:
        container_id: Docker container ID/name
        task_id: Optional task ID for symlink creation
        thread_ts: Optional Slack thread timestamp for index

    Returns:
        Path to the saved log file, or None if saving failed
    """
    CONTAINER_LOGS_DIR.mkdir(parents=True, exist_ok=True)

    log_file = CONTAINER_LOGS_DIR / f"{container_id}.log"

    try:
        # Get container logs using docker logs command
        result = subprocess.run(
            ["docker", "logs", container_id],
            capture_output=True,
            text=True,
            timeout=30
        )

        # Check log size before writing to prevent disk space exhaustion
        max_log_size = 100 * 1024 * 1024  # 100MB
        total_log_size = len(result.stdout) + len(result.stderr)
        if total_log_size > max_log_size:
            warn(f"Container logs exceed {max_log_size / (1024 * 1024):.0f}MB, truncating...")
            # Truncate stderr/stdout proportionally
            if result.stdout:
                result.stdout = result.stdout[:max_log_size // 2] + "\n\n[... truncated ...]\n"
            if result.stderr:
                result.stderr = result.stderr[:max_log_size // 2] + "\n\n[... truncated ...]\n"

        # Write logs (both stdout and stderr)
        with open(log_file, "w") as f:
            f.write(f"=== Container: {container_id} ===\n")
            f.write(f"=== Saved: {datetime.now().isoformat()} ===\n")
            if task_id:
                f.write(f"=== Task ID: {task_id} ===\n")
            if thread_ts:
                f.write(f"=== Thread TS: {thread_ts} ===\n")
            f.write("=" * 50 + "\n\n")

            if result.stdout:
                f.write("=== STDOUT ===\n")
                f.write(result.stdout)
                f.write("\n")

            if result.stderr:
                f.write("\n=== STDERR ===\n")
                f.write(result.stderr)

        # Create symlink from task_id if provided
        if task_id:
            task_log_link = CONTAINER_LOGS_DIR / f"{task_id}.log"
            # Remove existing symlink if present
            if task_log_link.is_symlink():
                task_log_link.unlink()
            # Create relative symlink
            task_log_link.symlink_to(f"{container_id}.log")

        # Update log index for correlation lookups
        update_log_index(container_id, task_id, thread_ts, str(log_file))

        if not _quiet_mode:
            info(f"Container logs saved: {log_file}")
            if task_id:
                info(f"  Symlink: {task_id}.log -> {container_id}.log")

        return log_file

    except subprocess.TimeoutExpired:
        warn("Timed out getting container logs")
    except FileNotFoundError:
        # Container doesn't exist or already removed
        pass
    except Exception as e:
        warn(f"Failed to save container logs: {e}")

    return None


def get_default_branch(repo_path: Path) -> str:
    """Get the default branch (main or master) for a git repository.

    Args:
        repo_path: Path to the git repository

    Returns:
        The default branch name ('main' or 'master'), or 'main' as fallback
    """
    # Try to get the default branch from git config (set by clone)
    result = subprocess.run(
        ["git", "config", "--get", "init.defaultBranch"],
        cwd=repo_path,
        capture_output=True,
        text=True
    )
    if result.returncode == 0 and result.stdout.strip():
        return result.stdout.strip()

    # Check if origin/HEAD exists and points to a branch
    result = subprocess.run(
        ["git", "symbolic-ref", "refs/remotes/origin/HEAD"],
        cwd=repo_path,
        capture_output=True,
        text=True
    )
    if result.returncode == 0 and result.stdout.strip():
        # Format: refs/remotes/origin/main -> main
        return result.stdout.strip().split("/")[-1]

    # Fall back to checking which branches exist
    for branch in ["main", "master"]:
        result = subprocess.run(
            ["git", "rev-parse", "--verify", f"refs/heads/{branch}"],
            cwd=repo_path,
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            return branch

    # Default to 'main' if nothing found
    return "main"


def _acquire_git_lock(repo_path: Path, timeout: float = 30.0) -> Optional[int]:
    """Acquire an exclusive lock for git operations on a repository.

    Git doesn't handle concurrent operations well. When multiple jib --exec
    containers try to create worktrees simultaneously, they can conflict on
    the same repo's config/index files. This function provides file-based
    locking to serialize git operations per repository.

    Args:
        repo_path: Path to the git repository
        timeout: Maximum time to wait for lock in seconds

    Returns:
        File descriptor for the lock (must be closed to release), or None if failed
    """
    import fcntl
    import errno

    # Create lock file in the repo's .git directory
    git_dir = repo_path / ".git"
    if git_dir.is_file():
        # Worktree - read the actual git dir path
        try:
            content = git_dir.read_text().strip()
            if content.startswith("gitdir:"):
                git_dir = Path(content[7:].strip())
        except Exception:
            pass

    if not git_dir.is_dir():
        return None

    lock_file = git_dir / ".jib-worktree-lock"

    start_time = time.time()
    retry_delay = 0.1

    while True:
        try:
            # Open lock file (create if doesn't exist)
            fd = os.open(str(lock_file), os.O_CREAT | os.O_RDWR)

            # Try to acquire exclusive lock (non-blocking)
            fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB)

            # Got the lock
            return fd

        except (OSError, IOError) as e:
            if e.errno in (errno.EAGAIN, errno.EWOULDBLOCK, errno.EACCES):
                # Lock held by another process
                elapsed = time.time() - start_time
                if elapsed >= timeout:
                    warn(f"Timeout waiting for git lock on {repo_path.name}")
                    if 'fd' in locals():
                        os.close(fd)
                    return None

                time.sleep(retry_delay)
                retry_delay = min(retry_delay * 1.5, 1.0)  # Exponential backoff, max 1s
                continue
            else:
                # Other error
                if 'fd' in locals():
                    os.close(fd)
                return None


def _release_git_lock(fd: int) -> None:
    """Release a git lock acquired with _acquire_git_lock."""
    import fcntl

    if fd is not None:
        try:
            fcntl.flock(fd, fcntl.LOCK_UN)
            os.close(fd)
        except Exception:
            pass


def create_worktrees(container_id: str) -> dict:
    """Create git worktrees for all configured local repositories.

    Worktrees are always based on the default branch (main or master),
    regardless of what branch is currently checked out on the host.

    Uses file-based locking to prevent conflicts when multiple jib --exec
    containers try to create worktrees simultaneously.

    Returns:
        Dictionary mapping repo paths to (worktree_path, repo_name) tuples
    """
    worktree_dir = Config.WORKTREE_BASE / container_id
    worktree_dir.mkdir(parents=True, exist_ok=True)

    worktrees = {}

    # Get configured local repositories
    local_repos = get_local_repos()
    if not local_repos:
        if not _quiet_mode:
            info("No local repositories configured. Run ./setup.py to add repositories.")
        return worktrees

    # Iterate through configured repos
    for repo_path in local_repos:
        if not repo_path.is_dir():
            continue

        # Check if it's a git repository with a .git DIRECTORY or file
        git_dir = repo_path / ".git"
        if not git_dir.exists():
            warn(f"  ✗ {repo_path.name} is not a git repository, skipping")
            continue

        repo_name = repo_path.name

        # Note: git worktree add works even when source repo is a worktree
        # It creates a new worktree of the same underlying main repository
        worktree_path = worktree_dir / repo_name

        # Acquire lock to prevent concurrent git operations on same repo
        lock_fd = _acquire_git_lock(repo_path, timeout=60.0)
        if lock_fd is None:
            warn(f"  ✗ Could not acquire lock for {repo_name}, skipping worktree")
            continue

        try:
            # Determine the default branch for this repo
            default_branch = get_default_branch(repo_path)

            if not _quiet_mode:
                info(f"Creating worktree for {repo_name} (from {default_branch})...")

            # Create temporary branch for this container, starting from the default branch
            branch_name = f"jib-temp-{container_id}"

            # Create worktree based on the default branch (not current HEAD)
            result = subprocess.run(
                ["git", "worktree", "add", str(worktree_path), "-b", branch_name, default_branch],
                cwd=repo_path,
                capture_output=True,
                text=True
            )

            if result.returncode == 0:
                # Store both worktree path and original repo path for cleanup
                worktrees[repo_name] = {"worktree": worktree_path, "source": repo_path}
                if not _quiet_mode:
                    success(f"  ✓ {repo_name} -> {worktree_path}")
            else:
                warn(f"  ✗ Failed to create worktree for {repo_name}: {result.stderr}")

        except Exception as e:
            warn(f"  ✗ Error creating worktree for {repo_name}: {e}")
        finally:
            _release_git_lock(lock_fd)

    return worktrees


def cleanup_worktrees(container_id: str) -> None:
    """Clean up worktrees for a container

    Args:
        container_id: Unique container identifier
    """
    import shutil

    worktree_dir = Config.WORKTREE_BASE / container_id

    if not worktree_dir.exists():
        return

    if not _quiet_mode:
        info(f"Cleaning up worktrees for {container_id}...")

    # Get configured local repositories for cleanup
    local_repos = get_local_repos()
    local_repos_by_name = {repo.name: repo for repo in local_repos}

    # Collect repos and their worktree admin directories
    # We need to forcibly remove admin dirs because container modified .git files
    # to point to container-only paths, which breaks git worktree prune
    repos_to_clean = {}  # repo_path -> list of admin dir names
    for worktree_path in worktree_dir.iterdir():
        if worktree_path.is_dir():
            repo_name = worktree_path.name
            original_repo = local_repos_by_name.get(repo_name)
            if original_repo and original_repo.exists():
                # Find the admin directory name from the worktree's .git file
                git_file = worktree_path / ".git"
                if git_file.is_file():
                    try:
                        content = git_file.read_text().strip()
                        # Format: "gitdir: /path/to/.git/worktrees/NAME" or
                        # "gitdir: /path/to/.git-main/repo/worktrees/NAME"
                        if "worktrees/" in content:
                            admin_name = content.split("worktrees/")[-1]
                            if original_repo not in repos_to_clean:
                                repos_to_clean[original_repo] = []
                            repos_to_clean[original_repo].append(admin_name)
                    except Exception:
                        pass

    # Remove container worktree directory first
    try:
        shutil.rmtree(worktree_dir)
        if not _quiet_mode:
            info(f"  ✓ Removed worktree directory")
    except Exception as e:
        warn(f"  ✗ Failed to remove directory {worktree_dir}: {e}")

    # Forcibly remove worktree admin directories
    # Don't rely on git worktree prune - it fails when .git files are corrupted
    admin_dirs_removed = 0
    for repo_path, admin_names in repos_to_clean.items():
        worktrees_dir = repo_path / ".git" / "worktrees"
        if worktrees_dir.is_dir():
            for admin_name in admin_names:
                admin_dir = worktrees_dir / admin_name
                if admin_dir.exists():
                    try:
                        shutil.rmtree(admin_dir)
                        admin_dirs_removed += 1
                    except Exception as e:
                        warn(f"  ✗ Failed to remove admin dir {admin_dir}: {e}")

    # Also run git worktree prune as a fallback for any we missed
    for repo_path in repos_to_clean.keys():
        try:
            subprocess.run(
                ["git", "worktree", "prune", "-v"],
                cwd=repo_path,
                capture_output=True,
                text=True,
                check=False  # Don't fail if prune has issues
            )
        except Exception:
            pass

    if admin_dirs_removed > 0 and not _quiet_mode:
        info(f"  ✓ Removed {admin_dirs_removed} worktree admin dir(s)")
        info(f"  Commits preserved on branch: jib-temp-{container_id}")


def setup() -> bool:
    """Interactive setup process"""
    print()
    info("=== Autonomous Software Engineering Agent - Setup ===")
    print()
    print("🤖 AUTONOMOUS ENGINEERING AGENT")
    print()
    print("This sets up a sandboxed environment for Claude to work as an autonomous")
    print("software engineer with minimal supervision.")
    print()
    print("OPERATING MODEL:")
    print("  • Agent: Plans, implements, tests, documents, creates PRs")
    print("  • Human: Reviews, approves, deploys")
    print()
    print("AGENT CAPABILITIES:")
    print("  ✓ Edit code and create commits in ~/repos/")
    print("  ✓ Run tests, linters, development servers")
    print("  ✓ Access Confluence docs (ADRs, runbooks, best practices)")
    print("  ✓ Create pull requests with @create-pr command")
    print("  ✓ Build accumulated knowledge with @save-context")
    print("  ✓ Network access for Claude API and package installs")
    print()
    print("SECURITY ISOLATION:")
    print("  ✗ NO access to SSH keys (cannot git push)")
    print("  ✗ NO access to gcloud credentials (cannot deploy)")
    print("  ✗ NO access to GSM secrets")
    print()
    print("HOW IT WORKS:")
    print("  1. Your local repos are mounted into the container as ~/repos/")
    print("  2. Git worktrees isolate container changes from your working directory")
    print("  3. Agent works on code, creates commits in worktrees")
    print("  4. YOU review commits and push from host (with credentials)")
    print()
    print("FUTURE CAPABILITIES (Roadmap):")
    print("  🔄 GitHub PR context")
    print("  🔄 Slack message context")
    print("  🔄 JIRA ticket context")
    print("  🔄 Email thread context")
    print()

    response = input("Continue? (yes/no): ").strip().lower()
    if response != "yes":
        info("Setup cancelled")
        return False

    print()
    info("Setting up mounts...")
    print()

    Config.CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    mounts = []

    # Check for configured local repositories
    local_repos = get_local_repos()
    if local_repos:
        info(f"Found {len(local_repos)} configured local repository(ies):")
        for repo in local_repos:
            print(f"    • {repo}")
        print()
        print("    These will be mounted as ~/repos/<repo-name>/ with git worktrees")
        print("    for isolated development.")
    else:
        warn("No local repositories configured.")
        print("    Run ./setup.py to configure local repositories.")
        print("    These will be mounted as ~/repos/<repo-name>/")

    # Add context-sync directory (read-only) - includes Confluence, JIRA, and more
    print()
    context_sync_dir = Path.home() / "context-sync"
    if context_sync_dir.exists():
        context_container_path = f"/home/{os.environ['USER']}/context-sync"
        # Add :z flag for SELinux systems
        mounts.append(f"{context_sync_dir}:{context_container_path}:ro,z")
        print(f"  ✓ Context sources: {context_sync_dir}")
        print(f"    Mounted as: ~/context-sync/ (read-only, SELinux relabeled)")

        # Show available context sources
        subdirs = []
        if (context_sync_dir / "confluence").exists():
            subdirs.append("confluence (ADRs, runbooks, docs)")
        if (context_sync_dir / "jira").exists():
            subdirs.append("jira (tickets, issues)")
        if (context_sync_dir / "github").exists():
            subdirs.append("github (PRs, issues)")
        if (context_sync_dir / "slack").exists():
            subdirs.append("slack (messages)")

        if subdirs:
            print(f"    Contains: {', '.join(subdirs)}")
        else:
            print(f"    Note: No context subdirectories found yet")
    else:
        warn(f"Context sync directory not found: {context_sync_dir}")
        warn(f"Expected directory with confluence/, jira/, etc. subdirectories")

    # Create and mount persistent directories for agent
    print()
    info("Setting up persistent directories...")

    # Sharing directory - single location for ALL persistent data
    Config.SHARING_DIR.mkdir(parents=True, exist_ok=True)
    Config.TMP_DIR.mkdir(parents=True, exist_ok=True)    # tmp/ inside sharing/

    sharing_container_path = f"/home/{os.environ['USER']}/sharing"
    # Add :z flag for SELinux systems
    mounts.append(f"{Config.SHARING_DIR}:{sharing_container_path}:rw,z")
    print(f"  ✓ Sharing: {Config.SHARING_DIR}")
    print(f"    Mounted as: ~/sharing/ (read-write, SELinux relabeled)")
    print(f"    Purpose: All persistent data")
    print(f"    - ~/sharing/tmp/           Persistent workspace (also at ~/tmp)")
    print(f"    - ~/sharing/context/       Context documents (@save-context)")
    print(f"    - ~/sharing/notifications/ Notifications to human")
    print(f"    - ~/sharing/incoming/      Incoming tasks from Slack")
    print(f"    - ~/sharing/analysis/      Analysis reports")

    # Create convenience symlink in container for tmp
    # Note: Actual symlink creation happens in container entrypoint

    # Check Anthropic API key authentication
    print()
    print(f"{Colors.BOLD}Claude Code authentication...{Colors.NC}")

    api_key = get_anthropic_api_key()
    if api_key:
        success("Anthropic API key configured")
        print(f"  API key: {api_key[:12]}...{api_key[-4:]}")
    else:
        warn("Anthropic API key not configured")
        print(f"  Set via: export ANTHROPIC_API_KEY=sk-ant-...")
        print(f"  Or save to: {Config.USER_CONFIG_DIR / 'anthropic-api-key'}")
        print()
        info("Container will not be able to use Claude without an API key.")

    print()
    print("Add additional directories? (optional)")
    print("Format: /path/to/dir        (read-write)")
    print("    or: /path/to/dir:ro     (read-only)")
    print("Press Enter on empty line when done")
    print()

    # Collect additional directories
    while True:
        dir_input = input("Additional directory (or Enter to finish): ").strip()
        if not dir_input:
            break

        # Parse mode
        if ":ro" in dir_input or ":rw" in dir_input:
            mount_path_str, mode = dir_input.rsplit(":", 1)
            if mode not in ["ro", "rw"]:
                warn(f"Invalid mode '{mode}', use 'ro' or 'rw'")
                continue
        else:
            mount_path_str = dir_input
            mode = "rw"

        # Expand and validate path
        mount_path = Path(mount_path_str).expanduser().resolve()

        # Check if dangerous
        if is_dangerous_dir(mount_path):
            print(f"⛔ BLOCKED: {mount_path}")
            print("   This directory contains credentials and will not be mounted.")
            print("   This is intentional to prevent AI from accessing sensitive files.")
            continue

        if not mount_path.exists():
            warn(f"Directory does not exist: {mount_path}")
            create = input("Create it? (yes/no): ").strip().lower()
            if create == "yes":
                try:
                    mount_path.mkdir(parents=True, exist_ok=True)
                    success(f"Created: {mount_path}")
                except Exception as e:
                    error(f"Failed to create directory: {e}")
                    continue
            else:
                continue

        # Add SELinux label for Fedora/RHEL compatibility
        mounts.append(f"{mount_path}:{mode},z")
        print(f"Added: {mount_path} ({mode}, SELinux relabeled)")

    # Save configuration
    Config.CONFIG_FILE.write_text("\n".join(mounts))

    print()
    info("Summary of mounted directories:")
    for mount in mounts:
        print(f"  • {mount}")
    print()

    proceed = input("Proceed with this configuration? (yes/no): ").strip().lower()
    if proceed != "yes":
        info("Setup cancelled")
        return False

    # Create Dockerfile and build image
    create_dockerfile()
    print()

    # Let Docker's cache handle what needs rebuilding
    info("Building Docker image (Docker will cache unchanged layers)...")
    if not build_image():
        return False

    print()
    success("Setup complete!")
    print()
    return True


def run_claude() -> bool:
    """Run Claude Code CLI in the sandboxed container (interactive mode)"""
    # Check if image exists
    with _host_timer.phase("check_image"):
        if _quiet_mode:
            status("Checking Docker image...")
        if not image_exists():
            info("Docker image not found. Running initial setup...")
            if not setup():
                return False

    # Load mount configuration
    with _host_timer.phase("check_config"):
        if not Config.CONFIG_FILE.exists():
            info("Configuration not found. Running initial setup...")
            if not setup():
                return False

    # Check Anthropic API key authentication
    with _host_timer.phase("check_api_key"):
        if _quiet_mode:
            status("Checking authentication...")

        api_key = get_anthropic_api_key()

        if not _quiet_mode:
            print()
            print(f"{Colors.BOLD}Checking Claude Code authentication...{Colors.NC}")

            if api_key:
                success("Anthropic API key configured")
                print(f"  API key: {api_key[:12]}...{api_key[-4:]}")
            else:
                warn("Anthropic API key not configured")
                print(f"  Set via: export ANTHROPIC_API_KEY=sk-ant-...")
                print(f"  Or save to: {Config.USER_CONFIG_DIR / 'anthropic-api-key'}")
                print()
                warn("Container will not be able to use Claude without an API key.")

            print()

    # Build/update image (Docker uses cache for unchanged layers - usually instant)
    with _host_timer.phase("build_image"):
        if _quiet_mode:
            status("Building Docker image...")
        if not build_image():
            error("Docker build failed")
            return False

    # Start gateway sidecar container (if not already running)
    with _host_timer.phase("start_gateway"):
        if _quiet_mode:
            status("Starting gateway sidecar...")
        if not start_gateway_container():
            error("Failed to start gateway sidecar")
            return False

    # Generate unique container ID
    with _host_timer.phase("prepare_container"):
        if _quiet_mode:
            status("Preparing container...")
        container_id = generate_container_id()

        if not _quiet_mode:
            info("Launching sandboxed Claude Code environment...")
            print()
            info(f"Container ID: {container_id}")
            print()

    # Create worktrees for repos (isolates container from host repos)
    with _host_timer.phase("create_worktrees"):
        if _quiet_mode:
            status("Creating isolated worktrees...")
        else:
            info("Creating isolated worktrees...")
            print()
        worktrees = create_worktrees(container_id)

    if worktrees and not _quiet_mode:
        print()
        info(f"Created {len(worktrees)} worktree(s)")
        print()

    # Register cleanup on exit
    def cleanup_on_exit():
        cleanup_worktrees(container_id)

    atexit.register(cleanup_on_exit)

    # Parse mount configuration (manual timing due to complex conditional logic)
    _host_timer.start_phase("configure_mounts")
    if _quiet_mode:
        status("Configuring mounts...")
    mount_args = []

    # Build set of container paths that worktrees will use (to avoid duplicates)
    worktree_container_paths = set()
    for repo_name in worktrees:
        worktree_container_paths.add(f"/home/{os.environ['USER']}/repos/{repo_name}")

    if Config.CONFIG_FILE.exists():
        mounts = Config.CONFIG_FILE.read_text().strip().split("\n")
        for mount in mounts:
            if not mount:
                continue
            parts = mount.rsplit(":", 2)  # Split from right, max 2 splits

            if len(parts) == 2:
                # Old format: path:mode
                mount_path, mode = parts
                container_path = mount_path
            elif len(parts) == 3:
                # New format: host_path:container_path:mode
                mount_path, container_path, mode = parts
            else:
                warn(f"Invalid mount format: {mount}")
                continue

            # Skip mounts that would conflict with worktree mounts
            # This handles migration from old direct repo mounts to worktrees
            if container_path in worktree_container_paths:
                if not _quiet_mode:
                    info(f"  Skipping {container_path} (using worktree instead)")
                continue

            # Ensure SELinux label is present (add if missing for backwards compatibility)
            if not mode.endswith(",z") and not mode.endswith(":z"):
                mode = f"{mode},z"

            mount_args.extend(["-v", f"{mount_path}:{container_path}:{mode}"])
            if not _quiet_mode:
                mode_str = "READ-ONLY" if "ro" in mode else "READ-WRITE"
                display_path = container_path if container_path != mount_path else mount_path
                print(f"  • {display_path} ({mode_str}, SELinux relabeled)")

    # Add worktree mounts for configured local repositories
    # worktrees dict has structure: {repo_name: {"worktree": path, "source": path}}
    for repo_name, repo_info in worktrees.items():
        worktree_path = repo_info["worktree"]
        source_path = repo_info["source"]
        container_path = f"/home/{os.environ['USER']}/repos/{repo_name}"
        mount_args.extend(["-v", f"{worktree_path}:{container_path}:rw,z"])
        if not _quiet_mode:
            print(f"  • ~/repos/{repo_name} (WORKTREE, isolated from host)")

        # Mount main repo .git directories (read-write) so worktrees can commit changes
        git_path = source_path / ".git"
        if git_path.is_dir():
            # Normal repo - mount .git directory directly
            git_container_path = f"/home/{os.environ['USER']}/.git-main/{repo_name}"
            mount_args.extend(["-v", f"{git_path}:{git_container_path}:rw,z"])
            if not _quiet_mode:
                print(f"  • ~/.git-main/{repo_name} (git metadata for worktree, read-write)")
        elif git_path.is_file():
            # Host repo is a worktree - read .git file to find actual git directory
            # Format is: "gitdir: /path/to/repo.git/worktrees/name"
            try:
                gitdir_content = git_path.read_text().strip()
                if gitdir_content.startswith("gitdir:"):
                    gitdir_path = Path(gitdir_content[7:].strip())
                    # Navigate up from worktrees/<name> to the main .git directory
                    # e.g., /path/.git/worktrees/foo -> /path/.git
                    if "worktrees" in gitdir_path.parts:
                        worktrees_idx = gitdir_path.parts.index("worktrees")
                        main_git_path = Path(*gitdir_path.parts[:worktrees_idx])
                        if main_git_path.is_dir():
                            git_container_path = f"/home/{os.environ['USER']}/.git-main/{repo_name}"
                            mount_args.extend(["-v", f"{main_git_path}:{git_container_path}:rw,z"])
                            if not _quiet_mode:
                                print(f"  • ~/.git-main/{repo_name} (git metadata, from host worktree)")
                        else:
                            warn(f"Could not find git directory for {repo_name}: {main_git_path}")
                    else:
                        warn(f"Unexpected gitdir format for {repo_name}: {gitdir_path}")
                else:
                    warn(f"Invalid .git file format for {repo_name}")
            except Exception as e:
                warn(f"Error reading .git file for {repo_name}: {e}")

    # Mount worktree base directory (used by both interactive and --exec modes)
    worktree_base_container = f"/home/{os.environ['USER']}/.jib-worktrees"
    mount_args.extend(["-v", f"{Config.WORKTREE_BASE}:{worktree_base_container}:rw,z"])
    if not _quiet_mode:
        print(f"  • ~/.jib-worktrees/ (worktree base directory)")

    # Mount host Claude configuration (interactive mode only)
    # This allows the container to use the host's Claude settings
    home = Path.home()
    claude_dir = home / ".claude"
    claude_json = home / ".claude.json"

    if claude_dir.is_dir():
        container_claude_dir = f"/home/{os.environ['USER']}/.claude"
        mount_args.extend(["-v", f"{claude_dir}:{container_claude_dir}:rw,z"])
        if not _quiet_mode:
            print(f"  • ~/.claude (Claude config directory)")

    if claude_json.is_file():
        container_claude_json = f"/home/{os.environ['USER']}/.claude.json"
        mount_args.extend(["-v", f"{claude_json}:{container_claude_json}:rw,z"])
        if not _quiet_mode:
            print(f"  • ~/.claude.json (Claude settings)")

    if not _quiet_mode:
        print()
    _host_timer.end_phase()  # configure_mounts

    # Remove old container if exists (cleanup any previous runs)
    with _host_timer.phase("cleanup_old_container"):
        subprocess.run(["docker", "rm", "-f", container_id],
                      stdout=subprocess.DEVNULL,
                      stderr=subprocess.DEVNULL)

    # Build docker run command on jib-network (shared network with gateway sidecar)
    _host_timer.start_phase("build_docker_cmd")
    # jib-network allows container-to-container communication while isolating from host
    worktree_host_path = Config.WORKTREE_BASE / container_id
    cmd = [
        "docker", "run",
        "--rm",  # Auto-remove container after exit
        "-it",   # Interactive with TTY
        "--name", container_id,
        "--network", JIB_NETWORK_NAME,  # Connect to jib-network for gateway access
        "-e", f"RUNTIME_USER={os.environ['USER']}",
        "-e", f"RUNTIME_UID={os.getuid()}",
        "-e", f"RUNTIME_GID={os.getgid()}",
        "-e", f"CONTAINER_ID={container_id}",
        "-e", f"JIB_QUIET={'1' if _quiet_mode else '0'}",
        "-e", f"JIB_TIMING={'1' if _host_timer.enabled else '0'}",
        "-e", f"GATEWAY_URL=http://{GATEWAY_CONTAINER_NAME}:{GATEWAY_PORT}",
        # Host worktree path - needed by git wrapper to translate container paths
        # Container sees /home/user/repos/X, but gateway needs ~/.jib-worktrees/<id>/X
        "-e", f"JIB_WORKTREE_HOST_PATH={worktree_host_path}",
    ]

    # GitHub authentication is handled by the gateway sidecar
    # The container does NOT receive GITHUB_TOKEN - all git/gh operations
    # route through the gateway which holds the credentials
    if not _quiet_mode:
        info("GitHub auth: Via gateway sidecar (credentials not in container)")

    # Add LLM provider preference and API keys
    llm_provider = get_llm_provider()
    cmd.extend(["-e", f"LLM_PROVIDER={llm_provider}"])
    
    # Add provider-specific API keys
    google_api_key = get_google_api_key()
    openai_api_key = get_openai_api_key()
    
    if google_api_key:
        cmd.extend(["-e", f"GOOGLE_API_KEY={google_api_key}"])
    if openai_api_key:
        cmd.extend(["-e", f"OPENAI_API_KEY={openai_api_key}"])
    
    # Set Anthropic API key if available
    # For non-Anthropic providers, the router handles auth via ANTHROPIC_AUTH_TOKEN
    if api_key:
        cmd.extend(["-e", f"ANTHROPIC_API_KEY={api_key}"])

    if not _quiet_mode:
        info(f"LLM Provider: {llm_provider}")
        info("Network mode: Bridge (isolated from host, outbound HTTP only)")
        print("  Container can: Access Claude API, download packages")
        print("  Container cannot: Access host services, accept inbound connections")
        print()

    # Add mount arguments
    cmd.extend(mount_args)

    # Add image name
    cmd.append(Config.IMAGE_NAME)

    # End timing for command build
    _host_timer.end_phase()  # build_docker_cmd

    # Print timing summary before launching container
    _host_timer.print_summary()

    # Final status update before launching
    if _quiet_mode:
        status("Launching Claude...")

    # Run container
    try:
        subprocess.run(cmd)
        return True
    except KeyboardInterrupt:
        print()
        warn("Interrupted by user")
        return False
    except Exception as e:
        error(f"Failed to run container: {e}")
        return False


def exec_in_new_container(
    command: List[str],
    timeout_minutes: int = 30,
    task_id: Optional[str] = None,
    thread_ts: Optional[str] = None,
    auth_mode: str = "host",
) -> bool:
    """Execute a command in a new ephemeral container with isolated worktrees.

    Creates worktrees for all repos to isolate changes (same as interactive mode):
    - Total isolation from interactive sessions and main repos
    - Worktrees allow parallel work without conflicts
    - Automatic cleanup (--rm)
    - All commits go to temporary branches (jib-temp-jib-exec-*)
    - Container logs persisted to ~/.jib-sharing/container-logs/

    Args:
        command: Command to execute
        timeout_minutes: Timeout in minutes (default: 30)
        task_id: Optional task ID for log correlation (auto-detected from command if not provided)
        thread_ts: Optional Slack thread timestamp for correlation
        auth_mode: Authentication method - 'host' mounts ~/.claude, 'api-key' passes env var

    Returns:
        True if successful, False otherwise

    Raises:
        ValueError: If auth_mode is not 'host' or 'api-key'
    """
    # Validate auth_mode parameter
    valid_auth_modes = ("host", "api-key")
    if auth_mode not in valid_auth_modes:
        raise ValueError(f"Invalid auth_mode '{auth_mode}'. Must be one of: {valid_auth_modes}")

    # Check if image exists
    if not image_exists():
        info("Docker image not found. Running initial setup...")
        if not setup():
            return False

    # Load mount configuration
    if not Config.CONFIG_FILE.exists():
        info("Configuration not found. Running initial setup...")
        if not setup():
            return False

    # Build/update image
    if not build_image():
        error("Docker build failed")
        return False

    # Start gateway sidecar container (if not already running)
    if not start_gateway_container():
        error("Failed to start gateway sidecar")
        return False

    # Generate unique container ID for this exec
    container_id = f"jib-exec-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{os.getpid()}"

    # Auto-detect task_id from command if not provided
    if not task_id:
        task_id = extract_task_id_from_command(command)

    # Auto-detect thread_ts from task file if not provided
    if not thread_ts and task_id:
        for arg in command:
            if ".md" in arg:
                thread_ts = extract_thread_ts_from_task_file(arg)
                break

    info(f"Executing command in new container: {container_id}")
    if task_id:
        info(f"Task ID: {task_id}")
    if thread_ts:
        info(f"Thread TS: {thread_ts}")
    print(f"Command: {' '.join(command)}")
    print(f"Timeout: {timeout_minutes} minutes")
    print()

    # Create worktrees for repos (isolates container from host repos)
    info("Creating isolated worktrees...")
    worktrees = create_worktrees(container_id)

    if worktrees:
        info(f"Created {len(worktrees)} worktree(s)")
        print()

    # Register cleanup on exit (even if container fails)
    def cleanup_on_exit():
        cleanup_worktrees(container_id)

    atexit.register(cleanup_on_exit)

    # Parse mount configuration
    mount_args = []

    # Build set of container paths that worktrees will use (to avoid duplicates)
    worktree_container_paths = set()
    for repo_name in worktrees:
        worktree_container_paths.add(f"/home/{os.environ['USER']}/repos/{repo_name}")

    if Config.CONFIG_FILE.exists():
        mounts = Config.CONFIG_FILE.read_text().strip().split("\n")

        for mount in mounts:
            if not mount:
                continue
            parts = mount.rsplit(":", 2)

            if len(parts) == 2:
                mount_path, mode = parts
                container_path = mount_path
            elif len(parts) == 3:
                mount_path, container_path, mode = parts
            else:
                warn(f"Invalid mount format: {mount}")
                continue

            # Skip mounts that would conflict with worktree mounts
            if container_path in worktree_container_paths:
                continue

            # Ensure SELinux label
            if not mode.endswith(",z") and not mode.endswith(":z"):
                mode = f"{mode},z"

            mount_args.extend(["-v", f"{mount_path}:{container_path}:{mode}"])

    # Add worktree mounts for configured local repositories
    # worktrees dict has structure: {repo_name: {"worktree": path, "source": path}}
    for repo_name, repo_info in worktrees.items():
        worktree_path = repo_info["worktree"]
        source_path = repo_info["source"]
        container_path = f"/home/{os.environ['USER']}/repos/{repo_name}"
        mount_args.extend(["-v", f"{worktree_path}:{container_path}:rw,z"])
        print(f"  • ~/repos/{repo_name} (WORKTREE, isolated)")

        # Mount main repo .git directories (read-write) so worktrees can commit changes
        git_path = source_path / ".git"
        if git_path.is_dir():
            # Normal repo - mount .git directory directly
            git_container_path = f"/home/{os.environ['USER']}/.git-main/{repo_name}"
            mount_args.extend(["-v", f"{git_path}:{git_container_path}:rw,z"])
            print(f"  • ~/.git-main/{repo_name} (git metadata)")
        elif git_path.is_file():
            # Host repo is a worktree - read .git file to find actual git directory
            try:
                gitdir_content = git_path.read_text().strip()
                if gitdir_content.startswith("gitdir:"):
                    gitdir_path = Path(gitdir_content[7:].strip())
                    if "worktrees" in gitdir_path.parts:
                        worktrees_idx = gitdir_path.parts.index("worktrees")
                        main_git_path = Path(*gitdir_path.parts[:worktrees_idx])
                        if main_git_path.is_dir():
                            git_container_path = f"/home/{os.environ['USER']}/.git-main/{repo_name}"
                            mount_args.extend(["-v", f"{main_git_path}:{git_container_path}:rw,z"])
                            print(f"  • ~/.git-main/{repo_name} (git metadata, from host worktree)")
                        else:
                            warn(f"Could not find git directory for {repo_name}: {main_git_path}")
                    else:
                        warn(f"Unexpected gitdir format for {repo_name}: {gitdir_path}")
                else:
                    warn(f"Invalid .git file format for {repo_name}")
            except Exception as e:
                warn(f"Error reading .git file for {repo_name}: {e}")

    # Mount host Claude configuration when using host auth mode
    if auth_mode == "host":
        home = Path.home()
        claude_dir = home / ".claude"
        claude_json = home / ".claude.json"

        if claude_dir.is_dir():
            container_claude_dir = f"/home/{os.environ['USER']}/.claude"
            mount_args.extend(["-v", f"{claude_dir}:{container_claude_dir}:rw,z"])
            print(f"  • ~/.claude (Claude config directory)")

        if claude_json.is_file():
            container_claude_json = f"/home/{os.environ['USER']}/.claude.json"
            mount_args.extend(["-v", f"{claude_json}:{container_claude_json}:rw,z"])
            print(f"  • ~/.claude.json (Claude settings)")

    print()

    # Build docker run command on jib-network (shared network with gateway sidecar)
    # Note: We don't use --rm so we can save logs before cleanup
    worktree_host_path = Config.WORKTREE_BASE / container_id
    cmd = [
        "docker", "run",
        "--name", container_id,
        "--network", JIB_NETWORK_NAME,  # Connect to jib-network for gateway access
        "-e", f"RUNTIME_USER={os.environ['USER']}",
        "-e", f"RUNTIME_UID={os.getuid()}",
        "-e", f"RUNTIME_GID={os.getgid()}",
        "-e", f"CONTAINER_ID={container_id}",
        "-e", "PYTHONUNBUFFERED=1",  # Force Python to use unbuffered output for real-time streaming
        "-e", f"GATEWAY_URL=http://{GATEWAY_CONTAINER_NAME}:{GATEWAY_PORT}",
        # Host worktree path - needed by git wrapper to translate container paths
        # Container sees /home/user/repos/X, but gateway needs ~/.jib-worktrees/<id>/X
        "-e", f"JIB_WORKTREE_HOST_PATH={worktree_host_path}",
    ]

    # Add logging configuration for log persistence
    log_config = get_docker_log_config(container_id, task_id)
    cmd.extend(log_config)

    # Add correlation environment variables for log tracing
    if task_id:
        cmd.extend(["-e", f"JIB_TASK_ID={task_id}"])
    if thread_ts:
        cmd.extend(["-e", f"JIB_THREAD_TS={thread_ts}"])

    # GitHub authentication is handled by the gateway sidecar
    # The container does NOT receive GITHUB_TOKEN - all git/gh operations
    # route through the gateway which holds the credentials

    # Add LLM provider preference and API keys
    llm_provider = get_llm_provider()
    cmd.extend(["-e", f"LLM_PROVIDER={llm_provider}"])

    # Add provider-specific API keys
    # Google and OpenAI keys are always passed (may be needed for other purposes)
    google_api_key = get_google_api_key()
    openai_api_key = get_openai_api_key()

    if google_api_key:
        cmd.extend(["-e", f"GOOGLE_API_KEY={google_api_key}"])
    if openai_api_key:
        cmd.extend(["-e", f"OPENAI_API_KEY={openai_api_key}"])

    # Set Anthropic API key only when using api-key auth mode
    # When using host auth, Claude Code will use ~/.claude for authentication
    if auth_mode == "api-key":
        api_key = get_anthropic_api_key()
        if api_key:
            cmd.extend(["-e", f"ANTHROPIC_API_KEY={api_key}"])

    # Add mount arguments
    cmd.extend(mount_args)

    # Add image name
    cmd.append(Config.IMAGE_NAME)

    # Add the command to execute
    cmd.extend(command)

    # Run container with configurable timeout
    timeout_seconds = timeout_minutes * 60
    success = False

    def cleanup_container():
        """Save logs and remove container."""
        try:
            # Save container logs before removal (with correlation info)
            save_container_logs(container_id, task_id, thread_ts)
        except Exception as e:
            error(f"Failed to save container logs: {e}")
            # Don't re-raise - continue with container removal
        finally:
            # Remove container
            try:
                subprocess.run(
                    ["docker", "rm", "-f", container_id],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL
                )
            except Exception as e:
                error(f"Failed to remove container: {e}")
                # Don't re-raise - original error is more important

    try:
        result = subprocess.run(cmd, timeout=timeout_seconds)
        success = result.returncode == 0
    except subprocess.TimeoutExpired:
        print()
        error(f"Container execution timed out after {timeout_minutes} minutes")
        # Kill the container if it's still running
        subprocess.run(["docker", "kill", container_id],
                      stdout=subprocess.DEVNULL,
                      stderr=subprocess.DEVNULL)
    except KeyboardInterrupt:
        print()
        warn("Interrupted by user")
        # Kill container on interrupt
        subprocess.run(["docker", "kill", container_id],
                      stdout=subprocess.DEVNULL,
                      stderr=subprocess.DEVNULL)
    except Exception as e:
        error(f"Failed to run container: {e}")
    finally:
        # Always save logs and cleanup container
        cleanup_container()

    return success


def main():
    parser = argparse.ArgumentParser(
        description="Run Claude Code CLI in an isolated Docker container (james-in-a-box)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  jib                                      # Run Claude Code (progress bar by default, auto-setup if needed)
  jib -v                                   # Run in verbose mode (detailed output)
  jib --time                               # Show startup timing breakdown for debugging
  jib --setup                              # Run full setup (delegates to setup.py)
  jib --reset                              # Reset configuration and remove Docker image
  jib --exec <command> [args...]          # Execute command in new ephemeral container
  jib --timeout 60 --exec <command>       # Execute with custom timeout (60 minutes)

Note: --exec spawns a new container for each execution (automatic cleanup with --rm)
      Default timeout is 30 minutes, configurable via --timeout
      If setup is incomplete, jib will prompt to run setup automatically
      Default shows progress bar; use -v for verbose output
        """
    )
    parser.add_argument(
        "--setup",
        action="store_true",
        help="Run full jib setup (services, config, Docker image)"
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="Clear configuration and start over"
    )
    parser.add_argument(
        "--timeout",
        type=int,
        default=30,
        metavar="MINUTES",
        help="Timeout in minutes for --exec commands (default: 30)"
    )
    parser.add_argument(
        "--auth",
        choices=["host", "api-key"],
        default="host",
        help="Authentication method for --exec: 'host' mounts ~/.claude from host (default), 'api-key' passes ANTHROPIC_API_KEY env var"
    )
    parser.add_argument(
        "--exec",
        nargs=argparse.REMAINDER,
        help="Execute a command in a new ephemeral container (automatic cleanup)"
    )
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Verbose mode: show detailed output instead of progress bar (default: quiet with progress bar)"
    )
    parser.add_argument(
        "--time",
        action="store_true",
        help="Show startup timing breakdown for debugging slow startup"
    )

    args = parser.parse_args()

    # Enable timing if --time flag is set
    if args.time:
        _host_timer.enabled = True

    # Initialize quiet mode globally
    # Quiet is the default; verbose (-v) overrides it
    global _quiet_mode
    _quiet_mode = not args.verbose
    if _quiet_mode:
        # Initialize statusbar with estimated steps for interactive mode
        # Steps: check docker image, check auth, build image, prepare container,
        #        create worktrees, configure mounts, configure github, launch
        init_statusbar(total_steps=8, enabled=True)


    # Handle reset
    if args.reset:
        warn("Resetting configuration...")
        import shutil
        if Config.CONFIG_DIR.exists():
            shutil.rmtree(Config.CONFIG_DIR)

        # Ask about persistent directories
        if Config.TOOLS_DIR.exists() or Config.SHARING_DIR.exists():
            print()
            warn("Persistent directories found:")
            if Config.TOOLS_DIR.exists():
                print(f"  • {Config.TOOLS_DIR} (reusable scripts/tools)")
            if Config.SHARING_DIR.exists():
                print(f"  • {Config.SHARING_DIR} (shared artifacts, context documents)")
            print()
            response = input("Remove these as well? (yes/no): ").strip().lower()
            if response == "yes":
                if Config.TOOLS_DIR.exists():
                    shutil.rmtree(Config.TOOLS_DIR)
                    warn(f"Removed: {Config.TOOLS_DIR}")
                if Config.SHARING_DIR.exists():
                    shutil.rmtree(Config.SHARING_DIR)
                    warn(f"Removed: {Config.SHARING_DIR}")
            else:
                info("Preserved persistent directories")

        success("Configuration reset. Run again to set up fresh.")
        return 0

    # Check prerequisites
    if not check_docker():
        return 1

    if not check_docker_permissions():
        return 1

    # Check host setup (services and directories)
    if not check_host_setup():
        return 1

    # Handle setup - delegate to setup.py
    if args.setup:
        info("Delegating to setup.py for complete jib configuration...")
        print()
        if not run_setup_script():
            return 1
        return 0

    # Handle exec - execute in a new ephemeral container
    if args.exec:
        if not exec_in_new_container(args.exec, timeout_minutes=args.timeout, auth_mode=args.auth):
            return 1
        return 0

    # Normal run
    if not run_claude():
        return 1

    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print()
        warn("Interrupted by user")
        sys.exit(0)
    except Exception as e:
        error(f"Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
