#!/usr/bin/env python3
"""
jib (james-in-a-box) - Run Claude Code CLI in an isolated Docker container

Prevents AI agents from accessing credentials while allowing full code editing.

Platform Support:
  - Linux (x86_64, ARM64): Fully supported
  - macOS (Intel, Apple Silicon): Fully supported
"""

import argparse
import atexit
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import List, Optional


class Colors:
    """ANSI color codes for terminal output"""
    BLUE = '\033[0;34m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    RED = '\033[0;31m'
    BOLD = '\033[1m'
    NC = '\033[0m'


def info(msg: str) -> None:
    print(f"{Colors.BLUE}[INFO]{Colors.NC} {msg}")


def success(msg: str) -> None:
    print(f"{Colors.GREEN}[SUCCESS]{Colors.NC} {msg}")


def warn(msg: str) -> None:
    print(f"{Colors.YELLOW}[WARNING]{Colors.NC} {msg}")


def error(msg: str) -> None:
    print(f"{Colors.RED}[ERROR]{Colors.NC} {msg}", file=sys.stderr)


def check_claude_credentials() -> bool:
    """
    Check if Claude credentials exist.

    Returns:
        bool: credentials_exist
    """
    claude_dir = Path.home() / ".claude"
    credentials_file = claude_dir / ".credentials.json"

    return credentials_file.exists()


class Config:
    """Configuration paths and constants"""
    CONFIG_DIR = Path.home() / ".jib"  # Docker staging directory
    CONFIG_FILE = CONFIG_DIR / "mounts.conf"
    DOCKERFILE = CONFIG_DIR / "Dockerfile"
    USER_CONFIG_DIR = Path.home() / ".config" / "jib"  # User config (secrets, preferences)
    GITHUB_TOKEN_FILE = USER_CONFIG_DIR / "github-token"
    IMAGE_NAME = "james-in-a-box"
    CONTAINER_NAME = "jib"
    KHAN_SOURCE = Path.home() / "khan"
    # Persistent directory for all shared data
    SHARING_DIR = Path.home() / ".jib-sharing"
    TMP_DIR = SHARING_DIR / "tmp"      # Persistent tmp workspace
    # Persistent .claude directory shared by all containers (in ~/.jib/)
    CLAUDE_SHARED_DIR = CONFIG_DIR / "claude"
    # Worktree base directory (ephemeral workspaces per container)
    WORKTREE_BASE = Path.home() / ".jib-worktrees"

    # Note: Each container gets its own worktree to isolate changes
    # Host repos (~/khan/) stay clean while containers work independently

    # Directories that are dangerous to mount (contain credentials)
    DANGEROUS_DIRS = [
        Path.home() / ".ssh",
        Path.home() / ".config" / "gcloud",
        Path.home() / ".gitconfig",
        Path.home() / ".netrc",
        Path.home() / ".aws",
        Path.home() / ".kube",
        Path.home() / ".gnupg",
        Path.home() / ".docker",
    ]


def get_platform() -> str:
    """Detect platform: linux or macos"""
    import platform
    system = platform.system().lower()
    if system == "linux":
        return "linux"
    elif system == "darwin":
        return "macos"
    return "unknown"


def get_github_token() -> Optional[str]:
    """Get GitHub PAT from jib config file (for host use).

    Returns:
        Token string if found and valid, None otherwise
    """
    if Config.GITHUB_TOKEN_FILE.exists():
        try:
            token = Config.GITHUB_TOKEN_FILE.read_text().strip()
            if token and token.startswith(("ghp_", "github_pat_")):
                return token
        except Exception:
            pass
    return None


def get_github_app_token() -> Optional[str]:
    """Generate GitHub App installation token for container use.

    Uses the github-app-token.py script to generate a fresh installation token
    from App credentials (App ID, Installation ID, private key).

    Returns:
        Installation token string if successful, None otherwise
    """
    # Check if App credentials exist
    app_id_file = Config.USER_CONFIG_DIR / "github-app-id"
    installation_id_file = Config.USER_CONFIG_DIR / "github-app-installation-id"
    private_key_file = Config.USER_CONFIG_DIR / "github-app.pem"

    if not all(f.exists() for f in [app_id_file, installation_id_file, private_key_file]):
        return None  # App not configured, fall back to PAT

    # Find the token generation script
    script_dir = Path(__file__).resolve().parent
    token_script = script_dir / "jib-tools" / "github-app-token.py"

    if not token_script.exists():
        warn(f"GitHub App token script not found: {token_script}")
        return None

    # Use the host-services venv Python which has cryptography installed
    # Fall back to system python3 if venv doesn't exist
    jib_root = script_dir.parent
    venv_python = jib_root / "host-services" / ".venv" / "bin" / "python"
    python_cmd = str(venv_python) if venv_python.exists() else "python3"

    try:
        result = subprocess.run(
            [python_cmd, str(token_script), "--config-dir", str(Config.USER_CONFIG_DIR)],
            capture_output=True,
            text=True,
            timeout=30
        )

        if result.returncode == 0:
            token = result.stdout.strip()
            if token and token.startswith("ghs_"):  # Installation tokens start with ghs_
                return token
            elif token:
                # Token format might vary, accept if non-empty
                return token

        # Log error but don't fail - we can fall back to PAT
        if result.stderr:
            warn(f"GitHub App token generation failed: {result.stderr.strip()}")

    except subprocess.TimeoutExpired:
        warn("GitHub App token generation timed out")
    except Exception as e:
        warn(f"GitHub App token generation error: {e}")

    return None


def check_docker_permissions() -> bool:
    """Check if user has permission to run Docker commands"""
    result = subprocess.run(
        ["docker", "ps"],
        capture_output=True,
        text=True
    )

    if result.returncode == 0:
        return True

    if "permission denied" in result.stderr.lower():
        error("Docker permission denied - you are not in the docker group")
        print()
        print("This usually means one of two things:")
        print("  1. You just installed Docker and need to log out/in for group membership")
        print("  2. You need to be added to the docker group")
        print()
        print("Solutions:")
        print()
        print("Option 1: Add yourself to docker group and re-login")
        print("  sudo usermod -aG docker $USER")
        print("  then LOG OUT and LOG BACK IN")
        print()
        print("Option 2: Run with sudo (temporary workaround)")
        print("  sudo $(which jib)")
        print()
        return False

    return False


def check_host_setup() -> bool:
    """Check if host setup is complete (services installed, directories exist)"""
    # Critical systemd services that should be installed
    critical_services = [
        "slack-notifier.service",
        "slack-receiver.service"
    ]

    # Important directories that should exist
    critical_dirs = [
        Path.home() / ".jib-sharing" / "notifications",
        Path.home() / ".jib-sharing" / "incoming",
        Path.home() / ".jib-sharing" / "responses",
    ]

    # Configuration file
    config_file = Path.home() / ".config" / "jib-notifier" / "config.json"

    issues_found = []

    # Check if systemd services are installed
    for service in critical_services:
        result = subprocess.run(
            ["systemctl", "--user", "list-unit-files", service],
            capture_output=True,
            text=True
        )
        if result.returncode != 0 or service not in result.stdout:
            issues_found.append(f"Service not installed: {service}")

    # Check if critical directories exist
    for dir_path in critical_dirs:
        if not dir_path.exists():
            issues_found.append(f"Directory not found: {dir_path}")

    # Check if config exists (warning only, not critical)
    config_warning = None
    if not config_file.exists():
        config_warning = f"Configuration file not found: {config_file}"

    # Report findings
    if issues_found or config_warning:
        warn("Host setup appears incomplete")
        print()

        if issues_found:
            error("Critical issues found:")
            for issue in issues_found:
                print(f"  âœ— {issue}")
            print()

        if config_warning:
            warn(config_warning)
            print()

        print("JIB requires host services to be installed for full functionality:")
        print("  â€¢ Slack integration (notifier and receiver)")
        print("  â€¢ Shared directories for notifications and task communication")
        print()
        print("To set up the host:")
        print("  cd ~/khan/james-in-a-box")
        print("  ./setup.sh")
        print()

        if issues_found:
            response = input("Continue anyway? (yes/no): ").strip().lower()
            if response != "yes":
                info("Please run ./setup.sh to configure the host first")
                return False

    return True


def check_docker() -> bool:
    """Check if Docker is installed and offer to install if not"""
    platform_name = get_platform()

    if subprocess.run(["which", "docker"], capture_output=True).returncode != 0:
        error("Docker is not installed.")

        if platform_name == "macos":
            # TODO: Add macOS Docker Desktop installation
            info("On macOS, please install Docker Desktop from:")
            info("  https://www.docker.com/products/docker-desktop")
            return False

        # Linux installation
        response = input("Install Docker now? (yes/no): ").strip().lower()
        if response == "yes":
            info("Installing Docker...")
            try:
                # Download installer
                subprocess.run(
                    ["curl", "-fsSL", "https://get.docker.com", "-o", "/tmp/get-docker.sh"],
                    check=True
                )
                # Run installer
                subprocess.run(["sudo", "sh", "/tmp/get-docker.sh"], check=True)
                # Add user to docker group
                subprocess.run(["sudo", "usermod", "-aG", "docker", os.environ["USER"]], check=True)
                # Cleanup
                os.remove("/tmp/get-docker.sh")

                success("Docker installed successfully!")
                print()
                warn("IMPORTANT: You need to log out and back in for group membership to take effect.")
                print("After logging back in, run this script again.")
                sys.exit(0)
            except Exception as e:
                error(f"Docker installation failed: {e}")
                return False
        else:
            error("Docker is required")
            return False

    # Check Docker daemon is running and we have permissions
    return check_docker_permissions()



def is_dangerous_dir(path: Path) -> bool:
    """Check if a directory is dangerous to mount (contains credentials)"""
    for dangerous in Config.DANGEROUS_DIRS:
        try:
            # Check if path is dangerous or contains dangerous
            if path.resolve() == dangerous.resolve():
                return True
            if path.resolve() in dangerous.resolve().parents:
                return True
            if dangerous.resolve() in path.resolve().parents:
                return True
        except Exception:
            pass
    return False


def create_dockerfile() -> None:
    """Create the Dockerfile for the container"""
    import shutil

    # Resolve symlinks to find the actual project directory
    script_dir = Path(__file__).resolve().parent

    # Copy docker-setup.py to config directory
    setup_script = script_dir / "docker-setup.py"
    setup_dest = Config.CONFIG_DIR / "docker-setup.py"

    if setup_script.exists():
        shutil.copy(setup_script, setup_dest)
        setup_dest.chmod(0o755)
    else:
        warn("docker-setup.py not found, skipping dev tools installation")

    # Copy claude-commands directory to config directory
    commands_src = script_dir / "claude-commands"
    commands_dest = Config.CONFIG_DIR / "claude-commands"
    if commands_src.exists():
        if commands_dest.exists():
            shutil.rmtree(commands_dest)
        shutil.copytree(commands_src, commands_dest)
        info("Claude commands copied to build context")
    else:
        warn("claude-commands directory not found")

    # Copy claude-rules directory to build context
    rules_src = script_dir / "claude-rules"
    rules_dest = Config.CONFIG_DIR / "claude-rules"
    if rules_src.exists():
        if rules_dest.exists():
            shutil.rmtree(rules_dest)
        shutil.copytree(rules_src, rules_dest)
        info("Claude rules directory copied to build context")
    else:
        warn("claude-rules directory not found, skipping agent rules")

    # Note: Claude credentials are mounted at runtime (not copied at build time)
    # This ensures the container always uses the host's CURRENT credentials
    # Avoids issues with stale/revoked OAuth tokens from previous builds
    info("Claude credentials will be mounted from host at runtime (see setup output above)")

    # Copy Dockerfile from script directory
    dockerfile_src = script_dir / "Dockerfile"
    if dockerfile_src.exists():
        shutil.copy(dockerfile_src, Config.DOCKERFILE)
        success("Build context prepared")
    else:
        error(f"Dockerfile not found at {dockerfile_src}")
        error("Cannot build without Dockerfile")


def build_image() -> bool:
    """Build the Docker image (Docker's cache makes this fast when nothing changed)"""
    # Always sync files to build context - Docker detects changes and rebuilds only what's needed
    create_dockerfile()

    try:
        cmd = [
            "docker", "build",
            "--build-arg", f"USER_NAME={os.environ['USER']}",
            "--build-arg", f"USER_UID={os.getuid()}",
            "--build-arg", f"USER_GID={os.getgid()}",
            "-t", Config.IMAGE_NAME,
            "-f", str(Config.DOCKERFILE),
            str(Config.CONFIG_DIR)
        ]

        # Docker automatically uses cache for unchanged layers
        subprocess.run(cmd, check=True)
        return True
    except subprocess.CalledProcessError:
        error("Docker build failed")
        return False


def image_exists() -> bool:
    """Check if Docker image exists"""
    return subprocess.run(
        ["docker", "image", "inspect", Config.IMAGE_NAME],
        capture_output=True
    ).returncode == 0


def generate_container_id() -> str:
    """Generate unique container ID based on timestamp and process ID"""
    timestamp = time.strftime("%Y%m%d-%H%M%S")
    pid = os.getpid()
    return f"jib-{timestamp}-{pid}"


def create_worktrees(container_id: str) -> dict:
    """Create git worktrees for all repos in ~/khan/

    Returns:
        Dictionary mapping repo names to worktree paths
    """
    worktree_dir = Config.WORKTREE_BASE / container_id
    worktree_dir.mkdir(parents=True, exist_ok=True)

    worktrees = {}

    if not Config.KHAN_SOURCE.exists():
        warn(f"Khan source directory not found: {Config.KHAN_SOURCE}")
        return worktrees

    # Iterate through all repos in ~/khan/
    for repo_path in Config.KHAN_SOURCE.iterdir():
        if not repo_path.is_dir():
            continue

        # Check if it's a git repository
        git_dir = repo_path / ".git"
        if not git_dir.exists():
            continue

        repo_name = repo_path.name
        worktree_path = worktree_dir / repo_name

        try:
            info(f"Creating worktree for {repo_name}...")

            # Create temporary branch for this container
            branch_name = f"jib-temp-{container_id}"

            # Create worktree
            result = subprocess.run(
                ["git", "worktree", "add", str(worktree_path), "-b", branch_name],
                cwd=repo_path,
                capture_output=True,
                text=True
            )

            if result.returncode == 0:
                worktrees[repo_name] = worktree_path
                success(f"  âœ“ {repo_name} -> {worktree_path}")
            else:
                warn(f"  âœ— Failed to create worktree for {repo_name}: {result.stderr}")

        except Exception as e:
            warn(f"  âœ— Error creating worktree for {repo_name}: {e}")

    return worktrees


def cleanup_worktrees(container_id: str) -> None:
    """Clean up worktrees for a container

    Args:
        container_id: Unique container identifier
    """
    worktree_dir = Config.WORKTREE_BASE / container_id

    if not worktree_dir.exists():
        return

    info(f"Cleaning up worktrees for {container_id}...")

    # Collect repos that need pruning
    repos_to_prune = set()
    for worktree_path in worktree_dir.iterdir():
        if worktree_path.is_dir():
            repo_name = worktree_path.name
            original_repo = Config.KHAN_SOURCE / repo_name
            if original_repo.exists():
                repos_to_prune.add(original_repo)

    # Remove container worktree directory
    # Don't use 'git worktree remove' because:
    # - Container modified .git files to point to container paths
    # - Git cannot validate the worktree state from the host
    # - Commits are already safe in git (on the jib-temp-* branch)
    # - We just need to clean up the working directory and prune metadata
    try:
        import shutil
        shutil.rmtree(worktree_dir)
        info(f"  âœ“ Removed worktree directory")
    except Exception as e:
        warn(f"  âœ— Failed to remove directory {worktree_dir}: {e}")

    # Prune stale worktree metadata from each repo
    # This removes the .git/worktrees/* admin directories
    # All commits on jib-temp-* branches are preserved
    for repo_path in repos_to_prune:
        try:
            subprocess.run(
                ["git", "worktree", "prune", "-v"],
                cwd=repo_path,
                capture_output=True,
                text=True,
                check=True
            )
        except Exception:
            pass  # Non-critical if pruning fails

    if repos_to_prune:
        info(f"  âœ“ Pruned metadata in {len(repos_to_prune)} repo(s)")
        info(f"  ðŸ’¡ Commits preserved on branch: jib-temp-{container_id}")


def setup() -> bool:
    """Interactive setup process"""
    print()
    info("=== Autonomous Software Engineering Agent - Setup ===")
    print()
    print("ðŸ¤– AUTONOMOUS ENGINEERING AGENT")
    print()
    print("This sets up a sandboxed environment for Claude to work as an autonomous")
    print("software engineer with minimal supervision.")
    print()
    print("OPERATING MODEL:")
    print("  â€¢ Agent: Plans, implements, tests, documents, creates PRs")
    print("  â€¢ Human: Reviews, approves, deploys")
    print()
    print("AGENT CAPABILITIES:")
    print("  âœ“ Edit code and create commits in ~/khan/")
    print("  âœ“ Run tests, linters, development servers")
    print("  âœ“ Access Confluence docs (ADRs, runbooks, best practices)")
    print("  âœ“ Create pull requests with @create-pr command")
    print("  âœ“ Build accumulated knowledge with @save-context")
    print("  âœ“ Network access for Claude API and package installs")
    print()
    print("SECURITY ISOLATION:")
    print("  âœ— NO access to SSH keys (cannot git push)")
    print("  âœ— NO access to gcloud credentials (cannot deploy)")
    print("  âœ— NO access to GSM secrets")
    print()
    print("HOW IT WORKS:")
    print("  1. Your ~/khan is MOUNTED into the container as ~/khan (read-write)")
    print("  2. Changes in container = changes on host (same files)")
    print("  3. Agent works on code, creates commits")
    print("  4. YOU review commits and push from host (with credentials)")
    print()
    print("FUTURE CAPABILITIES (Roadmap):")
    print("  ðŸ”„ GitHub PR context")
    print("  ðŸ”„ Slack message context")
    print("  ðŸ”„ JIRA ticket context")
    print("  ðŸ”„ Email thread context")
    print()

    response = input("Continue? (yes/no): ").strip().lower()
    if response != "yes":
        info("Setup cancelled")
        return False

    print()
    info("Setting up mounts...")
    print()

    Config.CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    mounts = []

    # Khan directory - mount READ-WRITE so agent can work directly
    if Config.KHAN_SOURCE.exists():
        khan_container_path = f"/home/{os.environ['USER']}/khan"
        # Mount as read-write with SELinux relabeling
        mounts.append(f"{Config.KHAN_SOURCE}:{khan_container_path}:rw,z")
        info(f"Khan workspace: {Config.KHAN_SOURCE}")
        print(f"    Mounted as: ~/khan/ (READ-WRITE, SELinux relabeled)")
        print(f"    Purpose: Agent works directly on code, makes commits")
        print(f"    Changes: Agent modifies files in place - you review and push")
    else:
        warn(f"{Config.KHAN_SOURCE} not found - workspace will not be available")

    # Add context-sync directory (read-only) - includes Confluence, JIRA, and more
    print()
    context_sync_dir = Path.home() / "context-sync"
    if context_sync_dir.exists():
        context_container_path = f"/home/{os.environ['USER']}/context-sync"
        # Add :z flag for SELinux systems
        mounts.append(f"{context_sync_dir}:{context_container_path}:ro,z")
        print(f"  âœ“ Context sources: {context_sync_dir}")
        print(f"    Mounted as: ~/context-sync/ (read-only, SELinux relabeled)")

        # Show available context sources
        subdirs = []
        if (context_sync_dir / "confluence").exists():
            subdirs.append("confluence (ADRs, runbooks, docs)")
        if (context_sync_dir / "jira").exists():
            subdirs.append("jira (tickets, issues)")
        if (context_sync_dir / "github").exists():
            subdirs.append("github (PRs, issues)")
        if (context_sync_dir / "slack").exists():
            subdirs.append("slack (messages)")

        if subdirs:
            print(f"    Contains: {', '.join(subdirs)}")
        else:
            print(f"    Note: No context subdirectories found yet")
    else:
        warn(f"Context sync directory not found: {context_sync_dir}")
        warn(f"Expected directory with confluence/, jira/, etc. subdirectories")

    # Create and mount persistent directories for agent
    print()
    info("Setting up persistent directories...")

    # Sharing directory - single location for ALL persistent data
    Config.SHARING_DIR.mkdir(parents=True, exist_ok=True)
    Config.TMP_DIR.mkdir(parents=True, exist_ok=True)    # tmp/ inside sharing/
    Config.CLAUDE_SHARED_DIR.mkdir(parents=True, exist_ok=True)  # claude/ inside .jib/

    sharing_container_path = f"/home/{os.environ['USER']}/sharing"
    # Add :z flag for SELinux systems
    mounts.append(f"{Config.SHARING_DIR}:{sharing_container_path}:rw,z")
    print(f"  âœ“ Sharing: {Config.SHARING_DIR}")
    print(f"    Mounted as: ~/sharing/ (read-write, SELinux relabeled)")
    print(f"    Purpose: All persistent data")
    print(f"    - ~/sharing/tmp/           Persistent workspace (also at ~/tmp)")
    print(f"    - ~/sharing/context/       Context documents (@save-context)")
    print(f"    - ~/sharing/notifications/ Notifications to human")
    print(f"    - ~/sharing/incoming/      Incoming tasks from Slack")
    print(f"    - ~/sharing/analysis/      Analysis reports")

    # Create convenience symlink in container for tmp
    # Note: Actual symlink creation happens in container entrypoint

    # Mount shared .claude directory (persistent across all containers)
    # This ensures all containers share the same authentication
    print()
    print(f"{Colors.BOLD}Claude Code authentication...{Colors.NC}")

    # Mount ~/.claude/ directory
    claude_container_path = f"/home/{os.environ['USER']}/.claude"
    mounts.append(f"{Config.CLAUDE_SHARED_DIR}:{claude_container_path}:rw,z")

    # Mount ~/.claude.json file (user state with userID)
    claude_json_file = Config.CONFIG_DIR / "claude.json"
    if not claude_json_file.exists():
        # Initialize with empty JSON object (Claude Code will populate on first run)
        claude_json_file.write_text("{}\n")
    claude_json_container_path = f"/home/{os.environ['USER']}/.claude.json"
    mounts.append(f"{claude_json_file}:{claude_json_container_path}:rw,z")

    # Check if credentials already exist in shared directory
    shared_creds = Config.CLAUDE_SHARED_DIR / ".credentials.json"
    if shared_creds.exists():
        success("Claude credentials found in shared directory")
        print(f"  All containers will use:")
        print(f"    ~/.claude/ â†’ {Config.CLAUDE_SHARED_DIR}")
        print(f"    ~/.claude.json â†’ {claude_json_file}")
    else:
        info("Claude credentials not yet configured")
        print(f"  Shared locations:")
        print(f"    ~/.claude/ â†’ {Config.CLAUDE_SHARED_DIR}")
        print(f"    ~/.claude.json â†’ {claude_json_file}")
        print()
        info("The first container will authenticate and save credentials.")
        info("All other containers will automatically use the same credentials.")

    print()
    print("Add additional directories? (optional)")
    print("Format: /path/to/dir        (read-write)")
    print("    or: /path/to/dir:ro     (read-only)")
    print("Press Enter on empty line when done")
    print()

    # Collect additional directories
    while True:
        dir_input = input("Additional directory (or Enter to finish): ").strip()
        if not dir_input:
            break

        # Parse mode
        if ":ro" in dir_input or ":rw" in dir_input:
            mount_path_str, mode = dir_input.rsplit(":", 1)
            if mode not in ["ro", "rw"]:
                warn(f"Invalid mode '{mode}', use 'ro' or 'rw'")
                continue
        else:
            mount_path_str = dir_input
            mode = "rw"

        # Expand and validate path
        mount_path = Path(mount_path_str).expanduser().resolve()

        # Check if dangerous
        if is_dangerous_dir(mount_path):
            print(f"â›” BLOCKED: {mount_path}")
            print("   This directory contains credentials and will not be mounted.")
            print("   This is intentional to prevent AI from accessing sensitive files.")
            continue

        if not mount_path.exists():
            warn(f"Directory does not exist: {mount_path}")
            create = input("Create it? (yes/no): ").strip().lower()
            if create == "yes":
                try:
                    mount_path.mkdir(parents=True, exist_ok=True)
                    success(f"Created: {mount_path}")
                except Exception as e:
                    error(f"Failed to create directory: {e}")
                    continue
            else:
                continue

        # Add SELinux label for Fedora/RHEL compatibility
        mounts.append(f"{mount_path}:{mode},z")
        print(f"Added: {mount_path} ({mode}, SELinux relabeled)")

    # Save configuration
    Config.CONFIG_FILE.write_text("\n".join(mounts))

    print()
    info("Summary of mounted directories:")
    for mount in mounts:
        print(f"  â€¢ {mount}")
    print()

    proceed = input("Proceed with this configuration? (yes/no): ").strip().lower()
    if proceed != "yes":
        info("Setup cancelled")
        return False

    # Create Dockerfile and build image
    create_dockerfile()
    print()

    # Let Docker's cache handle what needs rebuilding
    info("Building Docker image (Docker will cache unchanged layers)...")
    if not build_image():
        return False

    print()
    success("Setup complete!")
    print()
    return True


def run_claude() -> bool:
    """Run Claude Code CLI in the sandboxed container (interactive mode)"""
    # Check if image exists
    if not image_exists():
        info("Docker image not found. Running initial setup...")
        if not setup():
            return False

    # Load mount configuration
    if not Config.CONFIG_FILE.exists():
        info("Configuration not found. Running initial setup...")
        if not setup():
            return False

    # Ensure shared authentication files exist
    Config.CLAUDE_SHARED_DIR.mkdir(parents=True, exist_ok=True)
    claude_json_file = Config.CONFIG_DIR / "claude.json"
    if not claude_json_file.exists():
        # Initialize with empty JSON object (Claude Code will populate on first run)
        claude_json_file.write_text("{}\n")

    # Check Claude Code authentication in shared directory
    print()
    print(f"{Colors.BOLD}Checking Claude Code authentication...{Colors.NC}")

    shared_creds = Config.CLAUDE_SHARED_DIR / ".credentials.json"

    if shared_creds.exists():
        try:
            with open(shared_creds, 'r') as f:
                creds = json.load(f)

            scopes = creds.get("claudeAiOauth", {}).get("scopes", [])
            required_scopes = {
                "user:inference",
                "user:profile",
                "user:sessions:claude_code"
            }

            if required_scopes.issubset(set(scopes)):
                success("Claude Code authenticated with full scopes (shared)")
                print(f"  Location: {Config.CLAUDE_SHARED_DIR}")
            else:
                warn("Claude Code credentials found but missing required scopes")
                warn("  Container will work with limited functionality (--print mode only)")
                print(f"  Location: {Config.CLAUDE_SHARED_DIR}")
        except Exception as e:
            warn(f"Could not read shared credentials: {e}")
            warn("Container will prompt for authentication on first run")
    else:
        info("Claude Code not yet authenticated")
        info("Container will prompt for authentication on first launch")
        info("Credentials will be saved and shared across all containers")
        print(f"  Location: {Config.CLAUDE_SHARED_DIR}")

    print()

    # Build/update image (Docker uses cache for unchanged layers - usually instant)
    if not build_image():
        error("Docker build failed")
        return False

    # Generate unique container ID
    container_id = generate_container_id()

    info("Launching sandboxed Claude Code environment...")
    print()

    info(f"Container ID: {container_id}")
    print()

    # Create worktrees for repos (isolates container from host repos)
    info("Creating isolated worktrees...")
    print()
    worktrees = create_worktrees(container_id)

    if worktrees:
        print()
        info(f"Created {len(worktrees)} worktree(s)")
        print()

    # Register cleanup on exit
    def cleanup_on_exit():
        cleanup_worktrees(container_id)

    atexit.register(cleanup_on_exit)

    # Parse mount configuration
    mount_args = []
    mounts = Config.CONFIG_FILE.read_text().strip().split("\n")
    for mount in mounts:
        if not mount:
            continue
        parts = mount.rsplit(":", 2)  # Split from right, max 2 splits

        if len(parts) == 2:
            # Old format: path:mode
            mount_path, mode = parts
            container_path = mount_path
        elif len(parts) == 3:
            # New format: host_path:container_path:mode
            mount_path, container_path, mode = parts
        else:
            warn(f"Invalid mount format: {mount}")
            continue

        # Check if this is a khan repo mount - replace with worktree
        if Config.KHAN_SOURCE and mount_path.startswith(str(Config.KHAN_SOURCE)):
            # This is a khan repo - skip it, we'll add worktrees instead
            continue

        # Ensure SELinux label is present (add if missing for backwards compatibility)
        if not mode.endswith(",z") and not mode.endswith(":z"):
            mode = f"{mode},z"

        mount_args.extend(["-v", f"{mount_path}:{container_path}:{mode}"])
        mode_str = "READ-ONLY" if "ro" in mode else "READ-WRITE"
        display_path = container_path if container_path != mount_path else mount_path
        print(f"  â€¢ {display_path} ({mode_str}, SELinux relabeled)")

    # Add worktree mounts
    # Also track which main repos need their .git directories mounted
    main_repos_needed = set()
    for repo_name, worktree_path in worktrees.items():
        container_path = f"/home/{os.environ['USER']}/khan/{repo_name}"
        mount_args.extend(["-v", f"{worktree_path}:{container_path}:rw,z"])
        print(f"  â€¢ ~/khan/{repo_name} (WORKTREE, isolated from host)")
        main_repos_needed.add(repo_name)

    # Mount main repo .git directories (read-write) so worktrees can commit changes
    for repo_name in main_repos_needed:
        main_repo = Config.KHAN_SOURCE / repo_name
        if (main_repo / ".git").exists():
            # Mount the main repo's .git directory at a special location
            # Worktree .git files will be rewritten to point here
            git_container_path = f"/home/{os.environ['USER']}/.git-main/{repo_name}"
            mount_args.extend(["-v", f"{main_repo / '.git'}:{git_container_path}:rw,z"])
            print(f"  â€¢ ~/.git-main/{repo_name} (git metadata for worktree, read-write)")

    # Mount worktree base directory (used by both interactive and --exec modes)
    worktree_base_container = f"/home/{os.environ['USER']}/.jib-worktrees"
    mount_args.extend(["-v", f"{Config.WORKTREE_BASE}:{worktree_base_container}:rw,z"])
    print(f"  â€¢ ~/.jib-worktrees/ (worktree base directory)")

    print()

    # Remove old container if exists (cleanup any previous runs)
    subprocess.run(["docker", "rm", "-f", container_id],
                  stdout=subprocess.DEVNULL,
                  stderr=subprocess.DEVNULL)

    # Build docker run command with bridge networking (more secure than --net host)
    # Bridge mode allows outbound HTTP/HTTPS (for Claude API and packages) but no inbound access
    cmd = [
        "docker", "run",
        "--rm",  # Auto-remove container after exit
        "-it",   # Interactive with TTY
        "--name", container_id,
        "-e", f"RUNTIME_USER={os.environ['USER']}",
        "-e", f"RUNTIME_UID={os.getuid()}",
        "-e", f"RUNTIME_GID={os.getgid()}",
        "-e", f"CONTAINER_ID={container_id}",
        # Bridge networking: outbound HTTP allowed, no inbound ports exposed
    ]

    # Add GitHub token for container (prefer App token, fall back to PAT)
    github_app_token = get_github_app_token()
    if github_app_token:
        cmd.extend(["-e", f"GITHUB_TOKEN={github_app_token}"])
        info("GitHub auth: App installation token (1-hour expiry)")
        print("  Full GitHub API access including Checks API")
    else:
        # Fall back to PAT if App not configured
        github_token = get_github_token()
        if github_token:
            cmd.extend(["-e", f"GITHUB_TOKEN={github_token}"])
            info("GitHub auth: Personal Access Token (fallback)")
            warn("  Note: PAT cannot access Checks API. Configure GitHub App for full access.")
        else:
            warn("GitHub auth: Not configured (GitHub operations disabled)")
            print("  Configure GitHub App: ~/.config/jib/github-app-*.pem")

    info("Network mode: Bridge (isolated from host, outbound HTTP only)")
    print("  Container can: Access Claude API, download packages")
    print("  Container cannot: Access host services, accept inbound connections")
    print()

    info("Claude authentication: Will authenticate with browser on first run")
    print()

    # Add mount arguments
    cmd.extend(mount_args)

    # Add image name
    cmd.append(Config.IMAGE_NAME)

    # Run container
    try:
        subprocess.run(cmd)
        return True
    except KeyboardInterrupt:
        print()
        warn("Interrupted by user")
        return False
    except Exception as e:
        error(f"Failed to run container: {e}")
        return False


def exec_in_new_container(command: List[str]) -> bool:
    """Execute a command in a new ephemeral container with isolated worktrees.

    Creates worktrees for all repos to isolate changes (same as interactive mode):
    - Total isolation from interactive sessions and main repos
    - Worktrees allow parallel work without conflicts
    - Automatic cleanup (--rm)
    - All commits go to temporary branches (jib-temp-jib-exec-*)

    Args:
        command: Command to execute

    Returns:
        True if successful, False otherwise
    """
    # Check if image exists
    if not image_exists():
        info("Docker image not found. Running initial setup...")
        if not setup():
            return False

    # Load mount configuration
    if not Config.CONFIG_FILE.exists():
        info("Configuration not found. Running initial setup...")
        if not setup():
            return False

    # Build/update image
    if not build_image():
        error("Docker build failed")
        return False

    # Generate unique container ID for this exec
    container_id = f"jib-exec-{datetime.now().strftime('%Y%m%d-%H%M%S')}-{os.getpid()}"

    info(f"Executing command in new container: {container_id}")
    print(f"Command: {' '.join(command)}")
    print()

    # Create worktrees for repos (isolates container from host repos)
    info("Creating isolated worktrees...")
    worktrees = create_worktrees(container_id)

    if worktrees:
        info(f"Created {len(worktrees)} worktree(s)")
        print()

    # Register cleanup on exit (even if container fails)
    def cleanup_on_exit():
        cleanup_worktrees(container_id)

    atexit.register(cleanup_on_exit)

    # Parse mount configuration
    mount_args = []
    mounts = Config.CONFIG_FILE.read_text().strip().split("\n")

    for mount in mounts:
        if not mount:
            continue
        parts = mount.rsplit(":", 2)

        if len(parts) == 2:
            mount_path, mode = parts
            container_path = mount_path
        elif len(parts) == 3:
            mount_path, container_path, mode = parts
        else:
            warn(f"Invalid mount format: {mount}")
            continue

        # Check if this is a khan repo mount - skip it, we'll add worktrees instead
        if Config.KHAN_SOURCE and mount_path.startswith(str(Config.KHAN_SOURCE)):
            continue

        # Ensure SELinux label
        if not mode.endswith(",z") and not mode.endswith(":z"):
            mode = f"{mode},z"

        mount_args.extend(["-v", f"{mount_path}:{container_path}:{mode}"])

    # Add worktree mounts
    main_repos_needed = set()
    for repo_name, worktree_path in worktrees.items():
        container_path = f"/home/{os.environ['USER']}/khan/{repo_name}"
        mount_args.extend(["-v", f"{worktree_path}:{container_path}:rw,z"])
        print(f"  â€¢ ~/khan/{repo_name} (WORKTREE, isolated)")
        main_repos_needed.add(repo_name)

    # Mount main repo .git directories (read-write) so worktrees can commit changes
    for repo_name in main_repos_needed:
        main_repo = Config.KHAN_SOURCE / repo_name
        if (main_repo / ".git").exists():
            git_container_path = f"/home/{os.environ['USER']}/.git-main/{repo_name}"
            mount_args.extend(["-v", f"{main_repo / '.git'}:{git_container_path}:rw,z"])
            print(f"  â€¢ ~/.git-main/{repo_name} (git metadata)")

    print()

    # Build docker run command
    cmd = [
        "docker", "run",
        "--rm",  # Auto-remove container after exit
        "--name", container_id,
        "-e", f"RUNTIME_USER={os.environ['USER']}",
        "-e", f"RUNTIME_UID={os.getuid()}",
        "-e", f"RUNTIME_GID={os.getgid()}",
        "-e", f"CONTAINER_ID={container_id}",
    ]

    # Add GitHub token for container (prefer App token, fall back to PAT)
    github_app_token = get_github_app_token()
    if github_app_token:
        cmd.extend(["-e", f"GITHUB_TOKEN={github_app_token}"])
    else:
        github_token = get_github_token()
        if github_token:
            cmd.extend(["-e", f"GITHUB_TOKEN={github_token}"])

    # Add mount arguments
    cmd.extend(mount_args)

    # Add image name
    cmd.append(Config.IMAGE_NAME)

    # Add the command to execute
    cmd.extend(command)

    # Run container with 20-minute timeout (configurable via timeout_seconds parameter)
    try:
        result = subprocess.run(cmd, timeout=1200)  # 20 minutes
        return result.returncode == 0
    except subprocess.TimeoutExpired:
        print()
        error("Container execution timed out after 20 minutes")
        # Kill the container if it's still running
        subprocess.run(["docker", "kill", container_id],
                      stdout=subprocess.DEVNULL,
                      stderr=subprocess.DEVNULL)
        return False
    except KeyboardInterrupt:
        print()
        warn("Interrupted by user")
        return False
    except Exception as e:
        error(f"Failed to run container: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description="Run Claude Code CLI in an isolated Docker container (james-in-a-box)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  jib                                      # Run Claude Code (setup on first use)
  jib --setup                              # Reconfigure mounts
  jib --reset                              # Reset configuration and remove Docker image
  jib --exec <command> [args...]          # Execute command in new ephemeral container
  jib --exec python3 /home/jwies/khan/james-in-a-box/jib-container/components/incoming-processor.py /home/jwies/sharing/incoming/task.md

Note: --exec spawns a new container for each execution (automatic cleanup with --rm)
        """
    )
    parser.add_argument(
        "--setup",
        action="store_true",
        help="Force setup/reconfiguration"
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="Clear configuration and start over"
    )
    parser.add_argument(
        "--exec",
        nargs=argparse.REMAINDER,
        help="Execute a command in a new ephemeral container (automatic cleanup)"
    )

    args = parser.parse_args()


    # Handle reset
    if args.reset:
        warn("Resetting configuration...")
        import shutil
        if Config.CONFIG_DIR.exists():
            shutil.rmtree(Config.CONFIG_DIR)

        # Ask about persistent directories
        if Config.TOOLS_DIR.exists() or Config.SHARING_DIR.exists():
            print()
            warn("Persistent directories found:")
            if Config.TOOLS_DIR.exists():
                print(f"  â€¢ {Config.TOOLS_DIR} (reusable scripts/tools)")
            if Config.SHARING_DIR.exists():
                print(f"  â€¢ {Config.SHARING_DIR} (shared artifacts, context documents)")
            print()
            response = input("Remove these as well? (yes/no): ").strip().lower()
            if response == "yes":
                if Config.TOOLS_DIR.exists():
                    shutil.rmtree(Config.TOOLS_DIR)
                    warn(f"Removed: {Config.TOOLS_DIR}")
                if Config.SHARING_DIR.exists():
                    shutil.rmtree(Config.SHARING_DIR)
                    warn(f"Removed: {Config.SHARING_DIR}")
            else:
                info("Preserved persistent directories")

        success("Configuration reset. Run again to set up fresh.")
        return 0

    # Check prerequisites
    if not check_docker():
        return 1

    if not check_docker_permissions():
        return 1

    # Check host setup (services and directories)
    if not check_host_setup():
        return 1

    # Handle setup
    if args.setup:
        if not setup():
            return 1
        return 0

    # Handle exec - execute in a new ephemeral container
    if args.exec:
        if not exec_in_new_container(args.exec):
            return 1
        return 0

    # Normal run
    if not run_claude():
        return 1

    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        print()
        warn("Interrupted by user")
        sys.exit(0)
    except Exception as e:
        error(f"Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
